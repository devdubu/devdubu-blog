---
sticker: emoji//1f680

slug: "LLM의-기초-뼈대-세우기"
---
# LLM 지도
사실 ChatGPT는 굉장히 단순한 과정으로 동작한다.
주어진 입력에서 다음에 올 적절한 단어를 확률적으로 선택하고, 선택한 단어를 입력에 더해 문장이 끝날 때까지 선택하는 작업이 반복한다.
다음 단어를 선택하는 가장 단순한 행동에서 OpenAI 의 ChatGPT, 구글 Gemini 같은 혁신적인 제품이 만들어지는 것이다.

## 딥러닝과 언어 모델링

우선 LLM을 정확히 이해하기 위해 LLM 이 어떤 기술과 분야에 속하는지 살펴보자
LLM은 기술적으로 딥러닝에 기반을 두고 있다.
![Pasted-image-20250522104124.png](/img/이미지 창고/Pasted-image-20250522104124.png)
Deep Leaning이란 인간의 두뇌에 영감을 받아 만들어진 신경망으로서, 데이터의 패턴을 학습 하는 머신러닝의 한 분야다.
딥러닝은 표 형태의 정형 데이터 뿐만 아니라 텍스트와 이미지 같은 비정형 데이터(understuctured data)에서도 뛰어난 패턴 인식 성능을 보여 2010년대 중반 이후, AI 분야의 주류 모델로 자리 잡앗습니다

LLM은 사람의 언어를 컴퓨터가 이해하고 생성할 수 있도록 연구하는 자연어 처리에  분야에 속하며, 특히 그 중에서도 사람과 비슷하게 텍스트를 생성하는 방법을 연구하는 자연어 생성에 속합니다.

LLM은 다음에 올 단어가 무엇일지 예측하면서 문장을 하나씩 만들어가는 방식으로 텍스트를 생성하는데, 이렇게 다음에 올 단어를 예측하는 모델을 언어 모델이라고 합니다.
즉, 한마디로 간추리자면, LLM 딥러닝 기반의 언어 모델이다.

딥러닝 기반의 언어 모델이 지금처럼 자리 잡기까지 역사적으로 중요햇던 세 가지 사건이 있었습니다.
1. 2013년 구글에서 단어의 의미를 담아 숫자로 표현하는 워드투백(word2vec) 을 발표했습니다.
2. 2017년에는 기계 번역 성능을 높이기 위해 개발된 트랜스포머 아키텍처를 공개햇습니다.
3. 마지막으로 2018년 OpenAI가 트랜스포머 아키텍처를 활용한 GPT-1 모델을 공개햇습니다.

지금가지 살펴본 내용을 바탕으로 LLM을 좀 더 깊게 이해해보기 위해 이제부터는 딥 러닝 기반의 언어 모델링과 딥러닝 모델이 데이터를 이해하기 위해 사용하는 임베딩에 대해 살펴보고 , 그림 1.2 주요 사건이 어떤 영향을 미쳣는지 알아보자

![Pasted-image-20250522104050.png](/img/이미지 창고/Pasted-image-20250522104050.png)

### 데이터의 특징을 스스로 추출하는 딥러닝
2012년 입력 이미지를 적절한 클래스로 분류하는 이미지 인식 대회인 이미지넷에서 딥러닝 모델인 알렉스넷(AlexNet)이 우승하며 딥러닝 분야는 큰 주목을 받기 시작햇습니다.
알렉스넷은 전년도 1등 모델의 오류율(26%)을 10%p 가량 낮춘 16%의 오류율을 기록하며 우승을 차지햇습니다.

당시에 주로 활용하던 앝은 형태의 모델로는 오류율을 0.1%p 더 낮추기도 쉽지 않았기 때문에, 알렉스넷의 성능은 엄청난 충격으로 다가왓습니다.

하지만, 뛰어난 성능 만큼이나 사람들을 매료시킨 딥 러닝의 특징이 있었는데, 그것은 바로 단순하면서도 범용적인 무제를 해결하는 접근 방식이었습니다.

- 문제의 유형(예: 자언어 처리, 이미지 처리)에 따라 일반적으로 사용되는 모델을 준비한다
- 풀고자 하는 문제에 대한 학습 데이터를 준비한다.
- 학습 데이터를 반복적으로 모델에 입력한다.

이렇게 3단계만 거치면 문제는 간단 하게 풀린다.
딥 러닝은 이 처럼 단순한 접근 방식을 통해 기존에는 쉽게 풀 수 없었던 텍스트나 이미지 같은 비 정형 데이터 문제도 쉽게 풀어냇다.
딥 러닝의 놀라운 단순성은 기존의 머신 러닝 접근법과 비교했을 때, 명확히 드러난다.

딥러닝이 머신 러닝과 가장 큰 차이를 보이는 지점은 <mark>데이터의 특징은 누가 뽑는가?</mark> 이다.
기존 머신러닝에서는 아래와 같이 데이터의 특징을 연구자 혹은 개발자가 찾고, 모델에 입력으로 넣어 자동차인지 아닌지 결과를 출력했다.
아래 그림의 예시에서 데이터의 특징이란, 바퀴가 있고, 문이 달려있고, 창문이 있는 등 일반적인 자동차가 갖는 성질이 도리 수 있습니다.

반면 딥 러닝에서는 아래와 같이 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습합니다.


![Pasted-image-20250522104011.png](/img/이미지 창고/Pasted-image-20250522104011.png)

### 임베딩 : 딥러닝 모델이 데이터를 표현하는 방식
컴퓨터는 숫자만 처리 할 수 있기 때문에, 딥러닝 모델은 데이터의 의미를 숫자의 집합으로 표현합니다.
데이터의 의미와 특징을 포착해 숫자로 표현하는 것을 임베딩이라고 합니다ㅣ.

임베딩은 딥러닝을 이해할 때, 가장 중요한 개념 중 하나인데, 처음 접했을 때는 직관적으로 이해하기 어려우므로, 몇 년 전부터 한국에서 크게 유행하고 있는 MBTI 심리 검사의 예를 들어 임베딩의 개념을 한번에 이해해보자

사람은 매우 다양하고 고유하기 때문에, 어떤 사람을 이해하기 위해서는 많은 시간과 노력이 필요하다.
이는 자기 자신을 이해하는 것도 마찬가지다.
MBTI와 같은 심리 검사는 사람의 특정한 해석 틀에 따라 분석함으로써 비교적 단순하고 직관적으로 이해할 수 있는 관점을 제공한다. MBTI 사람을 아래와 같이 4범주로 나눕니다.

![Pasted-image-20250522104515.png](/img/이미지 창고/Pasted-image-20250522104515.png)

만약 검사 결과 어떤 사람이 각 범주에 아래 그림과 같은 점수를 받았다면, 그 사람의 MBTI는 INTP가 되고 숫자로 나타내면[0.3, 0.2, 0.9, 0.2] 로 간단하게 표현할 수 있다.
어떤 사람의 성향이라는 복잡한 데이터를 단 4개의 숫자로 표현하는 것이다.
우리가 사용하는 단어나 문장 도는 이미지, 비디오 등도 모두 사람에 비할 바는 아니지만, 각기 나름 대로 복잡 하고 미묘한 정보를 담고 있다.
즉, 임베딩이란 MBTI 검사 결과를 바탕으로 사람을 4개의 숫자로 표현하는 것처럼, 데이터를 그 의미를 담아 여러 개의 숫자의 집합으로 표현하는 것을 말한다.

![Pasted-image-20250522104931.png](/img/이미지 창고/Pasted-image-20250522104931.png)
이런식으로 숫자로 나타내면 무엇이 좋을까?

우리는 아래 그림과 같이 MBTI 검사 결과를 바탕으로 재미삼아 나와 비슷한 사람, 나와 다른 사람이 누군지 찾고, 어떤 점이 비슷하고 다른지 따져본다.
마찬가지로 데이터를 임베딩으로 표현하면 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련 없는 데이터를 구분할 수 있다.

![Pasted-image-20250522105110.png](/img/이미지 창고/Pasted-image-20250522105110.png)
임베딩은 거리를 계산 할 수 있기 때문에, 다음과 같은 작업에 활용할 수 있다.
- 검색 및 추천: 검색어와 관련이 있는 상품을 추천한다.
- 클러스터링 및 분류: 유사하고 관련이 있는 데이터를 하나로 묶는다.
- 이상치 탐지: 나머지 데이터와 거리가 먼 데이터는 이상치로 볼 수 있다

앞서 사람의 경우 MBTI 검사를 통해 4개의 숫자로 표현할 수 있었다.
그렇다면, 데이터는 어떻게 숫자로 변환할까
2013년 구글에서 발표한 Efficient Estimation of Word Represetations in Vector Space(벡터 공간에서 단어 표현의 효율적인 추정)라는 논무넹서 워드투벡 이라는 모델을 통해 단어를 임베딩으로 변환하는 방법을 소개했습니다.

아래와 같이 사람이 MBTI 검사를 통해 4개의 숫자로 표현된느 것처럼 단어가 워드투벡 모델을 통해 숫자의 집합인 임ㄷ베딩으로 변환된다.
이와 같이 단어를 임베딩으로 변환하는 것을 일컬어 단어 임베딩이라고 한다.

![Pasted-image-20250522233109.png](/img/이미지 창고/Pasted-image-20250522233109.png)

이때 MBTI 검사와 워드투벡 같은 단어 임베딩 모델의 공통점과 차이점을 비교한 결과를 표로 살펴보자.
사람의 성향은 MBTI 검사를 통해 4개의 숫자로 표현되지만, 단어의 경우에는 보통 수십에서 수만개의 숫자로 표현된다.
또한 MBTI 검사는 사람이 설계했기 때문에, 각 숫자가 어떤 의미인지 정확히 알 수 있었지만,
단어 임베딩에서는 0.1, 0.7과 같은 숫자가 어떤 의미인지 알기 어렵다.

딥러닝 모델이 데이터에서 특징을 추출하는 방법을 알아서 학습하기 때문에, 사람이 그 의미를 하나하나 파악할 수 없는 것이다.
숫자 하나하나의 의미는 알기 어렵지만, [0.1,0.7, ... ,0.3] 이라는 숫자 집합 전체로 입력한 단어의 의미를 담고 있습니다.


![Pasted-image-20250522233524.png](/img/이미지 창고/Pasted-image-20250522233524.png)

딥 러닝 모델은 데이터를 통해 학습하는 과정에서 그 데이터를 가장 잘 이해할 수 있는 방식을 함께 배운다.
그렇게 데이터의 의미를 숫자로 표현하는 것이 바로 임베딩이다.
아직 딥러닝 모델이 텍스트 데이터를 어떻게 학습하는지 살펴보지 않았다.

다음으로 딥 러닝 모델이 텍스트 데이터를 학습하는 가장 대표적인 방법인 언어 모델링에 대해 살펴본다.


### 언어 모델링 : 딥러닝 모델의 언어 학습법

언어 모델링이란, 모델이 입력받는 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식을 말한다.
앞서도 잠깐 언급했지만, 다음 단어를 예측하는 방식으로 훈련한 모델을 언어 모델이라고 한다.

언어 모델링은 텍스트를 생성하는 모델을 학습시키는 방법으로도 사용되지만, 대량의 데이터에서 언어의 특성을 학습하는 사전 학습 과제로도 많이 사용됩니다.

딥러닝 분야에서는 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀대 사용하는 방식을 많이 활용하는데, 이를 전이 학습이라고 부릅니다.

전이 학습에서는 대량의 데이터로 모델을 학습시키는 사전 학습과 특정한 문제를 해결하기 위한 데이터로 추가 학습하는 미세 조정의 두 단계로 나눠 학습을 진행한다.

언어 모델링은 자연어 처리 분야에서 사전학습을 위한 과제로 사용된다.

일찍이 이미지 인식 분야에서는 이미 자연어 처리 분야보다 먼저, 대량의 데이터로 이미 학습된 모델을 필요한 과제에 맞춰 추가로 학습하는 전이 학습 개념이 널리 활용되고 있었다.

보듯이 사전 학습 모델은 이미지넷 데이터셋을 통해 이미지에 나타난 동물을 분류 하도록 학습한다.

유방암이 양성인지 악성인지 분류하는 새로운 문제를 풀고자 할 때, 유방암 데이터만으로 학습한 모델보다 사전 학습 모델의 일부를 가져와 활용했을 때, 일반적으로 성능이 더 높다.

사전 학습에 사용된 이미지와 현재 풀고자하는 과제의 이미지가 다르더라도 , 선이나 점 같은 특징을 파악하는 능력은 공통적으로 필요하기 때문이다.
이때 사전 학습 모델을 미세 조정해 풀고자 하는 과제를 흔히 다운스트림 과제라고 부른다.

전이 학습은 학습 데이터가 적은 경우에 특히 유용한데, 아래 그럼에서 사전 학습한 모델을 활용하면 더 적은 유방암 이미지만으로도 높은 성능의 모델을 학습시킬 수 있다.

기존의 머신러니 모델 학습은 처음부터 끝까지 해결하려는 문제의 데이터로 학습하는 방식을 사용했다.
하지만 딥러닝의 전이 학습은 대량의 데이터로 학습하는 단계와 현재 해결하려는 문제의 데이터로 추가학습 하는 두 단계로 모델을 학습시킨다.

![Pasted-image-20250522234724.png](/img/이미지 창고/Pasted-image-20250522234724.png)

전이 학습의 프레임워크를 나타낸 아래 그림을 보면
![Pasted-image-20250522234813.png](/img/이미지 창고/Pasted-image-20250522234813.png)
바로 기존의 지도 학습 방식으로, 각각의 데이터셋으로 별도의 모델을 학습시켰다.
위 그림은 전이 학습 방식으로, 모델의 본체 부분은 대규모 데이터셋인 이미지넷으로 학습한 모델에서 가져오고 분류를 수행하는 헤드 부분은 해결하려는 작업의 데이터셋으로 추가 학습한다.

헤드 부분은 일반적으로 본체 부분에 비해 작기 때문에 비교적 적은 데이터로도 학습이 가능하다.
이때 헤드를 추가 학습하는 과정이 사전 학습에 비해 적은 양의 학습 데이터를 사용한다는 의미에서 미세 조정이라고 부른다.
자연어 처리 분야에서도 이미지 인식에서와 같이 전이 학습 개념을 활용하고 싶었지만 마땅한 사전 학습 방식을 찾지 못하고 있었다.
그러던 중 2018년 `fast.ai`의 제레미 하워드와 세바스찬 루더가 다음 단어를 예측하는 언어 모델링 방식으로 사전학습을 수행했을 때, 훨씬 적은 레이블 데이터로도 기존 지도 학습 모델의 성능을 뛰어넘는다는 사실을 발견했다.
(텍스트 분류를 위한 범용 언어 모델 미세 조정)

아래 그림엣 다음 단어를 예측하는 방식으로 언어모델을 사전에 학습하고 나서 아래 그림와 같이 다운 스트림 과제의 데이터셋으로 언어 모델 미세 조정을 수행하고 마지막으로 아래 와 같이 텍스트 분류 미세 조정을 했을 때, 바로 텍스트 분류 모델로 학습 시켰을 때보다 성능이 높았다.

![Pasted-image-20250522235425.png](/img/이미지 창고/Pasted-image-20250522235425.png)

텍스트 데이터에 따로 레이블이 없더라도, 단지 다음 단어를 예측하는 방식으로 사전 학습에 활용할 수 있는 길이 드디어 열린 것입니다.

제레미 하워드와 세바스찬 루더는 당시 많이 활용되던 순환 신경망에서 언어 모델링이 사전 학습 과제로 적합하다는 사실을 확인 했습니다. 2018년 은 2017년 구굴의 아쉬쉬 바스와니가 발표한(Attention is All you need)라는 논문엣 공개된 트랜스 포머 아키텍처를 활용한 모델이 처음 등장하던 시기였다.
대표적인 트랜스포머 모델로는 구글의 BERT 와 OpenAI의 GPT가 있다.

OpenAI는 GPT-1의 논문인 Improving Language Understanding by Generative Pre-Training 에서 트랜스포머 모델에서도 언어 모델링으로 사전 학습을 수행했을 때가 그렇지 않았을 때에 비해 다운 스트림 과제에서 보델의 성능이 높았다고 발표햇다.
이를 통해 언어 모델링은 자연어 처리 분야에서 가장 대표적인 사전 학습 방법으로 자리 잡았다.


### 언어 모델이 ChatGPT 가 되기 까지
이제부터 앞서 살펴본 딥러닝 기반 언어 모델이 커져 LLM 이 되는 과정과 LLM을 ChatGPT와 같이 대화가 가능한 모델로 만들어지기 까지의 과정을 살펴봅니다.
이 과정에서 중요한 단계를 요약해 나타내면, 아래의 그림과 같습니다.

![Pasted-image-20250523074413.png](/img/이미지 창고/Pasted-image-20250523074413.png)

2017년의 획기적인 사건이었던 트랜스포머 아키텍처의 공개는 2018~2020년 순차적으로 공개된 GPT 시리즈와 깊은 연관이 있다.
트랜스포머 아키텍처를 자세히 살펴보면서 2022년에 등장하여 AI 분야의 판도를 바꾼 ChatGPT와 그 기반 기술인 정렬에 대해서도 알아보자

### RNN 에서 트랜스포머 아키텍처로

딥러닝이나, 머신러닝 분야에서 텍스트는 단어가 연결된 문장의 현태의 데이터를 일컷는다.
이 처럼 작은 단위의 데이터가 연결되고, 그 길이가 다양한 데이터의 형태를 시퀀스라고 합니다.
예를 들어 텍스트 , 오디오, 시계열과 같은 데이터가 시퀀스 데이터라고 할 수 있다.

역사적으로 이러한 시퀀스 데이터를 처리하기 위해 크게 순환 신경망이나, 트랜스포머의 두 가지 아키텍처로 대표되는 다양한 모델을 사용해 왔다.
여기서 모델 아키텍처란 딥러닝 모델이 갖는 구조를 의미한다.

트랜스포머가 개발되기 전에는 RNN을 활용해서 텍스트를 생성했다.
RNN은 아래 그림과 같이 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측 한다.

![IMG-3822.jpeg](/img/이미지 창고/IMG-3822.jpeg)
RNN의 특징은 위 그림에서 확인할 수 있듯이 모델이 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축 할 수있다는 점이다.

그림에서 첫 번째 잠재 상태에 지금까지 입력 텍스트의 맥락을 압축한다는 점이다.
첫번째 입력은 '검은' 이 모델을 통과했을 때는 잠재 상태에 '검은' 의 정보가 쌓이고, 두 번째 입력인 '고양이가' 가 모델을 통하면, 잠재 상태에 '검은' 과 '고양이가'의 정보가 누적된다.
이렇게 입력이 늘어나면서 하나의 잠재 상태에 입력 텍스트의 맥락이 압축된다.

텍스트의 맥락을 압축하고 다음 단어를 예측하는 과정을 부각해 나타낸 아래 그림을 보면, 
RNN 은 '물을' 다음의 단어를 예측하기 위해 앞에서 나온 맥락인 '검은 고양이가 밥을 먹고 물을' 순차적으로 한 단어씩 처리해서 하나의 잠재 상태로 만든다.

그 잠재 상태를 통해 RNN은 다음 단어로 '마신다'가 나온다고 예측 한다.
이 방식은 여러 단어로 구성된 맥락을 하나의 잠재 상태에 압축하기 때문에, 메모리를 적게 사용 한다는 장점이 있습니다.
또한 다음 단어를 예측할 때 지금까지 계산을 통해 만들어진 잠재 상태와 입력 단어만 있으면 되기 때문에, 다음 단어를 빠르게 생성할 수있습니다.
하지만, 순차적으로 입력되는 단어를 하나의 잠재 상태에 압축하다 보니 먼저 입력한 단어의 의미가 점차 희석되면, 입력이 길어지는 경우 의미를 충분히 담지 못하고 성능이 떨어진다는 문제가 있습니다.

![Pasted-image-20250523081413.png](/img/이미지 창고/Pasted-image-20250523081413.png)

2017년 등장한 트랜스포머 아키텍처는 RNN의 순차적인 처리 방식을 버리고, 맥락을 모두 참조하는 어텐션 연산을 사용해 RNN의 문제를 대부분 해결했다.

![Pasted-image-20250523081510.png](/img/이미지 창고/Pasted-image-20250523081510.png)

트랜스포머 아키텍처와 어텐션 연산에 대해서는 2장에서 상세히 살펴보므로 여기서는 RNN이 하나의 잠재 상태로 맥락을 압축하던 것과는 달리 맥락 데이터를 그대로 모두 활용해 다음 단어를 예측한다는 사실만 기억하자

위 그림에서는 '마신다' 다음 단어를 예측할 때, 이 전의 맥락인 '검은', '고양이가', '밥을', '먹고', ' 물을' 과의 관계를 모두 계산 한다.

위 그림을 상단 RNN 그림 같이 표현하면, 아래의 형태를 띄고 있습니다.
![IMG-3825.jpeg](/img/이미지 창고/IMG-3825.jpeg)
트랜스포머 아키텍처는 맥락을 압축하지 않고, 그래도 활용하기 때문에 성능을 높일 수 있지만, 이 그림에서 유추할 수 있듯이 입력 텍스트가 길어지면 맥락 데이털르 모두 저장하고 있어야 하기 때문에, 메모리 사용량이 증가한다.

또한 매번 다음 단어를 예측할 때마다 맥락 데이터를 모두 확인해야 하기 때문에 입력이 길어지면 예측에 걸리는 시간도 증가합니다.
성능이 높아지는 대신, 무겁고 비효율적인 연산을 사용하게 된 것입니다.

트랜스포머 아키텍처는 많은 병렬 연산량이 필요하다는 단점이 있지만, 성능이 좋고 순차적으로 처리하는 RNN과 달리 병렬 처리를 통해 학습 속도를 높일 수있어 현재는 대부분의 LLM이 트랜스포머 아키텍처를 기반으로 하고 있다.

아래 그림과 같이 호율성과 성능 축으로 하는 그래프에 트랜스포머 RNN을 배치하면, 트랜스포머 성능이 높지만, 비효율적이고 RNN은 효율적이지만, 성능이 낮아 각각 좌상단 우하단에 위치한다.
![Pasted-image-20250523084745.png](/img/이미지 창고/Pasted-image-20250523084745.png)
이 그림에서 점선으로 표현된 원과 같이 성능은 높이면서 효율적인 아키텍처를 찾기 위한 연구가 꾸준히 있었지만, 대부분의 트랜스포머의 성능에 미치지 못해 큰 주목을 받지 못했다.
그러던 중 최근 뛰어난 성능과 효율성을 갖춘 새로운 아키텍처사 공개 되며 많은 기대를 받고 있다.(새롭게 등장한 모델 아키텍처 중 대표적인 맘바를 이 책의 마지막 장인 16장에서 살펴본다.)

### GPT 시리즈로 보는 모델 크기와 성능의 관계
2017년 트랜스포머 아키텍처가 등장한 뒤 2018년 트랜스포머 아키텍처를 활용한 구글의 BERT 와 OpenAI의 GPT 가 등장하면서, 자연어 처리 분야에서 주류 모델 아키텍처 엿던 RNN은 빠르게 트랜스포머로 대체됐다.

또한 앞서 언급한 대로 다음 단어를 예측하는 언어 모델링이 사전 학습 방식으로 자리 잡으면서 더 큰 모델을 학습시킬 수 있는 기반을 마련했다.


이런 흐름 속에서 더 큰 모델을 학습시켜 자연어 이해 및 생성 능력을 높이려는 시도가 이어졌다.
대표적으로 OpenAI는 아래의 그래프와 가팅 2018년에 1억 1700만 개의 파라미터를 사용하는 GPT-1을 공개하고 2019은 모델을 12.8배 키워 15억개의 파라미터를 사용하는 GPT-2를 공개했습니다.
또 1년 후 2020년 이번에는 모델을 116.7배 키워 1750억 개의 파라미터를 사용하는 GPT-3를 공개했습니다.

![Pasted-image-20250523084808.png](/img/이미지 창고/Pasted-image-20250523084808.png)
그렇다면 왜 모델의 크기가 커지고 학습 데이터가 많을 수록 모델의 성능이 높아질까?
언어 모델이 학습 데이터를 압축한다는 관점에서 본다면 이 사실을 더 직관적으로 이해할 수 있다.
언어 모델의 경우 학습 데이터와 언어 모델의 결과가 모두 생성된 언어다.

따라서 언어 모델이 학습하는 과정을 학습 데이터를 압축 하는 과정으로 해석 할 수 있다.

여기서 말하는 압축은 우리가 일상생활에서 사용하는 zip 파일을 만들 때 처럼 무손실 압축을 의미하는 것은 아닙니다.
공통되고 중요한 패턴을 남기는 손실 압축 입니다.
대표적인 오픈소스 LLM인 메타의 라마2 모델을 예시로 보면, 아래 그림과 같이 10TB의 텍스트로 학습해 최종적으로 140GB 크기의 모델이 된다.
학습 데이터 대비 약 1.4% 정도의 작은 모델에, 학습데이터가 갖고 있던 텍스트 생성의 패턴을 압축했다고 볼 수 있습니다.
![Pasted-image-20250523145049.png](/img/이미지 창고/Pasted-image-20250523145049.png)
압축의 관점에서 봤을 때 모데링 커지면 학습 데이터가 갖고 있는 언어 생성 패턴을 더 많이 학습 할 수 있기 때문에 모델 성능이 높아 진다고 이해할 수 있다.
하지만 모델이 계속해서 커진다고 성능이 높아지지 않고, 학습 데이터의 크기가 최대 모델 크기의 상한이라고 볼 수 있다.

예를 들어 학습 데이터 크기가 100GB 일 경우, 모델의 크기를 100GB 보다 키운다고 하더라도, 모델 성능이 높아지기를 기대하기 어렵다.

### ChatGPT 등장
2020년 1750억개의 파라미터를 가진 GPT-3는 충분히 뛰어난 텍스트 생성 능력을 보였지만, 2022년 GPT와 비교했을 때 큰 반향을 일으키지 못햇다.
다양한 이유가 있겠지만, 가장 큰 이유는 GPT-3는 그저 사용자의 말을 이어서 작성하는 능력 밖에 없었기 때문이다.

ChatGPT 는 사용자의 요청 사항을 이해하고 그에 맞춰 답변을 생성하지만, GPT-3는 그렇지 않앗다.
어마어마한 비용이 들지만, 만들 수 있는 가치는 비교적 적었던 GPT-3를 ChatGPT로 바꾼 것은 OpenAI가 Traning Language models to follow instructions with human feedback(사람의 피드백을 통해 언어 모델이 지시를 따드로도록 훈련하기) 이라는 논문의 연구 결과 발표와 함께 공개한 지도 미세 조정과 RLHF(사람의 피드백을 활용한 강화학습)라는 기술이었다.
이 기술을 통해 ChatGPT는 그저 사용자가 한 말 다음에 이어질 말을 생성하는 것이 아니라 사용자의 요청을 해결할 수 있는 텍스트를 생성하게 된다.

LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것을 정렬이라고 한다.
사용자가 LLM 의 답변에서 얻고자 하는 가치를 반영해 LLM을 학습해서 LLM이 사용자에게 도움이 되고 가치를 전달할 수 있도록 하는 것이다.

지도 미세 조정은 정렬을 위한 가장 핵심적인 학습 과정으로서, 언어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋으로 추가 학습하는 것을 뜻한다.
이때 지시 데이터 셋은 사용자가 요청 또는 지시한 사항과 그에 대한 적절한 응답을 정리한 데이터 셋을 의미한다.
OpenAI 는 수많은 데이터를 활용해 지도 미세 조정을 수행했다.
 그 결과 ChatGPT 와 같이 사용자의 요청에 맞춰 응답하는 모델을 만들 수 있었다.

하지만, 사용자의 요청에 맞춰 응답하는 것이 항상 옳은 것은 아닌데, 예를 들어 폭탄이나 약물을 제조하는 방법을 묻는 경우 AI가 정보를 제공해준다면 결국 위험해 질 수 있다.
또한 같은 내용의 답변이라도 사용자가 더 이해하기 쉽게 생성한다거나, 인종, 성별, 등에 차별적인 표현을 사용하지 않는 등 다양한 과점에서 사용자에게 도움이 되도록 노력해야한다

OpenAI에서는 두 가지 답변 중 사용자가 더 선호하는 답변을 선택한 데이터셋을 구축했는데, 이를 선호 데이터셋이라고 한다.
선호 데이터셋으로 LLM의 답변을 평가하는 리워드 모델을 만드록 LLM이 점점 더 높은 점수를 받을 수 있도록 추가 학습을 하는데, 이 때 강화 학습을 사용하기 때문에 이 기술을 일컬어 RLHF라고 불럿다.


## LLM 애플리케이션의 시대가 열린다.

ChatGPT 가 몰고 온 충격 이후 많은 조직에서 LLM을 활용한 애플리케이션을 개발하기 위해 노력하고 있다.
이제 LLM이 우리 생활에 미치는 영향을 살펴봄으로써 LLM이 중요한 이유부터 알아보자
이후의 장에서 LLM을 활용한 애플리케이션을 개발할 때 알고 있어야하는 핵심 개념을 살펴볼텐데 
1장에서는 sLLM과 RAG 을 간략하게 살펴본다.

![Pasted-image-20250523151735.png](/img/이미지 창고/Pasted-image-20250523151735.png)
### 지식 사용법을 획기적으로 바꾼 LLM
LLM이 사회에 큰 영향을 미치고 있는 이유는 다재다능함 때문이다.
LLM의 다재다능함이란 하나의 언어 모델이 다양한 작업에서 뛰어난 능력을 보이는 것을 의미한다.
기존에는 언어에 대해 다루는 AI분야인 자연어 처리 분야를 아래와 같이 크게 언어를 이해하는 자연어 이해
언어를 생성하는 자연어 생성의 두 분야로 나눠서 접근 했다.
![Pasted-image-20250523151937.png](/img/이미지 창고/Pasted-image-20250523151937.png)

또 각각의 영역에서도 일부 좁은 영역의 작업을 해결하기 위해 별도의 모델을 개발하는 방식으로 문제에 접근하는 경우가 많았다.

하지만, LLM경우 언어 이해와 언어 생성 능력 모두 뛰어나다.
처음부터 자연어 생성을 위한 모델이기 때문에, 언어 생성 능력이 뛰어나고, 모델의 크기가 커지면서 언어 추론 능력을 포함해 언어 이해 능력이 크게 높아졌다.

1.2절에서 살펴본 대로 지시 데이터셋으로 사용자의 요청에 응답하는 방식을 학습하면서 다양한 작업에 적절히 응답하는 능력도 갖췄다.
이처럼 LLM은 언어 이해와 생성 두 측면에서 모두에서 뛰어나고, 사용자의 요청에 맞춰 다양한 작업을 수행하는 다재다능함을 가졌다는 점에서 이 전까지의 자연어 처리 모델과 뚜렷이 구분된다.

그렇다면, 다재다능함은 왜 중요한 걸까,
사람이 하는 일 대부분은 언어 이해와 생성을 함께 필요로 하며, 여러 작업이 복합적으로 섞여 있기 때문이다.
예를 들어 아래 그림에 나타낸 개발자라면, 새로운 요구사항을 바탕으로 기존 코드를 이해해서 새로운 코드를 생성해야한다.
![Pasted-image-20250523152358.png](/img/이미지 창고/Pasted-image-20250523152358.png)

도 기술 A와 관련된 문서를 읽고 요약하거나 보고서를 작성하는 일도 한다.
마지막으로, 현재의 상황과 여러 제한 조건을 바탕으로 어떤 판단이나 의사 결정을 내리는 일도 수행해야한다.
세 가지 작업 모두 언어를 이해하는 것 뿐만 아니라, 결과를 언어로 생성해야한다.

기존의 자연어 처리 접근 방식에서는 이렇게 복합적인 작업을 수행하기 위해 아래 그림과 같이 언어 이해 모델과 언어 생성 모델을 각각 개발해 연결했다.

![Pasted-image-20250523152711.png](/img/이미지 창고/Pasted-image-20250523152711.png)
난이도나 복잡도에 따라서는 그림의 아랫부분과 같이 언어 이해에도 여러 개의 모델이 필요하고 언어 생성에도 여러 모델이 필요한 경우도 생긴다.
많은 모델을 개발하고 연결 할 수록 시스템의 복잡도가 높아지고 관리는 어려워진다.

하지만, 다재다능한 LLM을 활용한다면, 아래와 같이 하나의 LLM으로 작업을 수행하도록 만들 수 있다.

![Pasted-image-20250523152847.png](/img/이미지 창고/Pasted-image-20250523152847.png)
복잡도가 훨신 낮기 때문에 더 빠르게 다양한 작업에 AI를 활용할 수 있다는 장점이 있습니다.

LLM의 이런 다재다능함을 활용하면 아래의 그림과 같이 사용자가 LLM통해 검색한 정보가 요약되고 쉽게 해결된 결과를 통해 지식을 비교적 쉽게 습득 할 수있고, 습득한 지식을 더 빠른 시간 안에 새로운 결과물로 조합해 낼 수 있다.
![Pasted-image-20250523153048.png](/img/이미지 창고/Pasted-image-20250523153048.png)
LLM은 우리가 기존에 지식을 습득하고 활용하던 모든 측면에 영향을 줄 수 있기 때문에, 이전의 AI 모델보다 사회에 미치는 영향이 크다.
또는 대부분의 지식 노동자가 수행하는 작업이 언어의 이해와 생성을 모두 포함하기 때문에, 자동화하기 어려웠는데, LLM은 자연어 이해와 성능이 모두 뛰어나기 때문에, 비교적 간단한 작업에서는 사람을 완전히 대체 할 수 있어 많은 기대와 우려를 한번에 받고 있다.

### 더 효율적인 학습과 추론을 위한 기술
LLM의 기반이 되는 트랜스포머 아키텍처 연산은 무겁고 또 모델 성능을 높이기 위해 모델의 크기를 키우면서 LLM의 학습과 추론에 필요한 연산량이 크게 증가했다.
LLM은 많은 연산량을 빠르게 처리하기 위해 다른 딥러닝 모델과 마찬가지로 GPU를 사용한다.
GPU는 많은 연산을 병렬로 처리하는데 특화된 처리 장치인데, 고가의 장비이기 때문에, LLM을 사용하기 위한 비용 중 상당 부분이 GPU 비용에서 발생한다.

비용도 비요이지만, 특히 ChatGPT의 성공 이후 GPU 수요가 급증하면서 돈을 주고도 GPU를 구하지 못하는 품귀 현상도 나타나고 있다.

이런 배경에서 LLM을 학습하고 추론할 때, GPU를 더 효율적으로 사용해 적은 GPU 자원으로도 LLM을 활용할 수 있도록 돕는 연구가 활발히 진행되고 있다.

대표적인 모델 파라미터를 더 적은 비트로 표현하는 양자화 모델 전체를 학습하는 것이 아니라, 모델의 일부만 학습하는 LoRA 방식이 있다.
또한 무거운 어텐션 연산을 개선해 효율적인 학습과 추론을 가능하게 하는 연구도 있다.
이와 같은 연구와 진전은 LLM의 학습과 추론을 더욱 효율적으로 만들어, 적은 수의 GPU로도 높은 성능을 달성할 수 있게 해줬다.

이는 LLM을 사용하는 기업이나 연구자들에게 비용 절감은 물론 GPU의 접근성으 높여주는 중요한 이점을 제공한다.
또한 이러한 연구는 환경적 측면에서도 중요한 의미를 지닌다.
GPU의 효율적 사용은 에너지 소비를 줄이고, 따라서 탄소 발생량을 감소 시키는데도 기여한다.


### LLM의 환각 현상을 대처하는 검색 증강 생성(RAG) 기술
LLM은 매우 강력한 도구이지만, 한 가지 큰 문제가 있는데, 바로 <mark>환각 현상</mark>이라고 불리는 것이다.
이는 LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어 내는 현상을 말한다.

왜 이런 현상이 일어나는지 정확히 알기 어렵지만, 기본적으로 LLM은 학습 데이터를 압축해 그럴듯한 문장을 만들 뿐 어떤 정보가 사실인지, 거짓인지 학습한 적은 없어 특정 정보가 사실인지 판단할 능력은 없다.

또한 학습 데이터를 압축하는 과정에서 비교적 드물게 등장하는 정보는 소실될 텐데, 그런 정보의 소실이 부정확한 정보를 생성하는 원인이 될 수 있다.
OpenAI의 존 슐먼은 2023년 4월 EECS 콜로키움에서 발표한 <mark>사람의 피드백을 활용한 강화 학습 : 발전과 과제</mark>  에서 발지시 데이터셋으로 LLM을 지도 미세 조정하는 과정에서 LLM이 기존에 알지 못하는 정보가 포함된 경우 환각 현상을 유발할 수 있다고 제안했다.

OpenAI의 창립 멤버이기도 한 안드레이 카르파티는 LLM이 텍스트 생성하는 것은 마치 꿈꾸는 것과 비슷하다고 말하기도 했다.
즉, 환각 현상은 LLM만으로는 해결하기 쉽지 않은 치명적인 단점이라고 할 수 있다.

이러한 문제를 줄이기 위해 검색 증강 생성 Retrieavl Augmeneted 이라는 기술을 사용한다.
RAG 기술은 프롬프트에 LLM이 답변할 때 필요한 정보를 미리 추가함으로써 잘못된 정보를 생성하는 문제를 줄인다.

## LLM의 미래: 인식과 행동의 확장
앞으로 LLM은 어떤 방향으로 발전할까?

LLM은 AI 분야에서 가장 뜨거운 연구 분야이고, 그 만큼 다양한 방향으로 연구가 진행되고 있어, 먼 미래의 방향을 예측하기는 쉽지 않다.
LLM을 통해 텍스트에서 생성 모델의 위력을 실감한 이후 LLM의 능력을 확장해 더 다양한 작업을 수행할 수 있도록 빠르게 움직이고 있다.

대표적으로 아래 그림과 같이 세가지 큰 흐름이 있다.
먼저 LLM이 더 다양한 형식의 데이터를 입력으로 받을 수 있고 출력으로도 여러 형태의 데이터를 생성할 수 있도록 발전시킨 멀티 모달 LLM이 있다.
또 LLM이 텍스트 생성 능력을 사용해 계획을 세우거나 의사 결정을 내리고 필요한 행동까지 수행하는 에이전트 연구도 활발히 진행되고 있다.
LLM이 사용하는 트랜스포머 아키텍처를 새로운 아키텍처로 변경해 더 긴 입력을 효율적으로 처리하려는 연구도 주목받고 있다.
![Pasted-image-20250524194449.png](/img/이미지 창고/Pasted-image-20250524194449.png)

오디오와 비디오 같은 데이터는 텍스트에 비해 입력이 훨씬 긴데, LLM이 더 긴 입력을 처리할 수 있게 되면 오디오와 비디오 데이터를 처리하는 능력도 향상될 것이다.

새로운 아키텍처에 대해서는 1.2.1절에서 이미 살펴봤기 때문에, 이번 절에서는 멀티 모달 LLM과 에이전트에 대해 알아본다.

멀티 모달 모델이란, 아래 그림의 왼쪽 박스와 같이 다양한 형태의 입력을 받을 수 있는 LLM을 말한다.
LLM은 기본적으로 텍스트를 입력으로 받고 텍스트를 출력으로 내보내기 때문에, 다양한 사용 사례를 해결하는데 한계가 있다.
대표적으로 이미지에 대한 정보는 LLM은 기본적으로 텍스트를 입력으로 받고 텍스트를 출력으로 내보내기 때문에, 다양한 사용 사례를 해결하는 데 한계가 있다.
대표적으로 이미지에 대한 정보는 LLM이 처리하기가 꽤 까다로운데, 이런 문제는 해결하기 위해 멀티 모달 모델에 대한 연구가 활발히 진행되고 있다.

2024년 5월에 공개된 GPT-4o는 뛰어난 이미지 및 음성 처리 능력을 보여줬고, 구글의 제미나이, 엔트로픽의 클로드 같은 상업용 모델도 이미지를 함께 처리할 수 있다.
여러 오픈소스 멀티 모달 모델이 등장하고 있으며, RAG에서도 이미지와 텍스트를 함께 검색하는 멀티 모달 RAG 에 대한 관심이 높아지고 있다.

![Pasted-image-20250524195105.png](/img/이미지 창고/Pasted-image-20250524195105.png)

LLM을 단순히 텍스트를 생성하는 기능 외에 스스로 판단하고 행동하는 에이전트의 두뇌 로 사용하려는 시도도 늘고 있다. LLM은 뛰어난 언어 이해 및 추론 능력을 갖추고 있기 때문에, LLM을 활용하면 주어진 상황을 인식하고 필요한 행동을 계획해 직접 수행하게 만들 수 있다.
2023년 LLM을 핵심 엔진으로 사용하는 자동화 시스템인 AutoGPT가 공개되면서 LLM 기반 에이전트의 가능성에 대한 기대가 커졌고, 최근에는 마이크로소프트의 AutoGen, CrewAI 등 여러 개의 에이전트를 활용해 문제를 해결하는 멀티 에이전트 프레임워크도 많은 인기를 얻고 있다.

# LLM의 중추, 트랜스포머 아키텍처 살펴보기기

LLM은 모델 크기가 큰 딥러닝 기반의 언어 모델로 2024년 현재 대부분의 LLM이 트랜스포머 아키텍처를 기반으로 한다.
따라서 트랜스포머 아키텍처를 이해하지 않고는 LLM과 관련된 기술을 정확히 이해하기 어렵다.
그래서 이번 장에서는 트랜스포머 아키텍처를 코드 레벨에서 직접 구현해 보면서 세부적인 동작을 이해해 본다.
새로운 개념을 접할 때는 직관적인 이해와 세부적인 동작 이해가 모두 필요한데, 트랜스포머 아키텍처는 처음 접했을 때, 직관적으로 이해하기 어렵고 복잡도도 높다.
이어지는 절에서 트랜스포머를 이해할 수 있는 직관적인 해석을 가급적 많이 제시하고 코드 레벨에서 구현하여 혼란을 줄이면서 세부 동작을 이해할 수 있도록 도울 것이다.

이번 장에서는 먼저 기본에 자연어 처리에 사용하던 순환신경망과의 비교를 통해 트랜스포머 아키텍처를 구성하는 요소를 살펴본다.
트랜스포머 아키텍처는 언어 이해 하는 인코더와 언어를 생성하는 디코더 부분으로 나뉘는데, 자연어 처리 모델은 이중 어떤 부분을 사용하느냐에 따라 세 가지 그룹으로 나눌 수 있다.

모델을 세 가지 그룹으로 묶어 살펴봄으로써 다양한 모델을 효과적으로 이해하는 틀을 제공한다.
마지막으로, 트랜스포머 아키텍처를 활용한 모델을 학습시키는 주요 학습 방법을 소개한다.

해당 실제 [소스코드](https://github.com/onlybooks/llm)는 아래에 있다.

## 트랜스포머 아키텍처란
트랜스포머 아키텍처는 2017년 구글의 아쉬쉬 바스와니 외 7인이 발표한 논문에서 처음 등장했다.
이 논문에서는 머신러닝을 통해 언어를 번역하는 기계 번영 성능을 높이기 위한 방법을 연구했는데, 이 방법은 당시 널리 사용되던 RNN에 비해 성능 면에서 큰 폭으로 앞섰다.

또한 트랜스포머는 RNN에 비해 성능만 높은 것이 아니라 모델 학습 속도도 빨랐다.
완전히 새로운 형태의 모델이 성능과 속도 면에서 뛰어는 모습을 보이자 많은 인공지능 연구자들이 각자의 연구에 트랜스포머를 적용하기 시작했다.

현재 트랜스포머는 자연어 처리는 물론 컴퓨터 비전, 추천 시스템 등 모든 AI 분야에서 핵심 아키텍처로 사용되고 있다.

기존에 자연어 처리 문제에서 사용하던 RNN은 아래 그림과 같이 텍스트를 순차적으로 하나씩 입력하는 형태다
![Pasted-image-20250524201008.png](/img/이미지 창고/Pasted-image-20250524201008.png)
사람이 글을 읽을 때 왼쪽에서 오른쪽으로 차례대로 읽는 것처럼 딥러닝 모델에 텍스트를 순차적으로 넣어준 것이다.

위 그림에서 x는 텍스트 토큰을 의미한다.
토큰은 거의 모든 자연어 처리 연산의 기본 단위 이고, 보통 단어보다 짧은 텍스트 단위 이다.
하지만 지금은 단어와 같은 의미라고 생각하자 
h는 입력 토큰을 RNN 모델에 입력으로 사용하기 때문에 학습 속도가 느리고, 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서 성능이 떨어진다는 문제가 있었다.
또한 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실이나 그레이디언트 증폭이 발생하면서 학습이 불안정했다.

트랜스포머는 이런 RNN의 문제를 해결하기 위해 입력을 하나씩 순차적으로 처리하는 방식을 버리고 셀프 어텐션 이라는 개념을 도입했다.

셀프 어텐션은 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정하는 역할을 한다.
트랜스포머 아키텍처는 RNN에 비해 다음과 같은 장점을 보였다.

- 확장성
	- 더 싶은 모델을 만들어도 학습이 잘된다.
	- 동일한 블록을 반복해 사용하기 때문에 확장이 용이하다.
- 효율성
	- 학습할 때 병렬 연산이 가능하기 때문에, 학습 시간이 단축된다.
- 더 긴 입력 처리
	- 입력이 길어져도 성능이 거의 떨어지지 않는다.

현재 사용되는 대부분의 LLM은 트랜스포머 아키텍처를 활용하고 있고, 더 정확히 말하자면 트랜스포머 아키텍처가 있었기에, LLM이 가능했다고 할 수 있다.

어떤 원리로 작동하길래 성능과 속도를 모두 잡을 수 있었을까?
아래 그림과 같은 요소로 구성돼 있다.

트랜스포머 아키텍처는 아래 그림만 이해하면 모두 알았다고 봐도 될 정도로 아래 그림이 중요하다
아래 그림의 각 구성요소와 관계는 이 장에 전체를 걸쳐 자세히 알아볼 것이므로, 지금은 전체 구조와 각 층의 이름을 눈에 익혀보는 정도로 이해하고 넘어가도 충분하다


![IMG-3843.jpeg](/img/이미지 창고/IMG-3843.jpeg)

영어를 한국어로 번역한다고 가정하고 위 그림의 화살표를 따라 트랜스포머 아키텍처를 살펴보자
트랜스포머 아키텍처는 크게 인코더와 디코더로 나뉜다.
그림의 왼쪽 상자는 언어를 이해하는 역할을 하는 인코더이고, 오른쪽 상자는 언어를 생성하는 역할을 하는 디코더다.

공통적으로 입력을 임베딩 층을 통해 숫자 집합인 임베딩으로 변환하고 위치 인코딩 층에서 문장의 위치 정보를 더한다.
인코더에서는 층 정규화 멀티 헤드 어텐션, 피드 포워드 층을 거치며 영어 문장을 이해하고 그 결과를 그림 중간의 선에 나타나듯이 디코더로 전달한다.

디코더에서 인코더에서와 유사하게 층 정규화, 멀티 헤드 어텐션 연산과 수행하면서 크로스 어첸션 연산을 통해 인코더가 전달한 데이터를 출력과 함게 종합해서 프드백 포워드 층을 환국어 번역 결과를 생성한다.

이 장의 나머지 부분에서도 위 그림의 트랜스포머 아키텍처를 하나씩 자세히 살펴본다.
먼저 트랜스포머의 핵심인 어텐션 연산을 살펴보고, 층 정규화의 피드 포워드 층을 알아본다.
다음으로 세가지 층으 ㄹ종합해 구성되는 인코더를 알아보고 마지막으로 디코더를 살펴본다.

## 텍스트 임베딩으로 변환하기
컴퓨터는 텍스트를 그대로 계산에 사용할 수 없다.
따라서 텍스트를 숫자형식의 데이터로 변경해야 한다.

텍스트를 모델에 입력할 수 있는 숫자형 데이터인 임베딩으로 변환하기 위해서는 아래 그림과 같이 크게 세가지 과정을 거쳐야 한다.
먼저 텍스트를 적절한 단위로 잘라 숫자형 아이디를 부여하는 토론화를 수행한다.
다음으로 토큰 아이디를 토큰 임베딩 층을 통해 여러 숫자의 집합인 토큰 임베딩으로 변환한다.

마지막으로 위치 인코딩 층을 통해 토큰이 위치 정보를 담고 있는 위치 임베딩을 추가해 최종적으로 모델에 입력할 임베딩을 만든다. 

![Pasted-image-20250525124625.png](/img/이미지 창고/Pasted-image-20250525124625.png)

이번절에서는 세 가지 단계에서 어떤 일이 일어나는지 살펴봄으로 써 입력 텍스트를 숫자형 데이터로 변환하는 방법을 알아본다.

### 토론화
토큰화란 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것을 말한다.
한글은 아래 그림과 같이 작게는 자모 다누이부터 크게는 단어 단위로 나눌 수 있다.

음절은 중간 정도의 단위로 볼 수 있다.
토큰화를 할 때 어떤 토큰이 어던 숫자 아이디로 연결됐는지 기록해둔 사전을 만들어야 한다
예를 들어, 단어를 단위로 토큰화 하는 경우 어떤 단어를 몇번으로 변환했는지 모두 저장한다.
큰 단위를 기준으로 토큰화 할 수록 텍스트이 의미를 잘 유지된다는 장점이 있지만, 사전의 크기가 커진다는 단점이 있다.

단어로 토큰화를 하는 경우 텍스트에 등장하는 단어의 수만큼 토큰 아이디가 필요하기 때문에, 사전이 커진다.
또한 이전에 본 적이 없는 새로운 단어를 사전에 없기 때문에, 처리하지 못하는 OOV 문제가 자주 발생한다.

![Pasted-image-20250525125043.png](/img/이미지 창고/Pasted-image-20250525125043.png)

반대로 작은 단위로 토큰화하는 경우 사전의 크기가 작고 OOV 문제를 줄일 수 있지만, 텍스트의 의미가 유지되지 않는다는 단점이 있다.
아래 그림에서 단어 '파리'를 자모 단위로 나누는 경우 'ㅍ. ㅏ. ㄹ, ㅣ' 로 나뉘는데, 이렇게 나누면 파리의 의미가 거의 사라진다.

작은 단위와 큰 단위 모두 각각의 장단점이 뚜력하기 때문에 최근에는 아래 그림과 같이 데이터에 등장하는 빈도에 따라 토큰화 단ㅇ뉘로 결정하는 서브워드 토큰화 방식을 사용한다.
서브워드 토큰화 방식에서는 자주 나오는 단어는 단어 단위 그대로 유지하고 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 최대한 유지하면서 사전의 크기는 작고 효율적으로 유지할 수 있다.

예를 들어, 인사말로 자주 사용하는 '안녕' 이나 '대한민국' 같은 국가 이름, 유명한 사람 과의 이름과 같이 자주나오는 단어 형태를 유지하지만, 한글 데이터에서 비교적 드물게 등장하는 외국어나 특수 문자, 이모티콘 등은 작게 나눠 사전 크기가 커지지 않도록 한다.
한글의 경우 보통 음절과 단어 사이에서 토큰화 된다.
![Pasted-image-20250525125611.png](/img/이미지 창고/Pasted-image-20250525125611.png)

텍스트를 숫자 아이디로 바꾸는 과정은 아래 예제와 같은 코드로 수행할 수 있다.
최근의 토큰화는 서브워드 토큰화를 수행했다.
앞으로 등장하는 예시나 예제에서도 표현의 편의상 단어 단위로 토큰화환 경우가 있을 수 있는데 실제로는 서브워드 토큰화를 활용한다는 점은 변함이 없으니, 혼동이 없길 바란다.

이 코드에서는 나는 최근 파리 여행을 다녀왔다. 라는 문장을 띄워쓰기 단위로 분리하고 각각의 토큰에 0부터 토큰 아이디를 부여해 `str2idx` 딕셔너리에 저장 한다.

마지막으로, 각각ㄱ의 토큰을 `str2idx` 딕셔터리로 토크큰을 아이디로 변환해서 `input_ids` 에 저장한다.

```python
input_text = "나는 최근 파리 여행을 다녀왔다."
input_text_list = input_text.split()
print("input_text_list : ", input_text_list)

# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기

str2idx = { word: idx for idx, word in enumerate(input_text_list)}
str2str = { idx: word for idx, word in enumerate(input_text_list)}
print("str2idx : ", str2idx)
print("idx2str : ", idx2str)

# 토큰을 토큰 아이디로 변환
input_ids = [str2idx[word] for word in input_text_list]
print("input_ids: ", input_ids)

# 출력 결과
# input_text_list : ['나는', '최근', '파리', '여행을', '다녀왔다.']
# str2idx : { '나는' : 0, '최근' : 1, '파리': 2, '여행을': 3, '다녀왔다.': 4}
# idx2str : {  0 : '나는', 1 : '최근', 2 : '파리', 3 : '여행을', 4 : '다녀왔다.'}
# input_ids : [0,1,2,3,4]
```


### 토큰 임베딩으로 변환하기
딥러닝 모델이 텍스트 데이터를 처리하기 위해서는 입력으로 들어오는 토큰과 토큰 사이의 관계를 계산할 수 있어야 한다.
토큰과 토큰 사이의 관계를 계산하기 위해서는 토큰의 의미를 숫자로 나타낼 수 있어야하는데, 앞서 토큰화에서 부여한 토큰 아이디는 하나의 숫자일 뿐이므로 토큰의 의미를 담을 수 없다.

의미를 담기 위해서는 최소 2개 이상의 숫자 집합 벡터여야 한다.
1장에서 자세히 설명한 대로 데이터를 의미를 담아 숫자 집합으로 변환하는 것을 임베딩이라고 한다.
여기서는 토큰을 입베딩으로 변환하기 때문에 토큰 임베딩이라고 부른다.

그렇다면 토큰을 어떻게 토큰 임베딩으로 변환할 수 있을까?

아래 예제와 가팅 파이토치가 제공하는 `nn.Embedding` 클래스를 사용하면 토큰 아이디를 토큰 임베딩으로 변환할 수 잇다.
이 코드에서는 `nn.Embedding` 클래스에 사전 크기가 `len(str2idx)(=5)`이고 `embedding_dim` 차원의 임베딩을 생성하는 임베딩 층인 `embed_layer`를 만들고 입력 토큰을 임베딩 층을 통해 임베딩으로 변환햇다.

`embedding_dim`을 16으로 설정해 토큰 하나를 16차원의 벡터로 변환한다.
출력 결과를 보면 1개의 문장이고 5개의 토큰이 있고 16차원의 임베딩이 생성됐을 을 확인 할 수 있다.

```python
embedding_dim = 16
embed_layer = nn.Embedding(len(str2idx), embedding_dim)

input_embeddings = embed_layer(torch.tensor(input_ids)) # (5,16)
input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16)
input_embeddings.shape

# 출력 결과
# torch.Size([1,5,16])
```

그렇다면 위의 코드에서 임베딩 층(`embed_layer`) 은 토큰의 의미를 담아 벡터로 변환하는 것일까?
아직 아니다. 지금의 임베딩 층은 그저 입력 토큰 아이디(`input_ids`)를 16차원의 임의의 숫자 집합으로 바꿔줄 뿐 이다.
임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 데이터로 훈련되어있어야 한다.

바로 이 지점에서 딥러닝이 기존 머신러닝과 차별화 되는데 딥러닝에서 모델이 특정 작업도 함께 학습한다.
예를 들어 아래 그림과 같이 입력 텍스트가 속한 카테고리를 맞추는 텍스트 분류모델이 있다고 하자

딥러닝 모델은 학습 데이터를 통해 이 그림과 같이 하는 "최근 파리 여행을 다녀왔다." 라는 문자엥 맞는 카테고리로 잘 분류하도록 학습된다.
그 과정에서 딥러닝 모델의 첫 번째 단계인 임베딩 층도 학습되면서 점차 토큰의 의미를 잘 담은 임베딩을 생성하게 된다.


![Pasted-image-20250603201439.png](/img/이미지 창고/Pasted-image-20250603201439.png)


### 위치 인코딩
RRN 과 트랜스포머의 가장 큰 차이점은 입력을 순차적으로 처리하는지 여부다.
RNN은 입력을 순차적으로 처리하는데, 그렇기 때문에, 자연스럽게 입력 데이터의 순서 정보가 고려된다.

트랜스포머는 순차적인 처리 방식을 버리고 모든 입력을 동시에 처리하는데 그 과정에서 순서 정보는 사라지게 된다.
하지만, 텍스트에서 순서는 매우 중요한 정보이기 때문에, 추가 해줘야 하는데, 그 역할을 위치 인코딩이 담당한다.

Attenstion is All you need 논문에서는 사인 코사인을 활용한 수식을 통해 위치에 대한 정보를 입력했다.
하지만 그 이후에는 위치 인코딩도 위치에 따른 임베딩 층을 추가해 학습 데이터를 통해 학습하는 방식을 많이 활용하고 있다.

수식을 통해 위치 정보를 추가하는 방식이나 임베딩으로 위치 정보를 학습 하는 방식 모두 결국 모델로 추론을 수행하는 시점에서 입력 토큰의 위치에 따라 고정된 임베딩을 더해주기 때문에, 이를 절대적 위치 인코딩(absolute position encodeing)이라고 부른다.

절대적 위치 인코딩 방식은 간단하게 구현할 수 있다는 장점이 있지만, 토큰과 토큰 사이의 상대적인 위치 정보는 활용하지 못하고 학습 데이터에서 보기 어려웠던 긴 텍스트를 추론하는 경우에는 성능이 떨어진다는 문제가 있어, 최근에는 상대적인 위치 인코딩 방식(relative postion encoding) 방식도 많이 활용된다.
지금은 트랜스포머가 모든 입력 토큰을 동등하게 처리하기 때문에, 입력으로 위치 정보를 함게 더 해준다는 사실만 기억하면 충분하다.

절대적 위치 인코딩 중 위치 정보를 학습하는 방식을 코드로는 아래 예제와 같이 새로운 임베딩 층을 하나 추가하고 위치 인덱스(`position_ids`)에 따라 임베딩을 더하도록 구현할 수 있다.

이 코드에서는 최대 토큰 수(`max_position`)를 12로 설정하여 위치 인코딩을 생성하는 위치 임베딩 층(`position_embed_layer`)을 정의했다.

위치 아이디(`position_ids`)에는 0부터 입력 토큰의 수(`input_ids.size(1)`) 까지 1씩 증가하도록 데이터를 생성한다.
`position_ids` 를 위치 임베딩 층에 입력해 위치 인코딩(position_encodeings)을 생성하고 토큰 임베딩(`token_embeddings`)을 준비한다.

```python
embedding_dim = 16
max_position = 12
embed_layer = nn.Embedding(len(str2idx), embedding_dim)
position_embed_layer = nn.Embedding(max_postion, embedding_dim)

position_ids = torch.anrange(len(input_ids), dtype=torch.long).unsqueeze(0)
position_encoding = position_embed_layer(position_ids)
token_embeddings = embed_layer(torch.tensor(input_ids)) # (5,16)
token_embeddings = token_embeddings.unsqueeze(0) # (1, 5, 16)
inputs_embeddings = token_embeddings + position_emcodings
input_embeddings.shape

# 출력 결과
# torch.Size([1, 5, 16])
```

이번 절에서 살펴본 세 가지 과정을 그림으로 다시 정리한다면 아래와 같다.
![Pasted-image-20250603204135.png](/img/이미지 창고/Pasted-image-20250603204135.png)
먼저 "나는 최근 파리 여행을 다녀왔다" 라는 문장을 단어 단위로 토큰화를 수행하고 각 토큰에 토큰 아이디와 위치 아이디를 부여했다
이때 그림에서 토큰 아이디와 위치 아이디 모두 `[0,1,2,3,4]`의 값을 갖는데, 값은 동일하지만, 토큰 아이디는 사전(`str2idx`) 에 저장된 토큰의 고유한 아이디를 의미하고 위치 아이디는 토큰의 위치를 의미힌다.

예시 데이터이기에 같게 표현된 것이고 일반적으로 같진 않다.
하나의 숫자로 된 숫자 아이디는 데이터의 의미를 담을 수 없기 때문에, 의미를 담을 수 있도록 토큰 아이디와 위치 아이디를 토큰 임베딩 층과 위치 인코딩 층을 통해 토큰 임베딩과 위치 임베딩으로 변환한다.

1장에서 설명한대로 그림에서 첫번재 토큰 임베딩의 0.2, 0.3과 같은 개별 수는 의미를 해석하기 어렵지만, 숫자의 모임인 `[0.2, 0.3, ... ,0.9, 0.12]`가 나는 이라는 토큰의 의미를 담게 된다.

마친가지로 위치 임베딩에서 0.3, 0.1과 같은 개별 수의 의미를 해석하기는 어렵지만 `[0.3,0.1, ..., 0.8, 0.2]` 전체가 첫 번째 토큰 위치라는 의미를 담고 있다.

## 어탠션 이해하기
트랜스포머 아키텍처의 핵심은 논문의 제목 Attention is All you need에서도 알 수 있는듯이 어텐션이다.
어텐션의 사전적 의미는 주의 라고 번역할 수 있느넫, 텍스트를 처리하는 관점에서 입력한 텍스트에서 어떤 단어가 서로 관련되는지, 주의를 기울여 파악한다는 의미로 이해할 수 있다.
어떻게 하면 딥러닝 모델이 관련 있는 단어를 찾도록 만들 수 있을까?

이번절에서는 사람이 글을 읽는 방법을 들여다 보며 어텐션을 직관적으로 이해할 수 있는 방법을 알아보고, 어텐션의 핵심 새념인 쿼리, 키, 값 에 대해 살펴본 후, 코드 레벨에서 어텐션을 구현해본다.

### 사람이 글을 읽는 방법과 어텐션
사람이 글을 읽을 때 생각해보면, 쉬운 글을 읽을 때는 왼쪽에서 오른쪽으로 흐르듯이 읽지만, 복잡하고 어려운 글을 읽을 때는 자주 멈추면서 어떤 단어가 어떤 단어와 연결되는지 문장 안에서 고민하기도 하고 문장 간에 찾아보기도 한다.
어텐션은 이렇게 사람이 단어 사이의 관계를 고민하는 과정을 딥러닝 모데링 수행할 수 있도록 모방한 연산이다.
어텐션 작동 방식을 직관적으로 이해하기 먼저 사람이 단어 사이의 관계를 확인하는 방법을 살펴보자

다음 문장을 봤을 때, 파리가 도시인지 곤충인지 알 수 있겠는가?

- 00 00 파리000 000
	- 파리의 앞뒤 단어가 가려져 있기 때문에, 파리가 도시를 말하는지 곤충을 말하는지 판단할 수 없다.
	- 하지만, 다음 문장에서는 파리가 도시라는 사실을 명확히 알 수 없다.
- 나는 최근 파리 여행을 다녀왔다.

왜 첫 문장에서는 알 수 없고, 두번째 문장에서는 알 수 있었을까?
사람은 단어를 그 단어만으로 해석하지 않고, 주변의 단어를 통해 맥락을 추가한 새로운 단어를 이해하기 때문이다.
그 과정을 그림으로 나타내면 아래 그림과 같다.
![Pasted-image-20250603211807.png](/img/이미지 창고/Pasted-image-20250603211807.png)
파리 주변에 있는 여행과 다녀왔다 라는 맥락을 더해서 새롭게 프랑스의 수도인 파리로 이해하낟.
어텐션으로 이 과정으로 바라보면 파리와 관련이 깊은 여행을 과 다녀왔다 에 주의를 기울여 파리가 도시라고 해석한 것이다.

사람이 자연스럽게 관련이 있는 단어를 찾고 그 맥락을 반영해 단어를 재 해석하는 것처럼 어텐션 연산을 만들려면 어떻게 해야할까?
먼저, 단어와 단어 사이의 관계를 계산해서 그 값에 따라 관련이 깊은 단어와 그렇지 않은 단어를 구분할 수 있어야 한다.
다음으로, 관련이 깊은 단어는 더 많이, 관련이 적은 단어는 더 적게 맥락을 반영해야 한다.
이어지는 절에서는 단어와 단어 사이의 관계를 계산하는 방법을 알아보고, 관계에 따라 맥락을 반영하는 방법을 알아본다.

### 쿼리, 키, 값 이해하기

앞서 우리는 사람이 글을 이해하는 것처럼 딥러닝 모델이 작동하도록 하려면 단어 사이 의 관계를 계산해 관련이 있는지 찾고, 관련이 있는 단어의 맥락을 포함시켜 단어를 재 해석해야 한다고 정리했다. 트랜스포머 아키텍처를 개발한 연구진은 이 과정을 처리하 기 위해 쿼리, 키, 값이라는 개념을 도입했다. 쿼리, 키, 값은 정보 검색(intormation retrieval 분야에서 가져온 용어다. 흔히 검색창에서 검색을 할 때, 우리가 입력하는 검색어를 쿼리 query라고 한다. 쿼리를 입력하고 엔터 키를 치면, 검색 엔진은 수많은 자료 중에 쿼리와 관련이 있는 문서를 찾는다. 이때 쿼리와 관련이 있는지 계산하기 위해 문서가 가진 특 징을 키Key라고 한다. 키로는 문서의 제목, 문서의 본문, 저자 이름 등이 사용될 수 있다.

검색 엔진이 쿼리와 관련이 깊은 키를 가진 문서를 찾아 관련도순으로 정렬해서 문서를 제공할 때 문서를 값value이라고 할 수 있다. 우리가 검색하면서 원하는 것은 '값이다.

쿼리, 키, 값을 우리가 하고자 하는 관련 있는 단어 찾기에 대입해 보면 그림 2.9와 같 다. 예시 문장에서 우리는 '파리'와 관련이 있는 단어를 찾고자 하기 때문에 이때 쿼리 는 '파리'다. 
쿼리와 관련이 있는 단어를 찾을 때 키는 문장 속의 각 단어다. 

쿼리와 관련 이 있는 키를 찾은 경우 그 키에 연결된 값을 반환해야 하는데, 이때 값을 키의 토큰 임 베딩으로 뒀다. 

이렇게 뒀을 때 '파리'라는 쿼리로 `['나는', '최근', '파리', 여행을', '다녀 왔다']` 라는 키 묶음에서 관련이 있는 키를 찾았을 때, 그림 오른쪽과 같이 '파리'와 '여 행을', '다녀왔다'가 적절히 섞인 값이 된다면, 사람이 단어를 재해석하는 과정을 모방할 수 있다. 

이번 절에서는 이렇게 모방하기 위한 쿼리, 키, 값을 찾아나갈 것이다.

![Screenshot-2025-06-04-at-7.48.49-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-7.48.49-PM.png)

가장 간단하게 주변 단어의 맥락을 반영하는 방법에는 그림 2.10과 같이 단어를 모두 동등하게 반영하는 방식이 있다. 

이를 '평균' 방법이라고 하자. 평균 방법은 그림 2.10에 서처럼 모든 단어를 1씩 동등하게 반영하기 때문에 그 결과 그림 오른쪽과 같이 '파리' 라는 단어가 `['나는', '최근', '파리', '여행을', '다녀왔다']`를 모두 같은 비율로 갖게 된다.

이러면 단어를 재해석한다는 목표는 달성하지만, 관련이 깊은 단어를 더 많이 반영 한다는 목표는 달성하지 못한다. 또 모든 단어에 평균 방법을 사용할 경우 모두 같은 값 을 갖게 된다는 문제도 있다.


![Screenshot-2025-06-04-at-7.49.58-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-7.49.58-PM.png)

이와 같은 평균 방법이 너무 단순하다면, '가까이 있는 단어가 더 관련이 깊을 것'이라는 가정을 세우고 자기 자신에게 가장 높은 가중치를 주고 멀어질수록 가중치를 낮추는 방 식으로 그림 2.11과 같이 의미를 반영할 수도 있다. 
그림 2.11에서 '파리'는 세 번째 자 리에 있기 때문에 가장 가까운 자기 자신과는 3만큼 관계가 있고, 한 칸 떨어져 있는 '최 근'과 여행을'은 2, 두 칸 떨어져 있는 나는'과 다녀왔다'는 1 만큼 관계가 있다고 가정 했다. 

경험적으로 봤을 때 글에서 가까운 곳에 있는 단어일수록 관계가 있는 경우가 많 기 때문에 이 방식으로 '파리'를 재해석했을 때, 평균 방식보다는 더 나은 결과를 기대할 수 있다. 

하지만 여전히 우리가 원하는 그림 2.9의 결과와는 차이가 큰 것을 확인할 수 있다.

![Screenshot-2025-06-04-at-7.50.36-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-7.50.36-PM.png)

앞서 살펴본 두 가지 방식은 모두 쿼리와 키의 관련도를 계산하지 않고 모두 동등하게 반영한다거나 거리에 따라 차등을 둔다는 적당한 가정을 세워 맥락을 반영했다. 
그 결과 원하던 형태의 결과를 얻지 못했고 이렇게 규칙에 기반한 방법은 입력 데이터가 "나 는 최근 파리 여행을 다녀왔다"가 아니라, "나는 최근 파리 박물관을 다녀왔다'로 바뀌 더라도 동일한 계산을 하기 때문에 유연성이 떨어진다. 

입력 데이터에 따라 다른 결과 를 얻기 위해서는 관련도를 규칙이 아니라 데이터 자체에서 계산할 수 있어야 한다.

그렇다면 이제 '파리'와 `['나는', '최근', '파리', 여행을', '다녀왔다']`라는 키 집합의 관 계를 계산해 보자. 

한 가지 문제는 문자열은 그 자체로 계산할 수 없다는 점이다. 
하지만 2.1절에서는 토큰을 의미를 담아 숫자형 데이터로 변환하는 토큰 임베딩에 대해 다뤘다. 

따라서 쿼리와 키 토큰을 토큰 임베딩으로 변환하면 계산이 가능하다. 
그림 2.12 에서는 쿼리를 '파리'의 임베딩으로, 키를 각 토큰의 임베딩으로 두고 각각의 관계를 계 산해서 관련도를 계산한다.
이때 토큰 자체가 아니라 임베딩임을 나타내기 위해 토큰을 사각형 박스로 감싼 형태로 표현했다. 

벡터와 벡터를 곱해 관계를 계산하면 그 관련도 에 따라 주변 맥락을 반영할 수 있고, 문자열이 일치하지 않더라도 유사한 의미의 키로 저장된 정보를 검색할 수 있다.

하지만 그림 2.12와 같이 임베딩을 직접 활용해 관련도를 계산하는 방식은 두 가지 문 제가 발생할 수 있다. 

먼저, 같은 단어(예: 쿼리의 '파리'와 키의 '파리'끼리는 임베딩이 동 일하므로 관련도가 크게 계산되면서 주변 맥락을 충분히 반영하지 못하는 경우가 발생 할 수 있다. 

또한 토큰의 의미가 유사하거나 반대되는 경우처럼 직접적인 관련성을 띨 때는 잘 작동하지만 문법에 의거해 토큰이 이어지는 경우처럼 간접적인 관련성은 반영되기 어려울 수 있다. 

예를 들어 우리의 예시 문장에서 '나는' 토큰과 '최근' 토큰은 '다 녀왔다. 토큰에 누가, 언제를 나타내는 문법 관계로 연결되지만 토큰 자체로 봤을 때는 서로 유사하거나 반대되는 경우가 아니므로 직접 계산해서는 관련성을 찾기 어렵다

![Screenshot-2025-06-04-at-7.51.48-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-7.51.48-PM.png)

트랜스포머 아키텍처에서는 이와 같은 문제를 피하기 위해 그림 2.13과 같이 토큰 임베 딩을 변환하는 가중치 $Wo Wx$를 도입했다. 

딥러닝에서는 어떤 기능을 잘하게 하고 싶 을 때 가중치를 도입하고 학습 단계에서 업데이트되게 한다. 트랜스포머에서는$Wo Wx$

가중치를 통해 토큰과 토큰 사이의 관계를 계산하는 능력을 학습시킨 것이다. 그림 2.13 에서는 가중치를 통해 토큰 임베딩을 변환한 퀴리와 키를 각각 q, k로 표현 했다.

![Screenshot-2025-06-04-at-7.53.01-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-7.53.01-PM.png)

지금까지 트랜스포머에서 쿼리와 키를 통해 관계를 계산하는 과정을 '검색'이라는 관점 에서 이해해 봤다. 트랜스포머에서는 그림 2.14와 같이 값value도 토큰 임베딩을 가중치 $Wv$
를 통해 변환한다. 

이렇게 세 가지 가중치를 통해 내부적으로 토큰과 토큰 사이의 관계를 계산해서 적절히 주변 맥락을 반영하는 방법을 학습한다. 
쿼리와 키의 관계를 계산한 관련도 값과 토큰 임베딩을 값 가중치(W)로 변환한 값을 가중합 하면 이 절 을 시작하면서 우리가 구하고자 했던 '파리'를 재해석한 결과를 얻을 수 있다.

![Screenshot-2025-06-04-at-7.56.33-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-7.56.33-PM.png)


### 코드로 보는 어텐션

이전 절에서는 토큰 임베딩을 그대로 관계 계산에 사용하지 않고 쿼리, 키, 값 가중치를 도입하는 이유를 알아보고, 쿼리, 키, 값을 활용해 사람이 단어를 재해석하는 과정을 모방하는 연산을 그림을 통해 살펴봤다. 

그림 2.14에서 살펴본 방식은 트랜스포머 아키텍 처의 핵심인 어텐션 연산과 거의 동일하다. 
이번에는 코드로 어텐션 연산을 구현해 보 면서 그림 2.14의 계산 과정을 더 명확히 이해해 보자.

그림 2.14의 가중치를 파이토치에서 제공하는 `nn.Linear` 층을 사용해 코드로 구현할 수 도 있다(예제 2.4 참고). 

쿼리, 키, 값에 대한 각각의 가중치를 `weight_q`, `weight_k`, `weight_v`로 생성하고 입력으로 준비한 `input_embedding`을 선형 층에 통과시켜 쿼리, 키, 값을 생성한다.

```python
head_dim = 16
# 쿼리, 키, 값을 계산하기 위한 변환
weight_q = nn. Linear (embedding_dim, head_dim)
weight_k = nn. Linear (embedding_dim, head_dim)
weight_v = nn. Linear (embedding_dim, head_dim)

# 변환 수행
querys = weight_q( input_embeddings) # (1, 5, 16)
keys = weight_k(input_embeddings) # (1, 5, 16)
values = weight_v input_embeddings) # (1, 5, 16)
```

이제 선형 층을 통해 만든 퀴리, 키, 값으로 관계를 계산하는 어텐션 연산을 구현해 보자.
어텐션에는 여러 방식이 있는데, 예제 2.5에서는 「Attention is Al you need』 논문 에서 처음 사용된 스케일 점곱 방식을 사용했다. 
1. 단계별로 설명하면 먼저 쿼리와 키를 곱한다. 
2. 이때 분산이 커지는 것을 방지하기 위해 임베딩 차원 수(dim k)의 제곱근으로 나눈다. 
3. 다음으로 쿼리와 키를 곱해 계산한 스코어(scores)를 합이 1이 되도록 소프트맥스(softmax)를 취해 가중치(weights)로 바꾼다 . 
4. 마지막으로 가중치와 값을 곱해 입력과 동일한 형태의 출력을 반환 한다

```python
from math import sqrt import torch.nn. functional as F
def compute_attention(querys, keys, values, is_causal=False):
	dim_k = querys.size(-1) # 16
	scores = querys @ keys. transpose(-2, -1) / sqrt(dim_k) 0
	weights = F.softmax(scores, dim=-1) 0
	return weights @ values 3
```
각 과정을 시각화한 그림 2.15를 보자. 쿼리(9)는 4개의 키와 각각 곱해 예제 2.5에서의 스코어를 계산한다. 
스코어는 `[2.2, 1.1, -1.7, 0.2]`로 나왔는데, 
이대로는 어떤 단어와의 관계를 얼마나 반영할지 명확히 정하기 어렵기 때문에 합을 1로 만들 수 있도록 소프트 맥스를 취한다. 

이 그림에서는 $x_\{1}$,만 연산해서 나타냈지만, $x_\{2}$, $x_\{3}$, $x_\{4}$, 모두 동일한 과정으로 연산 한다.

![Screenshot-2025-06-04-at-8.02.29-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.02.29-PM.png)

마지막으로 그림 2.15에서 구한 가중치와 값을 곱한 후 더해서 주변 단어의 맥락을 반 영한 하나의 값 임베딩으로 만든다. 
그림 2.16에서 $v_\{1}$은 가중치가 0.672로 가장 크기 때 문에 가장 많은 비중으로 섞이고,
$v_\{3}$은 가중치가 0.013으로 가장 작기 때문에 가장 적은 비중으로 섞인다

![Screenshot-2025-06-04-at-8.03.13-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.03.13-PM.png)

어텐션을 거치고 나면 입력과 형태는 동일하면서 주변 토큰과의 관련도에 따라 값 벡터 를 조합한 새로운 토큰 임베딩이 생성되는데, 
이를 확인하는 코드는 예제 2.6과 같다. 이 코드에서 원본 입력인 `input_embeddings`의 형태는 `[1, 5, 16]`인데, `compute_attetion`에 입력해 어텐션을 거치고 난 후의 임베딩인 `after_attention_embeddings`의 형태도 `[1, 5,16]`임을 확인할 수 있다.

```python
print("원본 입력 형태: ", input_embeddings.shape)

after_attention_embeddings = compute_attention(querys, keys, values)

print("어텐션 적용 후 형태: ", after_attention_embeddings.shape) 
# 원본 입력 형태: torch.Size([1, 5, 16]) 
# 어텐션 적용 후 형태: torch.Size([1, 5, 161)
```

지금까지 살펴본 어텐션의 연산 과정을 예제 2.7과 같이 AttentionHead라는 하나의 클래스로 나타낼 수 있다. 

쿼리, 키, 값 벡터를 생성하기 위해 사용 하는 선형 증(`weight_q`, `weight_k`, `weight_v`)을 `_init__` 메서드에서 생성하고 `forward` 메서드에서는 앞서 생성 한 선형 층을 통해 쿼리, 키, 값 벡터를 생성하고 `compute_attention` 함수를 사용해 어텐션 연산을 수행한다. 

클래스를 사용할 때는 `AttentionHead`에 인자로 토큰 임베딩 차원(`token_embed_dim`)과 출력 차원(`head_dim`)을 전달 하는데, 예제 2.7에서는 입력과 출력의 차원이 같도록 둘 다 `embedding_dim`을 전달 했다.

```python
class AttentionHead nn.Module:
	def__init_(self, token_embed _dim, head_dim, is_causal=False):
		super().__init_()
		self. is_causal = is_causal
		self.weight_a = nn.Linear(token_embed_dim, head_dim) # 쿼리 벡터 생성을 위한 선형 층 
		self.neight_k=nn.Linear(token_embed_dim, head_dim) # 키 벡터 생성을 위한 선형 층 
		self.weight_v= m.Linear(token_embed_dim, head_dim) # 값 벡터 생성을 위한 선형 층

	def forward(self, querys, keys, values) :
		outputs = compute_attention(
		self.weight_q(querys), # 쿼리 벡터
		self.weight_k(keys),# 키 벡터
		self .weight_v(values), # 값 벡터
		is_causal=self. is_causal
	)
	return outputs
	
attention_head = AttentionHead (embedding_dim, embedding_dim)
after_attention_embeddings = attention_head( input_embeddings, input_embeddings,
input_embeddings )
```

### 멀티 헤드 어텐션

트랜스포머 아키텍처를 고안한 논문 저자들은 한 번에 하나의 어텐션 연산만 수행하는 게 아니라 여러 어텐션 연산을 동시에 적용하면 성능을 더 높일 수 있다는 사실을 발견 했다. 
이를 멀티 헤드 어텐션이라고 한다. 

직관적으로 이해하자면, 토큰 사이의 관계를 한 가지 측면에서 이해하는 것보다 여러 측면을 동시에 고려할 때 언어나 문장에 대한 이해도가 높아질 것이다. 

그림 2.17(a)는 하나의 어텐션 연산을 수행하는 스케일 점곱 어텐션이고, 그림 2.17(b)는 동시에 헤드의 수(그림의 h)만큼의 어텐션 연산을 수행하는 멀티 헤드 어텐션이다.

![Screenshot-2025-06-04-at-8.09.21-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.09.21-PM.png)

코드로는 예제 2.8과 같이 구현할 수 있다. 

`AttentionHead`와 대부분의 코드가 동일한데 헤드의 수(코드의 n_head)만큼 연산을 수행하기 위해 
1. 쿼리, 키, 값을 nLhead개로 쪼개고 
2. 각각의 어텐션을 계산한 다음 입력과 같은 형태로 다시 변환한다. 
3. 마지막으로 선형 증을 통과시키고 최종 결과를 반환한다. 
코드와 그림 2.17(b)를 대응시켜 보면, 코드의 
- 1은 그림에서 Q, K, V가 처음 통과하는 여러 선형 층에 대응되고, 
- 2번은 h번의 스케 일 점곱 어텐션에, 
- 3은 어텐션 결과를 연결하는 단계에, 
- 4는 마지막 선형 층에 대응된다. 
예제 2.8에서는 헤드의 수를 4로 두고 멀티 헤드 어텐션 연산을 수행하는데, 출력 결과를 보면 입력 형태와 출력 형태가 동일한 것을 확인할 수 있다.


`멀티 헤드 어텐션 구현`
```python
class MultiheadAttention(nn.Module):

def _init_(self, token_embed_dim, a_model, n_head, is_causal=False) :
	super().__init__()
	self.n_head = n_head
	self. is_causal = is_causal
	self weight_q = nn. Linear (token_embed _dim, d_model)
	self weight_k = nn. Linear (token_embed _dim, d_model)
	self weight_v = nn. Linear (token_embed_dim, d_model)
	self.concat_linear = nn. Linear(d_model, d_model)

def forward(self, querys, keys, values):
	B, T, C = querys.size()
	querys = self .weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
	keys = self .weight_k(keys).view(B, T, self.n _head, C // self.n_head).transpose(1, 2)
	values = self .weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose (1, 2) # --> 1
	
	attention = compute_attention(querys, keys, values, self. is_causal) # --> 2
	output = attention. transpose(1, 2). contiguous ( ). view(B, T, C) # --> 3
	output = self. concat_linear(output) • return output # --> 4


n_head = 4
mh_attention = MultiheadAttention( embedding_dim, embedding_dim, n_head)
after_attention_embeddings = mh_attention( input_embeddings, input_ embeddings, input_embeddings)
after_attention_embeddings.shape

# 출력 결과

# torch. Size( [1, 5, 16])
```


## 정규화와 피드 포워드 층

지금까지 트랜스포머 인코더와 디코더의 가장 핵심적인 연산인 어텐션 연산을 살펴봤 다. 
그림 2.2에서 인코더와 디코더를 살펴보면 어텐션 연산 이외에 층 정규화와 피드 포워드 층이 있는데, 이번 절에서는 두 가지 중에 대해 살펴본다. 

정규화란, 딥러닝 모 델에서 입력이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법이다. 

과거에는 배치 입력 데이터 사이에 정규화를 수행하는 배치 정규화(`batch normalzation`)를 주로 사용했으나 

트랜스포머 아키텍처에서는 특징 차원에서 정규화를 수 행하는 층 정규화(`layer normalization`)를 사용하는데, 
이번 절에서는 배치 정규화와 층 정규화 의 차이를 살펴본다. 
어텐션 연산이 입력 단어 사이의 관계를 계산해 토큰 임베딩을 조 정하는 역할을 한다면 전체 입력 문장을 이해하는 연산이 필요한데, 
트랜스포머 아키텍 처에서는 이를 위해 완전 연결 층(`fully conected layer`)인 피드 포워드 층을 사용한다. 
피드 포 워드 층을 알아보기에 앞서 층 정규화를 한번 살펴보자.

### 층 정규화 이해하기
딥러닝 모델에 데이터를 입력할 때, 입력 데이터의 분포가 서로 다르면 모델의 학습이 잘되지 않기 때문에, 데이터를 정규화하여 입력하는 것이 중요하다. 예를 들어, 사람의 나이와 키를 기반으로 몸무게를 예측하는 모델을 생각해 보자. 나이는 일반적으로 1세 에서 100세 사이의 값을 가지며, 키는 140cm에서 200cm 사이의 값을 갖는다. 만약 키 데이터를 mm 단위로 변환한다면, 그 범위는 1,400mm에서 2,000mm로 변경된다. 이 렇게 단위를 변경함으로써 데이터의 분포가 훨씬 넓어지고, 결과적으로 모델이 키 데이 터의 중요성을 과대평가할 가능성이 높아진다. 즉 mm 단위로 키를 입력하면, 모델이 나이와 키를 기반으로 몸무게를 예측하는 과정에서 키의 영향을 과도하게 반영하게 되 어 정확한 예측을 어렵게 만든다. 이러한 문제를 방지하기 위해, 데이터를 정규화하여 모든 입력 변수가 비슷한 범위와 분포를 갖도록 조정하는 것이다. 이를 통해 모델은 각 입력 변수의 중요성을 적절히 반영하여 좀 더 정확한 예측을 할 수 있게 된다.

입력 데이터가 딥러닝 모델의 각 층을 거치면서 위에서 살펴본 것과 같이 어떤 특성은 좁은 분포를 갖고 어떤 특성은 넓은 분포를 갖게 된다.
특히 층이 깊은 모델에서는 분포 의 차이가 발생할 가능성이 높아지고 그렇게 되면 학습이 잘되지 않는다. 
그런 이유로 딥러닝 분야에서는 층과 층 사이에 정규화를 추가해 학습을 안정적으로 만드는 기법을 사용해 왔다. 
정규화는 여러 데이터의 평균과 표준편차를 구해서 다음과 같은 식으로 계산한다. 

벡터 x를 정규화한 $norm_x$는 벡터 $x$에서 $x$의 평균을 빼고 $x$의 표준편차로 나눠 평균이 0이고 표준편차가 1인 분포를 갖게 된다.

$$
norm_x=(x - 평균) / 표준편차
$$

딥러닝에서는 평균과 표준편차를 구할 데이터를 어떻게 묶는지에 따라 크게 배치 정규화와 층 정규화로 구분한다. 
일반적으로 이미지 처리에서는 배치 정규화를 사용하고, 자연어 처리에서는 층 정규화를 사용한다.

배치 정규화는 이름에서 알 수 있듯이 그림 2.18과 같이 모델에 입력으로 들어가는 미니 배치 사이에 정규화를 수행한다.
자연어 처리에서 배치 정규화를 사용하지 않는 이유를 직관적으로 이해해 보자면, 

자연어 처리에서는 입력으로 들어가는 문장의 길이가 다양한데, 
배치 정규화를 사용할 경우 정규화에 포함되는 데이터의 수가 제각각이라 정규화 효과를 보장하기 어렵다. 

이 그림에서 배치 정규화는 배치 내의 서로 다른 문장 데 이터를 묶어 정규화를 수행한다. 이때 왼쪽의 첫 번째 토큰 묶음은 `['나는, '배치, '층]` 모두 실제 데이터이기 때문에 세 데이터의 평균과 표준편차로 정규화를 하는 효과가 있다. 

하지만 일곱 번째 토큰 묶음은 2개는 패딩 토큰이고 1개만 '사용한다'라는 실제 데 이터이기 때문에 사실상 정규화의 효과가 없다.

![Screenshot-2025-06-04-at-8.18.53-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.18.53-PM.png)
층 정규화는 이런 단점을 보완할 수 있도록 그림 2.19와 같이 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다. 그림에서 확인할 수 있듯이 문장별로 실제 데이터 의 수가 다르더라도 각각의 토큰 임베딩별로 정규화를 수행하기 때문에 정규화 효과에 차이가 없다
![Screenshot-2025-06-04-at-8.19.45-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.19.45-PM.png)
트랜스포머 아키텍처에서 층 정규화를 적용하는 순서에는 크게 두 가지 방식이 있다. 
원 트랜스포머 논문에서는 그림 2.20(a)와 같이 어션과 피드 포워드 층 이후 에 층 정규화를 적용했다. 

이를 사후 정규화(`post-norm`)라고 부른다. 

하지만 2020년 루이 빈 시옹Ruilbin Xiong과 윤창 양unchang Yang이 발표한 「On Layer Normalization in the Transformer Architecture(트랜스포머 아키텍처에서의 층 정규화)」(https://arxiv.org/ pdf/2002.04745.pdf) 논문에서 먼저 층 정규화를 적용하고 어텐션과 피드 포워드 층을 통과했을 때 학습이 더 안정적이라는 사실이 확인됐다. 

이를 사전 정규화(`pre-norm`)라고 부 른다. 현재는 그림 2.20(b)와 같은 사전 정규화가 주로 활용되며, 
이번 장에서 사용하는 모든 그림과 예제 코드도 사전 정규화를 기준으로 하고 있다.

![Screenshot-2025-06-04-at-8.21.06-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.21.06-PM.png)
층 정규화에 대해 길게 설명했지만, 예제 2.9와 같이 파이토치가 제공하는 LayerNorm 클 래스를 이용해 간단히 코드로 구현할 수 있다. 
이 코드에서는 우선 `nn.LayerNorm` 클래스 로 층 정규화 레이어를 만든다. 
이때 토큰 임베딩 차원(`embedding_dim`)을 인자로 전달했고, 입력 임베딩을 층 정규화 레이어에 통과시켜 정규화된 임베딩(`norm_x`)으로 만든다.

`층 정규화 코드`
```python
norm = nn. LayerNorm(embedding_dim)
norm_x = norm(input_embeddings)
norm_x. shape # torch. Size( [1, 5, 16])
norm_x.mean(dim=-1).data, norm_x.std(dim=-1).data # 실제로 평균과 표준편차 확인하기

# (tensor([2.2352e-08, -1.1176e-08, -7.4506e-09, -3.9116e-08, -1.8626e-08](2.2352e-08, -1.1176e-08, -7.4506e-09, -3.9116e-08, -1.8626e-08)),
# tensor ([1.0328, 1.0328, 1.0328, 1.0328, 1. 0328](1.0328, 1.0328, 1.0328, 1.0328, 1. 0328)))
```

### 피드 포워드 층
피드 포워드 층(`feed forward layer`)은 데이터의 특징을 학습하는 완전 연결 층(`fully connected layer`) 을 말한다. 
멀티 헤드 어텐션이 단어 사이의 관계를 파악하는 역할이라면 피드 포워드 층은 입력 텍스트 전체를 이해하는 역할을 담당한다. 

피드 포워드 층은 예제 2.10의 코드 와 같이 선형 층, 드롭아웃 층, 층 정규화, 활성 함수로 구성된다. 임베딩의 차원을 동일 하게 유지해야 쉽게 층을 쌓아 확장이 가능하기 때문에 입력과 출력의 형태가 동일하도 록 맞춘다. 
일반적으로 `d_model`차원에서 `d_model`보다 2~3배 큰 `dim_feedforward`차원으 로 확장했다가 다시 `d_model로` 변환한다.

`피드 포워드 층 코드`
```python
class PreLayerNormFeedForward(nn.Module):
	definit_(self, d_model, dim_feedforward, dropout) :
	super ( )._init_()
	self. linearl = nn. Linear(d_model, dim_feedforward) # 1
	self. linear2 = nn.Linear(dim_feedforward, d_model) # 18 2
	self.dropout1 = nn.Dropout(dropout) # 드롭아웃 층 1
	self.dropout2 = nn.Dropout(dropout) # 드롭아웃 층 2 
	self.activation= nn.GELU() # 활성 함수
	self.norm=nn.LayerNorm(d_modeL) # 층 정규화

def forward(self, src):
	x = self. norm(src)
	x = x + self. linear2(self. dropoutl(self.activation(self. linearl(x))))
	x = self. dropout2 (x)
	return x
```

이제 트랜스포머 인코더와 디코더에 공통적으로 필요한 모듈을 모두 살펴봤다. 
지금까지 설명한 모듈을 결합해 트랜스포머 인코더 층과 디코더 층을 구현해 보자.

## 인코더

트랜스포머 인코더는 그림 2.21과 같다. 

인코더는 이 그림과 같이 앞서 살펴본 멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태다. 
상자 안을 보면 밖으로 뻗어나갔 다가 다시 더해지는 화살표가 2개 있는 것을 확인할 수 있는데, 안정적인 학습이 가능하 도록 도와주는 잔차 연결(`residual cornection`)이다. 

잔차 연결은 화살표 모양 그대로 입력을 다 시 더해주는 형태로 구현한다. 또한 그림에서 블록이 $N_\{e}$번 반복된다고 표시되어 있는데, 트랜스포머 인코더는 그림의 인코더 블록을 반복해서 쌓아서 만든다.

![Screenshot-2025-06-04-at-8.25.49-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.25.49-PM.png)
하나의 인코더 층을 코드로 구현하면 예제 2.11과 같다.
`__init__` 메서드에서 앞서 살펴 본 멀티 헤드 어텐션 층, 층 정규화, 드롭아웃, 피드 포워드 층을 불러온다. 

다음으로 그 림 2.21의 인코더 순서 그대로, 
1) 입력인 src를 `self.norml`을 통해 층 정규화를 취하고 
2) 멀티 헤드 어텐션 클래스(MultiheadAttention)를 인스턴스화한 `self.attn`을 통해 멀티 헤드 어텐션 연산을 수행한 후 
3) 잔차 연결을 위해 어텐션 결과에 드롭아웃을 취한 `self.dropout1(attn_output)`과 입력`(src)`을 더해준다3
4) 마지막으로 `self. feed_forvard(x)`를 통해 피드 포워드 연산을 취한다

`인코더 층`
```python
class TransformerEncoderLayer(nn.Module):
	def _init_(self, d_model, nhead, dim_feedforward, dropout) :
		super( )._init_()
		self.attn = Mult theadAttention(d_model, dmodeL, nhead) # 멀티 헤드 어텐션 클래스
		self.norml = nn. LayerNorm(d_model) # 층 정규화
		self.dropoutl = nn.Dropout( dropout) # 드롭아웃
		self. feed_forward = PreLayerNormFeedForward(d _model, dim_feedforward, dropout) # 피드 포워드

	def forward(self, src):
		norm_x = self.norml(src) 0 # --> 1
		attn_output = self.attn(norm_x, norm_x, norm_x) # --> 2
		x = src + self.dropout1(attn_output) # 잔차 연결 --> 3
		
		# 피드 포워드
		x = self. feed_forward(x) # --> 4
		return X
```

이제 그림 2.21에 표시된 것과 같이 인코더 층(`TransformerEncoderLayer`)을 $N_\{e}$번 반복되도록 코드로 구현해 보자.

예제 2,12에서 `get_clones` 함수는 입력한 모듈(`module`)을 깊은 복 사를 통해 번 반복해 모듈 리스트에 담는다. 

`TransformerEncoder` 클래스에서는 인자로 전달받은 `encoder_layer`를 `get_cLones` 함수를 통해 
`num_Layers`번 반복해 `nn.ModuleList` 에 넣고 `forward` 메서드에서 `for` 문을 통해 순회하면서 인코더 층 연산을 반복 수행하도록 만든다.

`인코더 구현`
```python
import copy
def get_clones(module, N) :
	return nn.ModuleList([copy.deepcopy(module) for i in range(N)])
	
class TransformerEncoder (nn.Module):
	def __init__(self, encoder_layer, num_layers):
		super()._init_()
		self. layers = get_clones(encoder_layer, num_layers)
		self.num_layers = num_layers
		self.norm = norm

	def forward(self, src):
		output = src
		for mod in self. layers:
			output = mod( output )
		return output
```

## 디코더
디코더는 그림 2.22의 오른쪽 상자를 말한다. 

디코더는 인코더와 비교할 때 두 가지 부분 에서 차이가 있다. 
먼저, 인코더는 기본적인 멀티 헤드 어텐션을 사용하지만 디코더 블 록에서는 마스크 멀티 헤드 어텐션을 사용한다. 
디코더는 생성을 담당하는 부분으로, 사 람이 글을 쓸 때 앞 단어부터 순차적으로 작성하는 것처럼 트랜스포머 모델도 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성한다. 

이렇게 순차적으로 생성해야 하는 특징 을 인과적(`causal`) 또는 자기 회귀적(`auto-regressve`)이라고 말한다.

![Screenshot-2025-06-04-at-8.32.55-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.32.55-PM.png)
실제 텍스트를 생성할 때 디코더는 이전까지 생성한 텍스트만 확인할 수 있다. 
그런데 학 습할 때는 인코더와 디코더 모두 완성된 텍스트를 입력으로 받는다. 
따라서 어텐션을 그대로 활용할 경우 미래 시점에 작성해야 하는 텍스트를 미리 확인하게 되는 문제가 생긴다. 

이를 막기 위해 특정 시점에는 그 이전에 생성된 토큰까지만 확인할 수 있도록 마스 크를 추가한다. 
예제 2.5의 어텐션 코드에 is_causal 인자를 추가해서 예제 2.13과 같이 디코더(인과적)인 경우 `true`로 설정해 마스크 연산을 추가할 수 있게 한다. 

예제 2.5에 없 던 추가한 부분을 굵은 글꼴로 표시했다. 이 코드에서 가장 중요한 부분은 미래 시점의 토큰을 제거하기 위한 마스크(`temp_mask`)를 만드는 부분인데, 바로 이어서 설명한다.

`디코더에서 어텐션 연산(마스크 연산)`
```python
def compute_attention(querys, keys, values, is_causal=False):
	dim_k = querys. size(-1) # 16
	scores = querys @ keys. transpose(-2, -1) / sqrt(dim_k) # (1, 5, 5)
	if is_causal:
		query_length = querys.size(-2)
		key_length = keys. size(-2)
		temp_mask = torch.ones(query_length, key_length, dtype=torch.bool). tril(diagonal=0)
		scores = scores. masked_ftll(temp_mask == False, float("-inf"))
		
	weights = F.softmax(scores, dim=-1) # (1, 5, 5)
	return weights @ values # (1, 5,16)
```
예제 2.13의 코드를 살펴보면, `is_causal`이 참일 때는 `torch.ones`로 모두 1인 행렬에 `tril` 함수를 취해 그림 2.23의 가운데 행렬과 같이 대각선 아래 부분만 1로 유지되고 나머지는 음의 무한대(`-inf`)로 변경해 마스크를 생성한다. 

마스크(예제의 `temp_mask`)를 그림2.23의 왼쪽인 어션 스코어 행렬(예제의 `scores`)에 곱하면 행렬의 대각선 아랫부분만 어텐션 스코어가 남고 위쪽은 음의 무한대가 된다. 
예제 2,13에서 가중치(weights)를 만 들기 위해 소프트맥스를 취하는데, 이때 음의 무한대인 대각선 윗부분은 가중치가 0이 된다.

![Screenshot-2025-06-04-at-8.36.11-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.36.11-PM.png)

인코더와 디코더의 두 번째 차이는 크로스 어텐션(`cross atention`)이 있다는 것이다. 

그림 2.21 이나 2.22를 보면 인코더와 디코더가 연결되는 선이 있는데, 이것이 바로 인코더의 결 과를 디코더가 활용하는 크로스 어텐션 연산을 나타낸 선이다.

예를 들어, 영어에서 한국어로 번역한다고 했을 때 인코더가 영어 문장을 입력으로 받 아 처리한 결과를 번역한 한국어를 생성하는 디코더가 받아 활용한다. 이때 쿼리는 디 코더의 잠재 상태를 사용하고 키와 값은 인코더의 결과를 사용한다. 
예제 2.14에서는 인코더의 결과를 `forward` 메서드에 `encoder_output` 이라는 이름의 인자로 넣을 수 있도 록 했는데, `self.multthead_attn(x, encoder_output, encoder_output)`을 통해 크로스 어텐션 연산을 수행한다

`크로스 어텐션이 포함된 디코더 층`
```python
class TransformerDecoderLayer (nn.Module) :
	def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):
	super().__init_()
	self.self_attn = MultiheadAttention(d _model, d_model, nhead)
	self.multihead_attn = MultiheadAttention(d_model, d_model, nhead)
	self. feed_forward = PreLayerNormFeedForward(d _model, dim_feedforward, dropout)
	self.norml = nn. LayerNorm(d_model)
	self.norm2 = nn. LayerNorm(d_model)
	self.dropoutl = nn.Dropout (dropout)
	self.dropout2 = nn. Dropout( dropout )

	def forward(self, tgt, encoder_output, is_causal=True) :
		# 셀프 어텐션 연산
		x = self.norml(tgt)
		x = x + self. dropout1(self.self_attn(x, x, x, is_causal=is_causal))
		
		# 크로스 어텐션 연산
		x = self.norm2(x)
		x= x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output) )
		
		# 피드 포워드 연산
		x = self. feed_forward(x)
		return x
```

디코더는 인코더와 마찬가지로 디코더 층을 여러 번 쌓아 만든다. 
예제 2.15에서 앞서 살펴본 `get_clones` 함수를 사용해 디코더 층을 N번 반복하고 `nn.ModuleList`를 활용해 순회하도록 한다

`디코더 구현`
```python
import copy
def get_clones (module, N) :
	return nn.ModuleList [copy. deepcopy(module) for i in range(N)])

class TransformerDecoder (nn.Module):
	def _init_(self, decoder_layer, num_layers) :
		super( )._init__()
		self. layers = get_clones decoder_layer, num_layers)
		self.num_layers = num_layers
	def forward(self, tgt, src):
		output = tgt
		
		for mod in self. layers:
			output = mod( output, src)
		return output
```

지금까지 트랜스포머의 핵심인 어텐션 연산부터 시작해서 인코더와 디코더까지 구현해 봤다. 
다음 절에서는 트랜스포머를 활용한 다양한 모델 아키텍처를 살펴본다

## BERT. GPT, T5 등 트랜스포머를 활용한 아키텍처
앞서 트랜스포머 아키텍처가 인코더와 디코더로 이뤄졌다는 사실을 확인했다. 

트랜스 포머 아키텍처를 활용한 모델은 크게 세 가지 그룹으로 나눌 수 있는데, 

인코더만 활용 해 자연어 이해(`Natural Language Under standing. NLU`) 작업에 집중한 그룹,
디코더만 활용해 자 연어 생성(`Natural Language Generation, NLG`) 작업에 집중한 그룹, 

인코더와 디코더를 모두 활 용해 더 넓은 범위의 작업을 수행할 수 있도록 한 그룹이다.

다음 절부터 순차적으로 3개의 그룹을 살펴볼 텐데, 각각의 장단점을 정리하면 표 2.1 과 같다.

![Screenshot-2025-06-04-at-8.41.33-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.41.33-PM.png)

### 인코더를 활용한 BERT
트랜스포머의 인코더만을 활용해 자연어 이해 태스크에 집중한 대표적인 모델은

[BERT(`Bidirectional Encoder Representations from Transformers`)](https://arxiv.org/abs/1810.04805)이다. 

구글에서 개발한 BERT는 이름에서 나타나듯이 양방향(`bidirectional`) 문맥을 모두 활용해 텍스트를 이해한다. 

BERT는 그림 2.24의 왼쪽과 같이 입력 토큰의 일부를 마 스크 토큰으로 대체하고 그 마스크 토큰을 맞추는 마스크 언어 모델링(`Masked Language Modeling, MLM`) 과제를 통해 사전 학습한다. 
사전 학습한 모델은 이후에 필요한 다운스트림(`downstream`) 과제에 따라 미세 조정(`fine-tuning`)해 사용한다. 

그림 2.24에서는 사전 학습 된 BERT를 메일이 스팸인지 아닌지를 분류하는 텍스트 분류 작업으로 미세 조정했다.

하지만 BERT는 텍스트 분류뿐만 아니라, 토큰 분류(`token classtication`), 질문 답변(`question answering`), 자연어 추론(`natural language Iterence`) 등 다양한 자연어 이해 작업에서 훌륭한 성능을 보인다

![Screenshot-2025-06-04-at-8.44.20-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.44.20-PM.png)
특히 BERT는 양방향 문맥을 이해할 수 있다는 특징이 있어 자연어 이해 작업에서 뛰어난 성능을 보인다. 
BERT가 큰 성공을 거둔 이후로 BERT를 개선한 `ROBERTa`, `BLECTRA` 등 여러 변형 인코더 모델이 나왔다. 

BERT는 생성 작업에는 적합하지 않아 최근 LLM의 인기와는 다소 관련이 떨어진다고 생각할 수 있지만, LLM의 활용 과정에서 도 사용자 발화의 의도 분류와 같이 효율적인 LLM 활용을 위한 연결 작업에는 자연어 이해 작업이 많이 사용되기 때문에 여전히 가장 사랑받는 모델군 중 하나다.

### 디코더를 활용한 GPT
OpenAI에서 개발한 GPT(`Generative Pre-trained Transformer`)는 이름에서도 알 수 있듯이 생성 작업을 위해 만든 모델이다. 

그렇다 보니 트랜스포머 아키텍처 중 디코더만을 사용한다. 

생성 작업의 경우 입력 토큰이나 이전까지 생성한 토큰만을 문맥으로 활용하는 인과적 언어 모델링(`Causal Language Modeling, CLM`)을 사용하기 때문에 양방향이 아닌 단방향 방식이다. 

GPT는 다음 토큰을 예측하는 방식으로 사전 학습을 수행한다. 
디코더 모델 그룹은 확장이 용이하고 모든 자연어 처리 작업을 생성 작업으로 변환할 수 있기 때문에 OpenAI는 GPT 모델을 확장하여 더 다양한 문제 를 풀고자 했다.

OpenAI는 2020년에 발표한 「Scaling Laws for Neural Language Models(신경망 언어 모델에서의 규모 법칙)」(https://arxiv.org/pdf/2001.08361.pdf) 논문에서 모델의 크기 와 데이터의 크기가 커질수록 모델의 성능이 높아진다는 실험 결과를 발표했다. 

실제로 OpenAI는 그림 2.25와 같이 

- 2018년 1억 1,700만 개의 파라미터를 갖는 GPT-1 발표 이후로 
- 2019년 파라미터가 15억 개인 GPT-2, 
- 2020년에는 파라미터가 1,750억 개인 GPT-3를 발표했다. 

매년 모델의 크기를 약 10배, 100배 키운 것이다. 앞으로 이 책에 서 다루는 LLM(3장 제외)은 대부분 디코더만을 사용!한 생성 모델이다
![Screenshot-2025-06-04-at-8.47.04-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.47.04-PM.png)
그림 2.25에 나와 있는 GPT 세 버전의 사양을 비교하면 표 2.2와 같다. 
쌓는 디코더 층 수가 
- GPT-1에서는 12개에서 
- GPT-2 48개, 
- GPT-3에서는 96개로 증가했다. 
또한 모 델에서 사용하는 토큰 임베딩의 차원을 GPT-1에서는 768차원을 사용했지만 
GPT-3 에서는 12,288차원으로 크게 늘렸다. 

마지막으로 입력 토큰 수를 
GPT-1의 512개에서 
GPT-3이 2,048개로 4배 늘었다. 

결과적으로 GPT-3의 파라미터 수는 GPT-1 대비 약 1,500배 증가했다.

![Screenshot-2025-06-04-at-8.48.01-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.48.01-PM.png)

### 인코더와 디코더를 모두 사용하는 BART, T5

인코더와 디코더 중 하나를 선택한 이전 모델들과 달리 트랜스포머 원 논문과 같이 인 코더와 디코더 모두를 활용한 모델도 있다. BART(`Bidirectional and Auto-Regressive Transtormers`) 와 T5(`Text-to-Text Transter Transtormer`)는 자연어 처리 분야에서 인코더-디코더 트랜스포머 아키텍처를 대표하는 모델이다.

메타가 개발한 BART는 이전에 성공했던 BERT와 GPT의 장점을 결합한 모델이다. 

그림 2.26과 같이 인코더-디코더 모델을 사전 학습하기 위해 입력 테스트에 노이즈를 추 가하고 노이즈가 제거된 결과를 생성하는 과제를 수행하도록 한다. 

원본 트랜스포머 모 델과 유사한 형태를 가지면서 인코더 부분이 양방향 추론이 가능하다는 점이 BART의 특징이며, 
BERT보다 다양한 사전 학습 과제를 도입했고 더 자유로운 변형(노이즈) 추가가 가능하다는 점에 차이가 있다.

![Screenshot-2025-06-04-at-8.49.16-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.49.16-PM.png)
구글이 개발한 TS는 모든 자연어 처리 작업이 결국 '텍스트에서 텍스트 Text to Text 로의 변 환'이라는 아이디어를 바탕으로 한다. 
T5는 입력의 시작(`prefix`)에 과제 종류를 지정해서 하나의 모델에서 지정한 작업 종류에 따라 다양한 동작을 하도록 학습시킨 점이 특징이 었다.

그림 2.27에서 맨 위의 말풍선은 영어에서 한국어로 번역하도록 입력을 "영어를 한국어 로 번역:"으로 시작한다. 

또한 맨 아래 말풍선에서는 이어지는 문장을 요약하도록 입력 을 "요약:"으로 시작했다. 
T5를 통해 하나의 모델이 어떤 명령으로 시작하느냐에 따라 다양한 작업을 수행할 수 있다는 사실이 확인되면서 이후 생성 모델의 연구에 큰 영향을 미쳤다. 

복잡도가 비교적 낮지만 생성 작업이어서 언어 모델을 활용해야 하는 경우 T5는 비용 효율적이면서도 높은 성능을 보여주기 때문에 최근에도 많이 활용되고 있다.
![Screenshot-2025-06-04-at-8.50.48-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.50.48-PM.png)
## 주요 사전 학습 메커니즘
2.7절에서 트랜스포머 아키텍처와 그 변형 아키텍처에 대해 알아 봤다.

각각의 모델 그 룹은 고유한 학습 방법이 있는데, 이번 절에서는 디코더 모델을 학습시키는 인과적 언 어 모델링과 인코더 모델을 학습시키는 마스크 언어 모델링을 직관적으로 이해할 수 있도록 설명한다.

### 인과적 언어 모델링
인과적 언어 모델링은 문장의 시작부터 끝까지 순차적으로 단어를 예측하는 방식이다.

이전에 등장한 단어들을 바탕으로 다음에 등장할 단어를 예측한다.
이는 사람이 문장을 만드는 방식과 유사하다. 

예를 들어 '인과적 언어 모델링 학습 방법'에 대해 발표를 해야 하고 아직 자료 조사를 하지는 않았지만 마음이 급해 우선 발표 스크립트를 작성한다 고 하자. 

"안녕하세요. 저는 0O학번 OOO입니다. 오늘 인과적 언어 모델링 학습 방법 에 대해 발표하려고 합니다." 
인사와 자기 소개는 살면서 많이 해왔기 때문에 술술 적을 수 있다. 

하지만 발표 내용을 적으려고 하면 속도가 나지 않는다. 
왜냐하면 아직 주제에 대해 제대로 조사하지 않았기 때문에 잘 알지 못하고 그 분야의 단어, 내용에 대해 말해 본 경험이 없기 때문이다.

예시를 좀 더 자세히 들여다보자. 
한국어는 기본적으로 왼쪽에서 오른쪽으로 작성하기 때문에 인사말을 작성할 때 우리는 왼쪽부터 하나씩 단어를 작성한다. 
'안녕'까지 작성 한 상태에서 다음에 쓸 내용을 생각했을 때 내일이 발표이고 발표에서는 존댓말을 사용 하기 때문에 짧은 순간에 '하세요'를 붙여야 한다고 판단하고 안녕하세요'를 작성한다.

자기 소개를 작성할 때도 학교 안에서 하는 발표인지 여부에 따라 학교 이름을 생략할 지 순간적으로 결정한다. 
또한 조별 발표인지 여부에 따라 조 이름이나 번호를 추가할 지 등을 결정하면서 작성한다.

익숙한 상황과 내용이기 때문에 이 판단은 아주 짧은 시간에 이뤄지는데, 짧은 시간에 가능한 이유는 머릿속에서 확신에 가득 찬 판단이 내려지기 때문이다. 

이런 과정을 수 학적으로 이해하자면, 다음 단어로 특정 단어(안녕-> 하세요)가 올 확률이 다른 단어(안 녕->x)가 나올 확률보다 훨씬 큰 것이다.

사람의 경우 어릴 때부터 많은 글을 보면서 'A라는 단어 다음에는 B라는 단어가 자주 온다'는 직관을 배운다. 
언어 모델도 인과적 언어 모델링 방식으로 많은 데이터에 대해 다음 단어를 예측하는 방법을 학습함으로써 더 가능성이 높은(확신이 있는) 단어를 생성 하는 능력을 갖추게 된다.

GPT 같은 생성 트랜스포머 모델에서는 인과적 언어 모델링을 핵심적인 학습 방법으로 사용한다.

### 마스크 언어 모델링
마스크 언어 모델링을 직관적으로 이해하자면 시끄러운 공간에서 대화를 나누는 경우 를 상상하면 된다. 
시끄러운 상황에서 대화를 나누면 중간중간 몇 개의 단어는 들리지 않는다. 

익숙한 언어(예: 한국어)로 대화를 나누면 몇 개의 단어가 들리지 않아도 대화가 잘 이어진다.
앞에서 나온 단어를 통해 유추할 수도 있고 뒤에서 나오는 단어를 통해 앞 에 무슨 말을 했을지 유추할 수도 있다. 

하지만 익숙하지 않은 언어(예: 영어)로 대화를 나누면 한 단어만 들리지 않아도 대화의 흐름을 놓칠 수 있는데, 앞뒤 문맥으로 빠진 단 어를 예측하는 능력도 떨어지고 유추하고 있을 시간적 여유도 부족하기 때문이다. 

따라서 언어 모델이 마스크 언어 모델링 학습을 통해 중간에 빠진 단어를 잘 채울 수 있게 된다면 언어 능력이 뛰어나다고 할 수 있다.

마스크 언어 모델링은 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습시킨다. 

마스크 처리란, 그림 2.28과 같이 '먹고'를 `[MASK]`라는 특수 토큰 으로 대체하는 것을 말한다. 

마스크 처리를 하면 언어 모델은 입력이 무엇인지 알지 못 하는 상황에서 그 토큰이 무엇인지 맞춰야 한다. 
인과적 언어 모델링은 앞에서부터 뒤 로 순차적으로 토큰을 생성하는데, 이 방식은 지금까지 생성한 문맥만 활용할 수 있다 는 한계가 있다. 

사람이 책을 읽을 때를 생각해 보면, 독자는 책 내용을 이해할 때 순차 적으로만 읽지 않는다. 
잘 이해가 가지 않는 문장은 서로 단어가 어떻게 연결되는지 앞 에서 뒤로, 뒤에서 앞으로 뜯어보면서 이해한다. 

인과적 언어 모델링의 경우 단방향 예 측이기 때문에 다음 단어 예측이라는 목표가 자연스럽게 정해지지만, 양방향 방식의 경 우 새로운 작업 목표가 필요하다. 

BERT 연구팀은 그림과 같이 문장 사이에 토큰 일부를 마스크 처리해서 맞추는 방식을 사용했다

![Screenshot-2025-06-04-at-8.55.10-PM.png](/img/이미지 창고/Screenshot-2025-06-04-at-8.55.10-PM.png)

#### 참고 자료
- [트랜스포머 논문](https://arxiv.org/abs/1706.03762)
- [﻿﻿BERT EE](https://arxiv.org/abs/1810.04805)
- [﻿﻿GPT EE](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [﻿﻿OpenAl의 언어 모델 규모 법칙Scaling Law 논문](https://arxiv.org/pdf/2001.08361.pdf)
- [﻿﻿T5 EE](https://arxiv.org/abs/1910.10683)
- ﻿﻿[BART EE](https://arxiv.org/abs/1910.13461)
- ﻿[﻿안드레아 카르파티AndrejKarpathy 트랜스포머 구현](https://www.youtube.com/watch?v=kCc8FmEb1nY)

# 트랜스포머 모델을 다루기 위한 허깅페이즈 트랜스포머 라이브러리
2017년 트랜스포머 아키텍처가 공개된 이후 2018년 구글의 BERT와 OpenAI의 GPT가 개발되면서 트랜스포머 아키텍처를 활용한 모델이 쏟아져 나오기 시작했다. 

당시에 는 모델을 개발하는 조직마다 각자의 방식으로 모델을 구현하고 공개했는데, 핵심적인 아키텍처를 공유 함에도 구현 방식에 차이가 있어 모델마다 활용법을 익혀야 한다는 문제가 있었다. 
수많은 모델이 쏟아지는 상황에서 그런 진입 장벽으로 인해 연구와 개발 의 속도가 늦춰졌다. 
허깅페이스(Huggingface)팀이 개발한 트랜스포머 Transtormers 라이브러 리는 공통된 인터페이스로 트랜스포머 모델을 활용할 수 있도록 지원함으로써 이런 문 제를 해결했고 현재는 딥러닝 분야의 핵심 라이브러리가 됐다.

이번 장에서는 허깅페이스 트랜스포머 라이브러리에 친숙해질 수 있도록 실습을 포함 해 라이브러리의 구성요소를 살펴본다. 
먼저 허깅페이스 트랜스포머 라이브러리가 무엇이며, 왜 많은 사용자가 허깅페이스에 열광하는지 알아본다. 

다음으로 다양한 모델, 데이터셋, 모델 데모를 쉽게 공유하고 사용할 수 있도록 제공하는 허깅페이스 허브를 알아본다. 
그리고 모델 학습과 활용에 꼭 필요한 데이터셋, 모델, 토크나이저를 각각 살펴본다. 
마지막으로, 한국어 데이터셋을 활용해 텍스트 분류 모델을 만들고 활용하는 실습을 진행한다.

본격적으로 내용에 들어가기 전에 구글 코랩에서 다음 명령을 실행해 허깅페이스 트랜 스포머 활용에 필요한 라이브러리를 설치한다.

```shell
pip install transformers<mark>4.50.0 datasets</mark>3.5.0 huggingface_hub==0.29.0 -qqq
```

## 허깅페이스 트랜스포머란

허깅페이스 트랜스포머는 다양한 트랜스포머 모델을 통일된 인터페이스로 사용할 수 있 도록 지원하는 오픈소스 라이브러리다. 
만약 허깅페이스 트랜스포머가 없었다면 사람들은 새로운 모델이 공개될 때마다 그 모델을 어떻게 불러올 수 있는지, 모델이 어떤 함수를 갖고 있는지, 어떻게 학습시킬 수 있는지 파악하는 데 많은 시간을 써야 했을 것이다.

허깅페이스는 크게 트랜스포머 모델과 토크나이저를 활용할 때 사용하는 transformers 라이브러리와 데이터셋을 공개하고 쉽게 가져다 쓸 수 있도록 지원하는 datasets 라이브 러리를 제공해 트랜스포머 모델을 쉽게 학습하고 추론에 활용할 수 있도록 돕는다.

이제부터 허깅페이스 트랜스포머 라이브러리가 트랜스포머 모델 활용을 얼마나 쉽고 효율적으로 만들어 주는지 이해하기 위해 간단히 모델과 토크나이저를 불러오고 모델 에 입력해 보는 실습을 진행해 본다.

코드부의 각 줄에 대해서는 이어지는 절에서 하나 씩 살펴볼 예정이므로, 이번 절에서는 허깅페이스 트랜스포머 라이브러리로 여러 모델 을 불러와 활용하는 것이 얼마나 쉬운지 확인하는 것에 우선 집중해 보자.

허깅페이스 트랜스포머를 활용하면 서로 다른 조직에서 개발한 BERT와 GPT-2 모델을 예제 3.1과 같이 거의 동일한 인터페이스로 활용할 수 있다. 

AutoModeL과 AutoTokenizer 클래스를 사용해 BERT 및 GPT-2 모델과 토크나이저를 불러오고 토큰화를 수행해서 모델에 입력으로 넣어준다. 
모델의 이름에 해당하는 `bert-base-uncased`와 gpt2 이외 에는 두 코드가 사실상 동일한데, 이런 편리함 때문에 허깅페이스 트랜스포머를 많이 활용한다

`BERT 과 GPT-2` 모델을 할용할 때 허깅 페이스 트랜스포머 코드 비교
```python
from transformers import AutoModel, AutoTokenizer
text = "What is Huggingface Transformers?"

# BERT 모델 활용
bert_model = AutoModel.from_pretrained("bert-base-uncased") # 모델 불러오기
bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # 토크나이저 불러오기 
encoded_input = bert_tokenizer(text, return_tensors='pt') # 입력 토큰화 
bert_output = bert_model(**encoded_input ) # 모델에 입력 # GPT-2 모델 활용

gpt_modeL = AutoModeL.from_pretrained('gptt') # 모델 불러오기

gpt_tokenizer= AutoTokenizer.from_pretrained('gpt2') # 토크나이저 불러오기 
encoded_input = gpt_tokenizer(text, return_tensors='pt') # 입력 토큰화 
gpt_output= gpt_modeL(**encoded_input ) # 모델에 입력
```
허깅페이스 트랜스포머의 강력한 장점을 알아봤으니 이제 허깅페이스 허브를 탐색해 보자

## 허깅 페이스 허브 탐색하기 
허깅페이스의 허브는 다양한 사전 학습 모델과 데이터셋을 탐색하고 쉽게 불러와 사용 할 수 있도록 제공하는 온라인 플랫폼이다. 
또한 간단하게 자신의 모델 데모를 제공하 고 다른 사람의 모델을 사용해 볼 수 있는 스페이스Spaces도 있다. 
수많은 사용자들이 자신이 학습한 모델과 데이터셋을 공개하고, 유명한 모델이 모델 허브에 공개되지 않은 경우 허깅페이스 팀이 직접 모델을 변환해 공개하기도 한다. 

이 글을 쓰고 있는 시점(2024년 7월)을 기준으로 75만 개 이상의 사전 학습 모델과 17만 개 이상의 데이터셋, 21만 개 이상의 스페이스가 공개돼 있다. 
이번 절에서는 허깅페이스 허브의 모델, 데이 터셋, 스페이스에 대해 설명하고 탐색하는 방법을 살펴본다.

### 모델 허브
모델 허브에는 그림 3.1과 같이 어떤 작업Tasks에 사용하는지, 어떤 언어Languages로 학 습된 모델인지 등 다양한 기준으로 모델이 분류되어 있다. 
그림에서 '모델 분류'로 강조 표시한 박스 안에서 Tasks를 선택하면 작업 종류에 따라 모델을 필터링할 수 있다. 

모델 허브에서는 자연어 처리Natural Language Processing 뿐만 아니라 컴퓨터 비전Computer Vision, 오디오 처리Audio, 멀티 모달Mutimodal 등 다양한 작업 분야의 모델을 제공한다. 

모델 허브를 통해 사용자는 자신이 필요한 작업 분야와 언어 등에 따라 활용할 수 있는 사전 학 습 모델이 있는지 탐색할 수 있고 해당 분야에서 어떤 모델이 많이 사용되는지 확인할 수 있다. 

또한 전체 검색으로 강조 표시한 박스에서 검색하면 모델, 데이터셋, 스페이스, 사용자 등을 검색할 수 있다. 
검색창 오른쪽으로 여러 아이콘이 있는데, 모델을 클릭하면 그림 3.1과 같이 모델을 탐색할 수 있는 화면이 나오고, 데이터셋을 클릭하면 허깅페 이스의 datasets 라이브러리에서 제공하는 데이터셋을 탐색할 수 있는 화면으로 이동한다.

스페이스를 누르면 공개된 스페이스를 탐색할 수 있는 화면으로 이동한다
![스크린샷-2025-06-06-오후-7.12.10.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.12.10.png)
그림 3.2는 2024년 2월 구글이 공개한 젬마Gemma 모델의 화면이다. 

상단에서 모델의 이름과 요약된 정보를 아이콘 형태로 확인할 수 있다. 
어떤 작업을 위한 모델인지(예: 텍 스트 생성), 라이선스 유형 등을 확인할 수 있다. 
화면 왼쪽에는 모델에 대한 설명이 있다. 

필수사항은 아니기 때문에 모든 모델에 설명이 적혀 있는 것은 아니나, 잘 작성된 모 델 카드의 경우 모델의 성능, 관련 있는 논문 소개, 사용 방법 등의 정보를 제공한다. 

화면 오른쪽으로는 모델의 다운로드 수 추이를 볼 수 있는 모델 트렌드 그래프가 있고 그 아래쪽으로 모델을 간단히 테스트해 볼 수 있는 주론 API가 있다.
![스크린샷-2025-06-06-오후-7.13.00.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.13.00.png)
### 데이터셋 허브
데이터셋 허브 화면은 그림 3.3과 같이 모델 허브와 거의 동일한 형태다. 
모델 허브와 달리 분류 기준에 데이터셋 크기(Size), 데이터 유형(Format) 등이 추가로 있고 선택한 기준 에 맞는 데이터셋을 보여준다는 점이 다르다.

![스크린샷-2025-06-06-오후-7.13.33.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.13.33.png)
그림 3.4는 이번 장에서 실습에 사용할 [KILUE 데이터셋 페이지다](https://huggingface.co/datasets/klue). 
데이터셋 페이지의 상단에서는 데이터셋의 이름과 작업 종류, 크기, 언어, 라이선스 등 요약 정보를 확인할 수 있고, 화면 중앙과 같이 데이터셋을 바로 확인 할 수 있는 데이터셋 뷰어 기능을 제공 한다. 

데이터셋 뷰어 아래로 데이터셋에 대한 설 명을 제공한다. 데이터셋에 대한 설명은 필수사항이 아니기 때문에 제공하지 않는 데이 터셋도 있다.

대표적인 한국어 데이터셋 중 하나인 KLUE는 (한국어 언어 이해 평가 Korean Language Understanding Evaluation의 약자)로 텍스트 분류, 기계 독해, 문장 유사도 판단 등 다양한 작 업에서 모델의 성능을 평가하기 위해 개발된 벤치마크 데이터셋이다. 

KLUE에는 기계 독해 능력 평가를 위한 MRCMachine Reading Comprehension 데이터, 토픽 분류 능력 평가를

위한 YNAT(Younhap News Agency news headlines for Topic Classification) 데이터 등 8개의 데이터가 포함돼 있다. 
하나의 데이터셋에 여러 데이터셋이 포함된 경우 그림 3.4와 같이 서브셋 Subset으로 구분한다. 
유형split은 일반적으로 학습용(rain, 검증용Vaidation, 테스트용test 으 로 구분되는데, 정해진 것은 아니고 데이터셋에 따라 다른 이름을 사용하거나 다른 구분이 있기도 하다.
![스크린샷-2025-06-06-오후-7.15.19.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.15.19.png)

### 모델 데모를 공개하고 사용할 수 있는 스페이스
마지막으로 스페이스는 사용자가 자신의 모델 데모를 간편하게 공개할 수 있는 기능이다. 

모델을 개발하다 보면, 동료에게 모델 데모를 보여줘야 하는 경우도 있고 수업이나 발표를 위해 모델이 실행되는 화면이 필요한 경우도 있다. 
이때 로컬에서 실행되는 주 피터 노트북보다는 웹 페이지로 공유하는 것이 훨씬 편리한데, 스페이스를 사용하면 별 도의 복잡한 웹 페이지 개발 없이 모델 데모를 공유할 수 있다. 
스페이스 화면에 들어가 면 그림 3.5와 같이 다양한 모델이 공개된 것을 확인할 수 있다.
![스크린샷-2025-06-06-오후-7.16.06.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.16.06.png)
그림 3.6은 그림 3.5의 다양한 스페이스 중 2024년 2월 공개된 물체 인식object detection 모델인 Yolov9의 화면이다.
화면 왼쪽에 모델 추론에 사용할 이미지를 업로드할 수 있 는 영역이 있고, 그 아래로 사용할 모델의 종류와 추론에 사용할 모델의 설정을 선택할 수 있는 영역이 있다. 
이미지를 업로드하고 화면 하단의 Interence(추론) 버튼을 누르면 화면 오른쪽에 추론 결과가 표시된다.
![스크린샷-2025-06-06-오후-7.16.31.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.16.31.png)
그림 3.7은 YOLOv9 모델에 OpenAL의 이미지 생성 모델인 DALL-E 3를 사용해 생성 한 강아지와 고양이 사진을 입력해 인식한 결과다. 
이미지 왼쪽에 있는 강아지와 오른 쪽에 있는 두 마리의 고양이를 잘 인식했고, 공도 'sports ball'로 잘 인식한 것을 확인할 수 있다.
![스크린샷-2025-06-06-오후-7.17.06.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.17.06.png)
허깅페이스는 다양한 오픈소스 LLM과 그 성능 정보를 게시하는 리더보드Leaderboard를 운영하고 있다. 
리더보드는 모델 데모는 아니지만, 모델의 성능을 비교하는 웹 페이지 형태이기 때문에 스페이스를 활용해 제공하고 있다. 

많은 오픈소스 모델이 새롭게 공개 되고 있기 때문에 어떤 모델을 사용하는 것이 좋을지 판단하기 쉽지 않다. 
이럴 때 리 더보드를 살펴보면 각 모델의 크기와 성능을 한눈에 비교할 수 있기 때문에 탐색에 큰 도움이 된다. 
영어 데이터로 학습된 LLM의 리더보드는 [Open LLM Leaderboard](https:// huggingface.co/spaces/HuggingFacel14/open llm leaderboard)에서 확인할 수 있다. 

한국어 LLM은 그림 3.8과 같이 업스테이지Upstage에서 운영하는 [한국어 LIM 리더보드](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)에서 확인할 수 있다. 

두 리더보드 모두 비슷한 형태인데, 리더보드의 하단에 모델과 각 모델의 벤치마크 성능을 표시한다. 
이때 그림 왼쪽의 Select columns to show (컬럼 선택) 섹션을 보면 표에 어떤 컬럼을 표시할지 선택할 수 있다. 
Ko-XX로 표시된 것은 모델의 성능을 평가하는 벤치마크 데이터셋의 종류이고 Average(평균)는 벤치마크 평가의 평균 점수를 말한다. 

나머지는 모델의 정보를 나타내는 옵션이다. 그림 오른쪽에서 선택할 수 있는 섹션 은 탐색할 모델의 종류를 필터링하는 옵션이다. 

위에서부터 설명하면, Model types (모 델 타입) 섹션에서는 모델 학습 방식에 따라 사전 학습(pre-trained), 지도 미세 조정 학습(instruction-tuned) , 강화 학습(RL-tuned) 한 모델을 선택할 수 있다. 
Precision (정밀도) 섹션에서는 모델 파라미터의 데이터 형식에 따라 모델을 필터링할 수 있다. 

그림에서는 float16 형식 을 사용하는 모델을 선택하고 있다. Model sizes (모델 크기) 섹션에서는 10억 개 파라미 터 단위로 모델의 크기를 선택할 수 있다

![스크린샷-2025-06-06-오후-7.19.42.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.19.42.png)

## 허깅페이스 라이브러리 사용법 익히기

허깅페이스 허브를 통해 자신에게 맞는 모델과 데이터셋을 찾았다면, 이제는 코드를 통 해 모델과 데이터셋을 불러와 활용하는 방법을 익혀보자. 
모델을 학습시키거나 추론하 기 위해서는 모델, 토크나이저, 데이터셋이 필요한데, 이번 절에서는 하나씩 코드를 통 해 사용하는 방법을 익힌다. 
이어지는 절에서 모델 학습과 추론을 진행해 볼 텐데, 이번 절에서 익힌 내용을 바탕으로 진행하니 꼭 숙지하고 넘어가자.

### 모델 활용하기
허깅페이스 트랜스포머 라이브러리를 사용하면 허깅페이스 모델 허브의 모델을 쉽게 불러와 사용할 수 있다. 

허깅페이스 모델을 불러오기 전에 꼭 알아야 하는 사실이 있는데, 허깅페이스에서는 모델을 바디body와 헤드head로 구분한다는 점이다. 

이렇게 구분하는 이유는 같은 바디를 사용하면서 다른 작업에 사용할 수 있도록 만들기 위해서다. 
예를 들어, 그림 39와 같이 바디는 모두 구글에서 개발한 BERT 모델을 사용하지만 사용하 려는 작업에 따라 서로 다른 헤드를 사용할 수 있다. 

그림 3.9(a)는 문장 전체가 긍정인 지 부정인지를 분류하는 모델인데, 이때는 바디가 반환하는 여러 잠재 상태hidden state 중 가장 앞에 있는 [CLS] 토큰의 데이터만 받아 예측에 사용한다. 

그림 3.9(b)는 각 토큰이 사람이나 장소에 해당하는지 판단하는 개체명 인식(named entity recognition) 모델인데, 각 토 큰에 대해 판단해야 하기 때문에 모든 토큰의 데이터를 받아 각각 사람인지 장소인지를 예측한다. 
그림으로 표기된 것은 사람이나 장소에 해당하지 않는다는 예측 결과를 나타낸 것이다.

![스크린샷-2025-06-06-오후-7.21.18.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.21.18.png)
허깅페이스 트랜스포머 라이브러리에서는 모델의 바디만 불러올 수도 있고, 헤드와 함께 불러올 수도 있다. 
앞으로 모델을 세 번 불러와 볼텐데, 첫 번째로는 바디만 불러오고, 두 번째로는 헤드와 함께 불러오고, 마지막으로 헤드가 함께 있는 모델에 바디만 불러온다. 각각의 경우를 비교해 보자.

예제 3.2를 실행해 모델 바디를 불러와 보자.
```python
from transformers import AutoModel
model_id = 'klue/roberta-base'
model = AutoModel. from_pretrained(model_id)
```

AutoModel은 모델의 바디를 불러오는 클래스로, from_pretrained() 메서드에서 인자 로 받는 model_id에 맞춰 적절한 클래스를 가져온다. 
modeL_id가 허깅페이스 모델 허 브의 저장소 경로(예: klue/roberta-base)인 경우 모델 허브에서 모델을 다운로드하고, 로컬 경로인 경우(예:/textcassification) 지정한 로컬 경로에서 모델을 불러온다. 

예제 3.2에서는 모델 아이디로 klue/roberta-base를 지정해 [허깅페이스 링크](https:// huggingface.co/klue/roberta-base)의 모델을 내려받아 model 변수에 저장한다.

해당 모델은 RoBERTa 모델을 한국어로 학습한 모델인데, 여기서 RoBERTa는 구글의 BERT를 개선한 모델이다.

그렇다면 AutoModel 클래스는 어떻게 klue/roberta-base 저장소의 모델이 ROBERTa 계열의 모델인지 알 수 있을까? 허깅페이스 모델을 저장할 때 configjson 파일이 함께 저장되는데, 해당 설정 파일에는 예제 3.3과 같이 모델의 종류(model_type), 여러 설정 파라미터(num_attention_heads, num_hidden_layers 등), 어휘 사전 크기(vocab_size), 토크나이저 클래스(tokenzier_class) 등이 저장된다. 

AutoModel과 AutoTokenizer 클래스는 configjson 파일을 참고해 적절한 모델과 토크나이저를 불러온다. 
예제 3.3에서 klue/ roberta-base의 modeL_typeo "roberta"로 되어 있는데, AutoModel은 이 정보를 통해 RoBERTa 모델을 불러온다.

```json
{
	"architectures": ["RobertaForMaskedLM"],
	"attention_probs_dropout_prob": 0.1,
	"model_type" : "roberta",
	"num_attention _heads": 12,
	"num_hidden_layers": 12,
	...,
	"pad_token_id": 1,
	"type_vocab_size": 1,
	"vocab_size": 32000,
	"tokenizer_class": "BertTokenizer"
}
```

이번에는 텍스트 분류 헤드가 붙은 모델을 불러와 보자. 
예제 3.4를 실행하면, 모델 허브의 Samlowe/roberta-base-go_emotions 저장소에서 텍스트 분류 모델을 내려받 아 classification_model 변수에 저장한다. 

예제 3.4를 보면 모델 바디만 불러올 때와 달리 AutoModeLForSequenceClassification 클래스를 사용했는데, 이름에서도 알 수 있 듯이 텍스트 시퀀스 분류를 위한 헤드가 포함된 모델을 불러올 때 사용하는 클래스다.

Samlowe/roberta-base-go_emotions 모델에 대한 더 자세한 사항은 [허깅페이스 링크](https://huggingface.co/Samlowe/roberta-base-go_emotions)에서 확인할 수 있다

`분류 헤드가 포함된 모델 불러오기`
```python
from transformers import AutoModelForSequenceClassification
model_id = 'SamLowe/roberta-base-go_emotions'
classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)
```

Samlowe/roberta-base-go_emotions 모델은 그림 3.10과 같이 분류 헤드가 포함돼 있다. 
이 모델은 입력 문장이 어떤 감성을 나타내는지 분류하는데, 예를 들어 입력 문장 에 감탄admiration, 즐거움amusement, 화anger 등과 같은 감정이 포함돼 있는지를 분류한다

![스크린샷-2025-06-06-오후-7.26.07.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.26.07.png)
그렇다면 분류 헤드가 어떤 감정을 분류하는지 어떻게 알 수 있을까? 
앞서 모델 바디 를 불러올 때와 마찬가지로 저장소의 configjson 파일에서 확인할 수 있다. 

Samlowe/ roberta-base-go_emotions 저장소를 확인하면 예제 3.5와 같은 configjson 파일 을 찾을 수 있다.

앞서 본 klue/roberta-base와 동일하게 model_type은 "roberta"이 지만, 모델 아키텍처가 "~ForSequenceClasstf ication"으로서 분류를 위한 모델임을 알 수 있다. 
또 헤드의 분류 결과가 어떤 의미인지 확인할 수 있는 id2label을 갖고 있다.

id2Label을 살펴보면, 0은 감탄admirator, 1은 즐거움amusement, 2는 화anger를 의미한다

```json
{
	"_name_or_path": "roberta-base",
	"architectures": [
		"RobertaForSequenceClassification"
	],
		•••
		"model_type": "roberta",
		"id2label": {
			"0": "admiration",
			"1": "amusement",
			"2": "anger",
		}
	}
}
```

이제 마지막으로 텍스트 분류를 위한 아키텍처에 모델 바디만 불러와 보자. 앞서 살펴 본 대로AutoModeLForSequenceCLassfication 클래스를 사용하면 분류 헤드가 붙은 모델을 불러올 수 있다. 
이 클래스를 사용해 모델 바디 부분의 파라미터만 있는 klue/ roberta-base 저장소의 모델을 불러오면 어떻게 될까? 

예제 3.6을 실행하면 AutoModel ForSequenceClass if tication 클래스로 Klue/roberta-base 모델을 불러온다.

`분류 헤드가 랜덤으로 초기화된 모델 불러오기`
```python
from transformers import AutoModelForSequenceClassification
model_id = 'klue/roberta-base'
classification model = AutoModelForSequenceClassification.from_pretrained(model_id)
```

모델을 불러올 때 에러는 발생하지 않지만, 예제 3.7과 같은 경고가 나타난다. 
경고의 내 용은 모델의 바디 부분은 klue/roberta-base의 사전 학습된 파라미터를 불러왔지만

klue/roberta-base 모델 허브에서는 분류 헤드에 대한 파라미터를 찾을 수 없어 랜덤 으로 초기화했다는 것이다. 
분류 헤드는 랜덤으로 초기화됐기 때문에 그대로 사용해서 는 안 되고 추가 학습 이후에 사용하라고 안내하고 있다

`분류 모델에 바디만 있는 모델을 불러올 경우 경고 표시`
```shell
- Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj. bias', 'classifier dense weight', 'classifier.dense.bias', 'classifier.out_proj.
- weight '1  
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

- ﻿-> RobertaForSequenceClasstfication의 일부 부분은 klue/roberta-base에서 가져온 것이 아니고 새롭게 (랜덤으로) 초기화됐습니다. 따라서 작업에 활용하려면 학습시켜야 합니다.
```
경고의 내용을 명확히 이해하기 위해 그림으로 표현하면 그림 3.11과 같다. 

바디 부분은 klue/roberta-base 모델에서 가져왔지만 분류 헤드는 랜덤으로 초기화된 상태다. 
현재 상태에서는 분류 헤드가 학습되지 않았기 때문에 의미 있는 분류를 하지 못한다. 
그렇다면 어떻게 분류 헤드를 학습해 분류 모델을 만들 수 있을까? 
이어지는 절에서 한국어 데이터를 불러와 기사를 적절한 카테고리로 분류하는 모델을 만들어 본다.

![스크린샷-2025-06-06-오후-7.30.25.png](/img/이미지 창고/스크린샷-2025-06-06-오후-7.30.25.png)
### 토크나이저 활용하기
토크나이저는 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환한다. 

필요한 경우 특수 토큰을 추가하는 역할도 한다. 
토크나이저도 학습 데이터를 통해 어휘 사전을 구축하기 때문에 일반적으로 모델과 함께 저장한다. 
토크나이저도 모델을 불러올 때와 마찬가지로 허깅페이스 모델 저장소 아이디를 통해 불러올 수 있다. 
허깅페이스 허브에서 모델과 토크나이저를 불러오는 경우 동일한 모델 아이디로 맞춰야 한다.

예제 3.8을 실행하면 AutoTokenizer 클래스를 통해 앞서 모델을 불러올 때도 사용한

klue/roberta-base 저장소의 토크나이저를 불러온다. [모델 저장소](https://huggingface.co/klue/roberta-base/tree/main)를 확인하면 tokenizer_config,json과 tokenizer json

2개의 파일을 확인할 수 있는데, 모델에 대한 정보가 configjson 파일에 저장돼 있던 것처럼 토크나이저에 대한 정보를 저장하고 있다.
tokenizer_config,json은 토크나이저 의 종류나 설정에 대한 정보를 갖고 있고 tokenizer json 파일은 실제 어휘 사전 정보를 갖고 있다.

```python
from transformers import AutoTokenizer
model_id = 'klue/roberta-base'
tokenizer = AutoTokenizer. from_pretrained(model_id)
```

토크나이저는 예제 3.9와 같이 사용할 수 있다. 
tokenizer에 텍스트를 입력하면 토큰 아이디의 리스트인 tnput_ids, 토큰이 실제 텍스트인지 아니면 길이를 맞추기 위해 추 가한 패딩padding 인지 알려주는 attention_mask, 토큰이 속한 문장의 아이디를 알려주는 token_type_ids를 반환한다. input_ids는 토큰화했을 때 각 토큰이 토크나이저 사 전의 몇 번째 항목인지를 나타낸다. 

input_ids의 첫 번째 항목은 0이고 두 번째 항목은 9157인데, 각각 [CL.S]와 '토크'에 대응되는 것을 확인할 수 있다. attention_mask가

1이면 패딩이 아닌 실제 토큰임을 의미한다. 
token_type_ids가 0이면 일반적으로 첫 번 째 문장임을 의미하는데 더 정확한 내용은 예제 3.16에서 설명한다. 
토큰 아이디를 다 시 텍스트로 돌리고 싶다면 토크나이저의 decode 메서드를 사용하면 된다. 

이때 [CLS] 나 [SEP] 같은 특수 토큰이 추가된 것을 확인할 수 있는데 3, 만약 특수 토큰을 제외하고 싶다면 skip_special_tokens 인자를 True로 설정하면 된다

```python
tokenized = tokenizer("토크나이저는 텍스트를 토큰 단위로 나눈다")
print( tokenized)

# {'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 21,
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 01,
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11}

print( tokenizer.convert_ids_to_tokens(tokenized[ 'input_ids']))
# ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '#를', '토', '##큰', '단위', '#로', '나눈다', '[SEP]']
print( tokenizer. decode(tokenized[ 'input_ids' ]))
# [CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]
print( tokenizer. decode(tokenized[ 'input_ids'], skip_special_tokens=True))
# 토크나이저는 텍스트를 토큰 단위로 나눈다
```

토크나이저는 한 번에 여러 문장을 처리할 수도 있다. 
예제 3.10에서는 2개의 문장('첫 번째 문장'과 '두 번째 문장)을 리스트로 함께 넣었다. 
출력 결과를 확인하면 `input_ids`, `attention_mask`, `token_type_ids` 모두 각 문장을 토큰화해 2개의 리스트를 반환한 것을 확인할 수 있다.

토크나이저에 여러 문장 넣기
```python
tokenizer(['첫 번째 문장', 두 번째 문장])

# {'input_ids': [0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2](0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2),
# 'token_type_ids': [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0](0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0),
# 'attention_mask': [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1](1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1)}
```

한 번에 2개의 문장을 모델에 넣어야 하는 경우가 있다. 
예를 들어 2개의 문장이 서로 원인과 결과 관계인지 학습시키고 싶다면, 두 문장을 한번에 모델에 입력해야 한다. 
이 때 2개의 문장이 하나의 데이터라는 것을 표시하기 위해 예제 3.11과 같이 한번 더 리스트로 감싸준다. 
출력 결과를 확인하면 예제 3,10과는 달리 하나의 결과만 반환하는 것을 확인할 수 있다.

```python
tokenizer(['첫번째 문장', 두 번째 문장]('첫번째 문장', 두 번째 문장))

# {'input_ids': [0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2](0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2),
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0](0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1](1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1)}
```

토크나이저의 `batch_decode()` 메서드를 사용하면 `input_ids` 부분의 토큰 아이디를 문자 열로 복원할 수 있는데, 예제 3.10과 예제 3.11의 토큰을 문자열로 복원하면 예제 3.12 와 같다.
기본적으로 토큰화를 하면 [CLS] 토큰으로 문장을 시작하고 [SEP]으로 문장을 끝내는데, 2개의 문장을 한번에 토큰화하면 [SEP]으로 두 문장을 구분한다. 
특수 토큰 은 모델의 아키텍처에 따라 달라질 수 있으니 사용하려는 토크나이저가 어떤 특수 토큰 을 사용하는지 확인해 볼 필요가 있다.

토큰 아이디를 문자열로 복원
```python
first_tokenized_result= tokenizer(['첫 번째 문장', 두 번째 문장])['input_ids']
tokenizer. batch_decode(first_tokenized_result)
# ['[CLS] 첫 번째 문장 [SEP]', [CLS] 두 번째 문장 [SEP]']
second_tokenized_result= tokenizer(['첫 번째 문장', 두 번째 문장']('첫 번째 문장', 두 번째 문장'))['tnput_ids']
tokenizer. batch _decode(second_tokenized_result)
# ['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']
```

토큰화 결과 중 token_type_ids는 문장을 구분하는 역할을 한다. 
BERT는 학습할 때 2개 의 문장이 서로 이어지는지 맞추는 NSP(Next Sentence Prediction) 작업을 활용하는데, 이를 위해 문장을 구분하는 토큰 타입 아이디를 만들었다. 

그래서 BERT 모델의 토크나이저 를 불러오면 예제 3.13에서 보듯이 문장에 따라 토큰 타입 아이디를 구분한다. 
이 코드에서 `klue/bert-base` 토크나이저를 사용하는 경우
첫 번째 문장의 토큰 타입 아이디는 0, 두 번째 문장의 토큰 타입 아이디는 1이다. 
하지만 `klue/roberta-base`의 경우 `token_type_ids`가 모두 0인 것을 볼 수 있는데 ,
RoBERTa 계열 모델의 경우 NSP 작업을 학습 과정에서 제거했기 때문에 문장 토큰 구분이 필요 없다. 

실제 3에서 영어 버전의 원본 `roberta-base` 토크나이저로 영어 문장을 토큰화하면 결과에 `token_type_ids` 항목 자체가 없는 것을 확인할 수 있다.

```python
bert_tokenizer = AutoTokenizer. from_pretrained 'klue/bert-base') # 1
bert_tokenizer(['첫 번째 문장', '두 번째 문장']('첫 번째 문장', '두 번째 문장'))
# {'input_ids': [2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3](2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3),
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1](0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1),
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1](1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)}

roberta_tokenizer = AutoTokenizer. from_pretrained( 'klue/roberta-base' ) # 2
roberta_tokenizer(['첫 번째 문장, 두 번째 문장']('첫 번째 문장, 두 번째 문장'))
# {'input_ids': [0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2](0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2),
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0](0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1](1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)}

en_roberta_tokenizer = AutoTokenizer. from_pretrained(' roberta-base' ) # 3

en_roberta_tokenizer(['first sentence', 'second sentence']('first sentence', 'second sentence') )
# {'input_ids': [0, 9502, 3645, 2, 2, 10815, 3645, 2](0, 9502, 3645, 2, 2, 10815, 3645, 2),
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1](1, 1, 1, 1, 1, 1, 1, 1)}
```

마지막으로 attention_mask는 해당 토큰이 패딩 토큰인지 실제 데이터인지에 대한 정보 를 담고 있다. 
패딩은 모델에 입력하는 토큰 아이디의 길이를 맞추기 위해 추가하는 특 수 토큰이다. 

예제 3.14와 같이 토크나이저의 padding 인자에 `'Longest'`를 입력하면 입력한 문장 중 가장 긴 문장에 맞춰 패딩 토큰을 추가한다.
예제 3.14의 예시 문장을 보 면 두 번째 문장인 '두 번째 문장은 첫 번째 문장보다 더 길다.'가 첫 번째 문장보다 더 길다. 
따라서 더 긴 문장에 맞춰 패딩 토큰을 추가한다면 첫 번째 문장에 패딩이 추가될 것이다. 

실제로 예제 3.14의 실행 결과를 보면 `tnput_ids`에서 첫 번째 문장에 패딩 토큰 (토큰 아이디 = 1)이 6개 추가된 것을 확인할 수 있고 `attention_mask`에는 패딩 토큰을 나타내는 숫자 0이 6개 붙은 것을 확인할 수 있다.

attention_mask 확인
```python
tokenizer(['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='Longest')
# {'input_ids': [[O, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1],
# [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 3632, 831, 647, 2062, 18, 2]],
# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 01,
#[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
```

### 데이터셋 활용하기
datasets 라이브러리를 사용하면 앞서 허깅페이스 허브에서 살펴봤던 데이터셋을 코드 로 불러올 수 있다. 
예제 3.15를 실행해 KLUE 데이터셋의 서브셋 중 하나인 MRC 데이 터셋을 내려받아 보자.

KLUE MRC 데이터셋 다운로드
```shell
from datasets import load_dataset
klue_mrc_dataset = load _dataset ( 'klue', 'mrc' )
# Klue_mrc_dataset_only_train = load_dataset( 'klue', "mrc', split='train')
```

데이터셋을 불러오는 함수는 Load_dataset 으로서, 데이터셋의 이름인 klue와 서브 셋 이름인 mrc를 `Load_dataset` 함수에 인자로 전달해서 MRC 데이터셋을 내려받았다. 
`klue_mrc_dataset`의 내용을 확인하면 그림 3.12와 같다. 

데이터셋에는 학습(train) 과 검증(validation) 유형의 데이터가 각각 17,554개, 5841개 있으며 제목(title), 내용 (context)과 질문(question), 정답(answers) 등의 컬럼이 있다.

![스크린샷-2025-06-06-오후-8.16.27.png](/img/이미지 창고/스크린샷-2025-06-06-오후-8.16.27.png)

만약 유형이 train인 데이터만 받고 싶다면 예제 3.15에서 주석 처리된 코드와 같이 `load_dataset()` 함수에 `SpLit='train'` 인자를 입력하면 된다.

데이터셋 저장소에 있는 데이터만 불러올 수 있는 것은 아니고 
예제 3.16과 같은 코드 로 로컬에 있는 파일이나 파이썬 객체를 받아 데이터셋으로 사용할 수 있다. 
코드를 보면 로컬에 저장된 CSV 파일을 불러오기 위해 `Load_dataset`에 데이터의 형식인 CSV를 지 정하고 파일 경로를 `data_files`의 인자로 전달했다. --> 1

파이썬 딕셔너리를 데이터셋으 로 변환하고 싶은 경우 Dataset 클래스의 `from_dict` 메서드를 사용해 데이터셋으로 변 환할 수 있다. -->2

데이터 처리에서 많이 사용하는 판다스 데이터프레임을 데이터셋으로 변환하고 싶은 경우 Dataset 클래스의 `from_pandas` 메서드를 사용하면 된다. --> 3

더 다양 한 데이터 형식을 다루는 방법에 대한 자세한 사항은 [Datasets 불러오기Load 공식 문서](https://huggingface.co/docs/datasets/loading)에서 확인할 수 있다.

로컬의 데이터 활용하기
```python
from datasets import load_dataset

# 로컬의 CSv 데이터 파일을 활용
dataset = load_dataset("csv", data_files="my_file.csv") ®

# 파이썬 딕셔너리 활용
from datasets import Dataset

my_dict = {"a": [1, 2, 3]}
dataset = Dataset. from_dict(my_dict) ®
# 판다스 데이터프레임 활용
from datasets import Dataset import pandas as pd
df = pd.DataFrame({"a": [1, 2, 31})
dataset = Dataset.from_pandas(df) 3
```

## 모델 학습 시키기
지금부터 한국어 기사 제목을 바탕으로 기사의 카테고리를 분류하는 텍스트 분류 모델 을 학습하는 실습을 진행해 보자. 
먼저 실습에 사용할 데이터셋을 준비하고 모델과 토 크나이저를 불러와 모델을 학습시킨다. 

허깅페이스 트랜스포머에서는 간편하게 모델 학습을 수행할 수 있도록 학습 과정을 추상화한 트레이너Trainer API를 제공한다. 
트레이너 API를 사용하면, 학습을 간편하게 할 수 있다는 장점이 있지만 내부에서 어떤 과정을 거치는지 알기 어렵다는 단점도 있다. 
따라서 이번 실습에서는 먼저 전체적인 흐름을 이해하기 위해 트레이너 API를 사용하는 실습을 진행하고 내부 동작을 이해할 수 있도 록 트레이너 API를 사용하지 않고 모델을 학습시키는 실습도 진행한다. 
마지막으로, 학습을 마친 모델을 저장하거나 공유할 수 있도록 허깅페이스 허브에 업로드하는 방법을 알아본다.

### 데이터 준비
실습 데이터는 KLUE 데이터셋의 YNAT 서브셋을 활용한다. 
YNAT에는 연합 뉴스 기사 의 제목과 기사가 속한 카테고리 정보가 있다. 
이번 실습에서는 연합 뉴스 기사의 제목 을 바탕으로 카테고리를 예측하는 모델을 만들어 본다. 

실습을 준비하기 위해 예제 3.17 을 실행해 데이터셋을 불러오자. 
KLUE의 YNAT 학습 및 검증 데이터셋을 다운로드해 각각 `klue_tc_train`, `klue_tc_eval` 변수에 저장한다.

```python
klue_tc_train = load_dataset( 'klue', 'ynat', split='train' )
klue_tc_eval = load _dataset( 'klue', 'ynat', split='validation' )
klue_tc_train
```

`klue_tc_train`에 저장된 데이터를 확인하면 그림 3.13과 같이 뉴스 제목(title), 뉴스가 속한 카테고리(label) 등의 컬럼으로 이뤄진 총 45,678개의 데이터가 있다.
![스크린샷-2025-06-08-오전-8.08.11.png](/img/이미지 창고/스크린샷-2025-06-08-오전-8.08.11.png)
다음으로 개별 데이터의 형태를 보기 위해 첫 번째 데이터를 예시로 살펴보자. 
`klue_tc_ train[0]`의 데이터를 확인하면 그림 3.14와 같다. 
제목(title)으로 '유튜브 내달 2일까지 크리에이터 지원 공간 운영 이라는 뉴스 제목이 있고, 레이블(Label)은 3으로 되어 있다.
![스크린샷-2025-06-08-오전-8.08.44.png](/img/이미지 창고/스크린샷-2025-06-08-오전-8.08.44.png)
- guid: 데이터의 고유 ID
- ﻿﻿title: 뉴스 제목
- ﻿﻿Label: 속한 카테고리 ID
- ﻿﻿url: 뉴스 링크
- ﻿﻿date: 뉴스 입력 시간

데이터에는 레이블 값이 숫자로 되어 있어 어떤 카테고리인지 알기 어려운데, 예제 3.19 를 통해 카테고리를 확인할 수 있다. 
이 코드에서는 데이터셋의 정보를 저장하고 있는 features 속성에서 Label 컬럼의 항목별 이름을 확인한다. 
그림 3.14의 예시 데이터는 레이블이 3이었는데, 예제 3.19에 따르면 생활문화 카테고리임을 확인할 수 있다(레이 블은 0부터 시작하기 때문에 3은 네 번째 항목이다).

```python
klue_tc_train. features [' label '].names
# ['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치' ]
```

분류 모델을 학습시킬 때 guid, urL, date 컬럼은 필요하지 않기 때문에 예제 3.18의 코드로 데이터에서 제거한다.
`remove_columns` 메서드에 제거할 컬럼의 이름(guid, url, date)을 리스트 형태로 전달하면 데이터셋에서 지정한 컬럼을 삭제한다. 
예제 코드의 출 력 결과를 보면 title과 Label 컬럼만 남은 것을 확인할 수 있다.

```python
klue_tc_train = klue_tc_train. remove_columns( ['guid', 'url', 'date'])
klue_tc_eval = klue_tc_eval. remove_columns( ['guid', 'url', 'date' ])
klue_tc_train

# 출력 결과
# Dataset(1
# features: ['title', 'label'],
# num_rows: 45678
# })
```

이제 카테고리를 확인하기 쉽도록 LabeL_str 컬럼을 추가해 보자. 
예제 3.19에서 데이 터셋의 정보를 갖고 있는 features 속성에서 label 컬럼을 확인하면, 레이블 ID와 카테 고리를 연결할 수 있는 `ClassLabel` 객체가 있다. 해당 객체에는 ID를 카테고리로 변환하는 `int2str`이라는 메서드가 있다. 
`int2str` 메서드에 아이디 1을 입력하면 '경제' 카테고리를 반환하는 것을 확인할 수 있다. 

이를 활용해 label 컬럼의 숫자형 아이디를 카테고리 이름으로 변환하는 `make_str_Label` 함수를 정의하고 데이터셋의 map 메서드를 사용 해 `label_str` 컬럼을 추가 한다. 
예제 3.19의 마지막 출력 결과를 보니 `label_str` 컬럼이 잘 추가된 것을 확인할 수 있다.

```python
klue_tc_train. features[ 'label']
# ClassLabel(names=['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'], id=None)

klue_tc_train. features[ 'label '].int2str (1)
# '경제'

klue_t_label = klue_tc_train. features [ 'label']
def make_str_label(batch):
	batch[ 'label_str'] = klue_tc_label. int2str(batch[ 'label' ])
	return batch

klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000 )

klue_tc_train[0]
#{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'Label: 3, 'Label_str: '생활문화'
```

빠른 실습 진행을 위해 학습 데이터는 모두 사용하지 않고 10,000개만 추출해 사용한다. 
데이터셋에서 `train_test_split` 메서드를 사용하면 입력한 `test_size` 값에 맞춰 학습 데이터셋과 테스트 데이터셋으로 분리한다. 
이제 예제 3.20에서는 `klue_tc_train`에 서 `test_size`를 10,000으로 지정해 랜덤으로 10,000개의 데이터를 추출한다. 
학습이 잘되고 있는지 확인할 검증 데이터와 성능 확인에 사용할 테스트 데이터는 검증 데이터 셋(`klue_tc_eval`)에서 각각 1,000개씩 뽑아 사용한다. 
예제 3.20을 실행하면 학습 데이 터 중 10,000개만 남기고 검증 데이터 중 1,000개 검증용, 다른 1,000개는 테스트용 으로 분리한다.

```python
train_dataset = klue_tc_train. train_test_split(test_size=10000, shuffle=True, seed=42)['test']
dataset = klue_tc_eval. train_test_split(test_size=1000, shuffle=True, seed=42)
test _dataset = dataset ['test' ]
valid _dataset = dataset[ 'train']. train_test_split(test_size=1000, shuffle=True, seed=42) ['test' ]
```

### 트레이너 API를 사용해 학습하기
허깅페이스는 학습에 필요한 다양한 기능(데이터로더 준비, 로깅, 평가, 저장 등)을 학습 인자(`TrainingArguments`)만으로 쉽게 활용할 수 있는 트레이너 API를 제공한다. 
먼저 트레이너 API를 사용해 학습을 수행하는 코드를 작성해 보자. 

예제 3.21은 필요한 라이브러 리를 불러오고 모델과 토크나이저를 불러와 데이터셋에 토큰화를 수행한다. 
`tokenize_function`은 데이터의 title 컬럼에 토큰화를 수행한다. 

학습에 사용할 분류 모델을 불 러오기 위해 `AutoModeLForSequenceClassification` 클래스로 `klue/roberta-base` 모델 을 불러온다. 
앞서 살펴본 대로 모델 바디의 파라미터만 있는 `kue/roberta-base` 모델을 불러오면 분류 헤드 부분은 랜덤으로 초기화된다. 

이 코드에서는 분류 헤드의 분류 클래스 수를 지정하기 위해 `num_Labels` 인자에 데이터셋의 레이블 수인 `len(train_dataset.features['Label'].names)`를 전달했다.

```python
import torch
import numpy as np 

from transformers import (
	Trainer, 
	TrainingArguments, 
	AutoModelForSequenceClassification,
	AutoTokenizer
)

def tokenize_function( examples):
	return tokenizer (examples["title"], padding="max_length", truncation=True)

model_id = "klue/roberta-base"
model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels= len(train_dataset. features[ 'label']. names) )

tokenizer = AutoTokenizer.from_pretrained(model_id)
train_dataset = train_dataset.map(tokenize_function, batched=True)
valid_dataset = valid_dataset.map( tokenize_function, batched=True)
test_dataset = test_dataset.map( tokenize_function, batched=True)
```

다음으로 예제 3.22에서 학습에 사용할 인자를 설정하는 `TrainingArguments`에 학습 인 자를 입력한다. 

학습 에포크 수(`num_train_epochs`), 배치 크기(`per_device_train_batch_size`), 
결과를 저장할 폴더(output_dir), 평가를 수행할 빈도(evaluation_strategy) 등을 설정한다. 

에포크 수는 1로, 배치 크기는 8로, 결과는 results 폴더에 저장하고 한 에포크 학습이 끝날 때마다 검증 데이터셋에 대한 평가를 수행하도록 `evaluation_strategy` 인 자를 "epoch"로 설정했다. 

다음으로 학습이 잘 이뤄지고 있는지 확인할 때 사용할 평가 지표를 정의한다(`compute_metrics`). 
모델의 예측 결과인 `eval_pred`를 입력으로 받아 예측 결과 중 가장 큰 값을 갖는 클래스를 `np.argmax` 함수로 뽑아 `predictions` 변수에 저장하고 `predictions`와 정답이 저장된 Labels가 같은 값을 갖는 결과의 비율을 정확도accuracy 로 결과 딕셔너리에 저장해 반환한다.

```python
training_args = TrainingArguments(
	output_dir="./results", num_train_epochs=1,
	per _device_train_batch_size=8, 
	per_device_eval_batch_size=8,
	evaluation_strategy="epoch",
	Learning_rate=5e-5, push_to_hub=False
)

def compute_metrics(eval_pred) :
	logits, labels = eval_pred
	predictions = np.argmax logits, axis=-1)
	return {"accuracy": (predictions == labels) .mean (
```

마지막으로, 예제 3.23의 코드로 Trainer에 앞서 준비한 데이터셋과 설정을 인자로 전 달하고 `train()` 메서드로 학습을 진행한다. 
학습이 끝나면 `evaluate()` 메서드로 테스트 데이터셋에 대한 성능 평가를 수행한다. 
출력 결과를 확인해 보면 약 84%의 정확도로 기사의 카테고리를 잘 분류했다.

```python
trainer = Trainer (
	model=model, 
	args=training_args, 
	train_dataset=train_dataset,
	eval_dataset=valid_dataset,
	tokenizer=tokenizer, 
	compute_metrics=compute_metrics,
)

trainer. train()

trainer.evaluate(test_dataset) # 정확도 0.8
```

허깅페이스가 제공하는 트레이너 API를 사용하면 데이터셋을 준비하고 학습 인자를 설 정하는 데 필요한 몇 줄의 코드만으로도 텍스트 분류를 위한 데이터셋을 준비하고 모델을 학습시킬 수 있다. 
다음으로 트레이너 API를 사용하지 않았을 때의 분류 모델 학습 방법을 알아보자.

### 트레이너 API를 사용하지 않고 학습하기
앞서 살펴본 트레이너 API는 추상화를 통해 간편하게 사용할 수 있다는 장점이 있지만 그만큼 내부 동작을 파악하기 어렵다는 단점이 있다. 

특히 파이토치나 텐서플로 같은 딥러닝 프레임워크를 사용해 직접 모델을 학습시켜 본 경험이 없다면 트레이너 API 내 부에서 어떤 일이 벌어지는지 알 수 없기 때문에 놀라움만큼이나 찜찜한 대상이 될 수 있다. 

따라서 트레이너 API 내부에서 어떤 일이 벌어지는지 한번은 확인해 보는 것이 좋다. 
지금부터 트레이너 API를 사용하지 않고 허깅페이스 모델을 학습시켜 보자.

예제 3.24에서는 Trainer를 사용한 예제와 비슷하게 모델과 토크나이저를 불러오고, 토큰화에 사용할 `tokenize_function` 함수를 정의한다. 
Trainer를 사용하는 예제에서는 Trainer가 내부적으로 수행하던 GPU의 모델 이동(model. to(device))을 직접 수행해야 한다는 차이가 있다.

```python
import torch
from tqdm.auto import tadm 
from torch.utils.data import DataLoader 
from torch.optim import AdamW

def tokenize_function(examples): #제목(title) 컬럼에 대한 토큰화
	return tokenizer (examples["title"], padding="max_length", truncation=True)

# 모델과 토크나이저 불러오기
device = torch. device ("cuda" if torch. cuda. is_available() else "cpu")
model_id = "klue/roberta-base"
model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset. features[' label ']. names) )
tokenizer = AutoTokenizer. from_pretrained(model_id)

model.to(device)
```

다음으로 예제 3.25의 코드를 사용해 데이터 전처리를 수행한다. 전처리를 위해 `make_dataloader` 함수를 정의했는데, 함수 내부에서 `tokenize_function` 함수를 사용해 토 큰화를 수행하고, `rename_coLumn` 메서드를 통해 기존에 `"label"`이었던 컬럼 이름을

`"Labels"`로 변경한다. titLe 컬럼의 토큰화를 수행했기 때문에 이제는 불필요해진 title 컬럼을 `remove_columns` 메서드를 사용해 제거한다. 
마지막으로 파이토치에서 제공하는 `DataLoader` 클래스를 사용해 데이터셋을 배치 데이터로 만든다. 
Trainer를 사용하면 예 제의 코드 중 토화를 제외한 나머지를 알아서 처리해 주는데, 그런 점에서 트레이너 API가 편리한 점이 많다.

```python
def make_dataloader(dataset, batch_size, shuffle=True):
	dataset = dataset.map( tokenize_function, batched=True) with_format( "torch" )
	
	# 데이터셋에 토큰화 수행
	dataset= dataset. rename_colum("Label", "Labels") # 컬럼 이름 변경 
	dataset = dataset.remove_colums(column_names=['title']) # 불필요한 컬럼 제거
	return DataLoader (dataset, batch_size=batch_size, shuffle=shuffle)

# 데이터로더 만들기
train_dataloader = make_dataloader train_dataset, batch_size=8, shuffle=True)
valid_dataloader = make_dataloader(valid _dataset, batch_size=8, shuffle=False)
test_dataloader = make_dataloader (test_dataset, batch_size=8, shuffle=False)
```

이제 학습과 평가에 사용할 함수를 만들어 보자. 
1. 예제 3.26에서 학습을 수행하는 `train_epoch` 함수에서는 먼저, train() 메서드를 사용해 모델을 학습 모드로 변경하고, 앞서 생성한 데이터로더에서 배치 데이터를 가져와 모델에 입력으로 전달한다. 
2. 배치 데이 터 안에는 토큰 아이디를 담고 있는 `tnput_ids`, 어텐션 마스크를 갖고 있는 `attention_mask`, 정답 레이블을 가진 Labels 키가 있는데, 각각 model에 인자로 전달해 모델 계산을 수행한다.
3. 모델의 계산을 거친 결과에는 레이블과의 차이를 통해 계산된 손실(10ss)이 있는데, 이 손실 값을 통해 역전파를 수행한다. 옵티마이저(`optinizer`)의 Step() 메서드 를 호출하면 역전파 결과를 바탕으로 모델을 업데이트한다. 

`total_Loss`의 경우 학습 이 잘되고 있는지 확인하기 위해 집계한다.

```python
def train_epoch(model, data_loader, optimizer) :
	model. train() O
	total loss = 0
	for batch in tqdm (data_loader):
		optimizer. zero_grad()
		tnput_ids = batch['tnput_ids']. to(device) # 모델에 입력할 토큰 아이디
		attention_mask= batch['attention_mask'].to(device) # 모델에 입력할 어션 마스크 
		labeLs = batch['Labels'].to(device) # 모델에 입력할 레이블
		outputs = model (input_ids, attention_mask=attention mask, labels=labels) # 7= A1 0
		loss = outputs.Loss # 손실
		loss.backward() # 역전파
		optimizer.step() # 모델 업데이트 ③
		total_loss += loss. item )
	avg_loss = total_loss / len(data_loader )
	return avg_loss
```

예제 3.27에서는 평가에 사용할 `evaluate` 함수를 정의한다. 
`evaluate` 함수는 모델 계산 을 수행하고, 손실을 집계하는 등 많은 부분에서 `train_epoch` 함수와 비슷하다. 

1. 하지만 모델을 학습 모드가 아닌 추론 모드로 설정하고, 
2. 모델 계산 결과의 `Logits` 속성을 가져와 `torch.argmax` 함수를 사용해 가장 큰 값으로 예측한 카테고리 정보를 찾고 
3. 실제 정답과 비교해 정확도를 계산하는 부분이 추가돼 있다
이를 통해 손실뿐만 아니라 정확도를 직접 확인할 수 있다.

```python
def evaluate(model, data_loader):
	model. eval() # 1
	total_loss = 0
	predictions = []
	true_labels = []
	with torch.no_grad():
		for batch in tqdm(data_loader):
			input_ids = batch[ 'input_ids']. to(device)
			attention_mask = batch[ 'attention_mask' ].to(device)
			labels = batch ['labels']. to (device)
			outputs = model (input_ids, attention _mask=attention_mask, labels=labels)
			logits = outputs. logits
			loss = outputs. loss
			total_loss += loss. item )
			preds = torch.argmax(logits, dim=-1) # 2
			predictions.extend(preds.cpu( ).numpy())
			true_labels .extend (labels.cpu().numpy ( ))
	avg_loss = total_loss / len(data_loader )
	accuracy = np. mean(np.asarray (predictions) == np. asarray(true_labels)) # 3
	return avg_loss, accuracy
```

마지막으로, 앞서 정의한 함수를 사용해 학습을 진행해 보자. 예제 3.28에서는 학습 에 포크 수(`num_epochs`)를 1로 설정하고 학습에 Adaml 옵티마이저를 사용한다. 
학습률(ur)도 이전 실습과 동일하게 설정했다. 
`for` 문을 통해 한 에포크 학습을 수행한다. 
이때 앞서 정의한 `train_epoch` 함수로 학습을, `evaluate` 함수로 성능을 평가한다.

```python
num_epochs = 1
optimizer = AdamW(model. parameters(), lr=5-5)

# 학습 루프
for epoch in range(num_epochs) :
	print( f"Epoch {epoch+1}/{num_epochs}")
	train_loss = train_epoch(model, train_dataloader, optimizer)
	print(f"Training loss: {train_loss}")
	valid_loss, valid_accuracy = evaluate(model, valid_dataloader)
	print(f"Validation loss: {valid_loss}")
	print(f"Validation accuracy: {valid_accuracy}")

# 테스트
-, test_accuracy = evaluate(model, test_dataloader)
print(f"Test accuracy: {test_accuracy}") # &E 0.82
```

지금까지 Trainer를 사용했을 때와 사용하지 않았을 때 모델을 학습시키는 방법을 알아 봤다. 
Trainer를 사용하면 간편하다는 장점이 있고, Trainer를 사용하지 않으면 내부 동작을 명확히 확인할 수 있고 직접 학습 과정을 조절할 수 있다는 장점이 있다.

학습한 모델은 나중에 다시 사용할 수 있도록 저장하거나 협업을 위해 공유하는 경우가 많은데, 
이를 위해 허깅페이스에서는 모델 허브에 모델을 업로드하고 불러올 수 있는 기 능을 지원한다. 
이번에는 학습한 모델을 허깅페이스 허브에 저장하는 방법을 알아보자.

### 학습한 모델 업로드하기
학습한 모델은 예제 3.29와 같은 코드로 허깅페이스 허브에 업로드할 수 있다.

`huggingface_hub` 라이브러리는 허깅페이스에 프로그래밍 방식으로 접근할 수 있는 기능을 지원하는데, 
허깅페이스의 계정 토큰을 통해 로그인할 수 있다. 

계정 토큰에 대한 정보는 [허깅페이스 링크](https://huggingface.co/docs/hub/security-tokens)에서 확인할 수 있다. 
업로드 방법은 크게 Trainer를 사용했을 때와 사용하지 않았을 때로 나뉘는데, Trainer를 사용한 경우 trainer 인스턴스에서 `push_to_hub()` 메서드를 사용하면 학습한 모델과 토크나이저를 함께 모델 허브에 업로드한다. 

직접 학습한 경우 모델과 토크나이 저를 각각 `push_to_hub()` 메서드로 업로드할 수 있다.

```python
from huggingface_hub import login

login( token="본인의 허깅페이스 토큰 입력" )
repo_id = f"본인의 아이디 입력/roberta-base-klue-ynat-classification" # Trainer를 사용한 경우
trainer.push_to_hub(repo_id)

# 직접 학습한 경우
model. push_to_hub repo_id)
tokenizer. push_to_hub(repo_id)
```

## 모델 추론하기
이번 절에서는 모델로 추론하는 방법을 알아본다. 
모델을 학습시킬 때 허깅페이스에서 제공하는 트레이너 API를 활용하는 방법과 직접 학습을 수행하는 방법이 있었다. 
추론 할 때도 마찬가지로 모델을 활용하기 쉽도록 추상화한 파이프라인 ipeline을 활용하는 방법이 있고 직접 모델과 토크나이저를 불러와 활용하는 방법이 있다. 
먼저 파이프라인을 사용해 간단히 모델을 활용하는 방법을 알아보자.

### 파이프라인을 활용한 추론
허깅페이스는 예제 3.30과 같이 토크나이저와 모델을 결합해 데이터의 전후처리와 모델 추론을 간단하게 수행하는 pipeline을 제공한다. 
파이프라인은 크게 작업 종류, 모델, 설정을 입력으로 받는다. 작업 종류는 텍스트 분류, 토큰 분류 등 작업에 맞춰 설정 하고 모델에 저장소 아이디를 설정하면 된다.

파이프라인 인자에 대한 더 자세한 설명 은 허깅페이스의 [파이프라인 공식 문서](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.config)에서 확인할 수 있다. 
예제 3.30에서는 텍 스트 분류 작업을 위한 모델을 불러오기 위해 pipeline에 인자로 `text-classification` 과 모델 아이디를 전달한다. 
예제 3.29를 실행해 모델을 허깅페이스 허브에 업로드했다 면 예제 3.30에서 모델 아이디의 계정 부분을 자신의 아이디로 입력하면 된다.

```python
from transformers import pipeline

modeL_id= 본인의 아이디 입력/roberta-base-klue-ynat-classification"

model_pipeline = pipeline "text-classification", model=model_id)

model_pipeline(dataset["title"][ :5])
# [{'Label': '경제', 'score': 0.9940265417098999}, 
# {'Label' : '사회','score': 0.9847791790962219},
# {'Label': 'IT과학', 'score': 0.9899107813835144}, 
# {'Label': '경제', 'score': 0.993854284286499}, 
# {'Label' : '사회', 'score': 0. 9936111569404602}]
```

파이프라인에 추론하고자 하는 텍스트를 입력하면 예제 3.30과 같이 예측 확률이 가장 높은 레이블과 그 확률을 반환한다. 
다음으로 직접 추론하는 방법을 알아보자.

### 직업 추론하기
직접 모델과 토크나이저를 불러와 pipel ine과 유사하게 추론을 구현한다면 예제 3.31 과 같이 구현할 수 있다.
`__init__` 메서드에서 입력받은 모델 아이디(modelL_id)에 맞는 모델과 토크나이저를 불러온다. 
`Custompipeline`의 인스턴스를 호출할 때 내부적으로 `__call__` 메서드를 사용하는데, tokenizer를 통해 토큰화를 수행하고 모델 추론을 수행하고 가장 큰 예측 확률을 갖는 클래스를 추출해 결과로 반환한다.
예제의 마지막에서 결과를 확인하면 앞서 허깅페이스 파이프라인의 결과와 동일하다.

```python
import torch

from torch.nn. functional import softmax 
from transformers import AutoModelForSequenceClassification, AutoTokenizer

class CustomPipeline:
	def __init__(self, model_id):
		self model = AutoModelForSequenceClassification.from_pretrained(model_id)
		self.tokenizer = AutoTokenizer. from_pretrained(model_id)
		self.model.eval()

	def __call__(self, texts) :
		tokenized = self. tokenizer(texts, return_tensors="pt", padding=True, truncation=True) # ---> 1
		
	with torch.no_grad(): # ---> 2
		outputs = self.model(**tokenized)
		logits = outputs.logits
		
	probabilities = softmax logits, dim=-1) # ---> 3
	scores, labels = torch.max probabilities, dim=-1)
	labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]
	
	return [{"label": label, "score": score. item )} for label, score in zip( labels_
str, scores)]

custom_pipeline = CustomPipeline(model_id)
custom_pipeline(dataset[ 'title'][ :5])

# [{'Label': '경제', 'score': 0.9940265417098999}, 
# {'Label' : '사회','score': 0.9847791790962219},
# {'Label': 'IT과학', 'score': 0.9899107813835144}, 
# {'Label' : '경제','score': 0.993854284286499},
# {' label': '사회', 'score': 0.9936111569404602}]
```

# 말 잘 듣는 모델 만들기

LLMLarge Language Model (대규모 언어 모델)은 다음 단어를 예측하는 방식으로 대량의 텍스 트를 학습해서 뛰어난 텍스트 생성 능력을 보여줬다. 
2020년 OpenA가 GPT-3를 발표 했을 때 수많은 사람들은 마치 사람이 작성한 것 같은 생성 결과의 퀄리티에 놀랐다. 
하지만 GPT-3는 단순히 다음 단어를 예측하는 방식으로 학습했기 때문에, 사용자의 요청 에 적절히 응답하기보다는 사용자의 말에 이어질 법한 텍스트를 생성한다는 한계를 지니고 있었다. 
오늘날 챗GPT를 사용할 때 내 말에 이어질 텍스트를 생성해 달라고 요청 하는 경우는 드물 것이다. 
그런 의미에서 보자면 챗GPT가 할 수 있는 일에 비해 당시 GPT-3가 할 수 있는 일은 훨씬 적었다.

그렇다면 과연 GPT-3는 어떻게 챗GPT가 될 수 있었을까? 
OpenAI는 두 단계를 거쳐 GPT-3를 챗GPT로 변화시켰다. 먼저, 네이버 지식인과 같이 요청(또는 질문)과 답변 형식으로 된 지시 데이터셋instruction dataset을 통해 GPT-3가 사용자의 요청에 응답할 수 있도록 학습시켰다.
다음으로 사용자가 더 좋아하고 사용자에게 더 도움이 되는 답변을 생성할 수 있도록 추가 학습을 시켰다. 
이를 사용자의 선호 `reference`를 학습한다고 말한다. 

선호를 학습한 LLM은 예를 들어 차별적인 표현을 사용하지 않고, 사용자가 위험해 질 수 있는 정보(예: 무기 또는 약물의 제조 방법)에 대한 답변을 피하는 등 더 정제된 답변을 생성하게 된다.

이번 4장에서는 가상의 코딩 테스트 서비스인 '엔지니어스'에서 코딩 공부를 하러 온 사용자들이 공부하는 과정을 살펴보며, GPT-3가 챗GPT로 발전할 수 있었던 방법을 설명한다. 
사람들이 더 선호하는 답변을 생성할 수 있도록 모델을 조정하는 방법은 크게 강화 학습(reinforcement learning)을 사용하는 방법과 사용하지 않는 방법으로 나눌 수 있다. 
OpenAI가 챗GPT를 개발할 때 강화 학습 방법 중 하나인 '근접 정책 최적화Proximal Policy Optimization (이하 PPO)를 사용해 선호 학습에 강화 학습이 필요하다고 알려졌다. 

하지만 PPO는 하이퍼파라미터에 민감하고 학습이 불안정해 많은 연구자와 개발자에게 좌절을 안겼다. 
이후 강화 학습을 사용하지 않고 선호를 학습시키는 다양한 기술이 개 발되고 활용되고 있다.

이 장에서는 먼저 LLM이 사용자의 요청에 맞춰 응답하도록 학습시키는 지도 미세 조정(supervised fine-tuning)에 대해 알아본다. 
그리고 강화 학습을 사용해 사람의 선호를 학습시키는 RLIFR(Reintorcement Learning from Human Feedback) 와 PPO에 대해 알아본다. 마지막으로 강화 학습을 사용하지 않고 선호를 학습시켜 최근 많은 관심을 받고 있는 기각 샘플링(rejective sampling)과 직접 선호 최적화 (Direct Preference Optimization, DPO) 방법을 살펴본다.

## 코딩 테스트 통과하기 : 사전 학습과  지도 미세 조정
가상의 코딩 테스트 서비스 '엔지니어스'는 사용자가 코딩 테스트를 통과해 원하는 직장에서 일할 수 있도록 학습 자료와 코딩 테스트 연습 기능을 제공한다. 
비전공자인 A는 엔지니어스를 통해 개발자로 직무를 변경하고자 한다. 
A는 어떻게 공부를 시작하고 코딩 테스트를 통과할 수 있을까? 

이제부터 A의 공부 과정을 살펴보며, LLM이 사용자의 요청에 응답할 수 있도록 학습하는 방법을 알아본다.

### 코딩  개념 익히기 : LLM의 사전 학습
비전공자로서 개발자를 지원하고 있는 A는 프로그래밍을 처음 공부하기 때문에 우선 파이썬(Python)이라는 프로그래밍 언어에 대해 공부하기로 한다. 

기본적인 파이썬 문법을 공부하면서 변수가 무엇인지, for 문을 활용해 반복적인 작업을 처리하는 방법, if 문을 사용해 경우에 따라 처리 방식을 달리하는 방법 등을 익힌다. 
클론 코딩(clone coding)이 프 로그래밍 공부에 많은 도움이 된다는 추천 글을 읽고 아직은 어렵지만 여러 프로젝트의 코드를 읽고 따라 작성해 보기도 한다. 
또 친구가 코딩 테스트를 통과하려면 반드시 알 아야 한다고 추천해 줘서 자료구조와 알고리즘 책도 찾아 읽는다.

이렇게 A가 다양한 자료를 보며 프로그래밍을 처음 공부하는 과정은 그림 4.1과 같이 LLM의 사전 학습과 유사하다. 

LLM은 보통 인터넷상에 있는 다양한 텍스트 데이터를 수 집한 대용량의 텍스트로 사전 학습한다. 
1.1절에서 살펴본 대로 LLM은 딥러닝 기반의 언어 모델이며, 다음 단어를 예측하는 언어 모델링을 통해 텍스트를 이해하는 방법을 학습한다.

2023년 메타(Meta)에서 공개한 라마-2(Llama-2) 모델은 약 10TB 분량의 텍스트를 사전 학습에 사용했는데 사전 학습 데이터의 경우 코드, 블로그, 기사, 광고 등 다양한 글이 섞여 있기 때문에 사전 학습 데이터에서 다음 단어를 예측하는 방법으로 학습하는 경우 LLM이 특정한 형태로 응답하거나 사용자의 요청에 따라 응답하길 기대하기는 어렵다.

사전 학습 동안은 LLM이 언어에 대한 전체적인 이해도가 높아지고 바로 다음에 올 단어 를 점점 더 잘 예측하게 된다.
![Screenshot-2025-06-08-at-6.40.27-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-6.40.27-PM.png)
언어 모델이란, 그림 4.2와 같이 다음에 올 단어의 확률을 예측하는 모델이다. 
그림에서"최고의 프로그래밍 언어는?"이라는 문장을 입력했을 때 사전에 있는 단어가 다음에 나 타날 확률을 각각 계산한다.
잘 학습된 언어 모델이라면, 프로그래밍 언어인 '파이썬'이 나 '자바'가 등장할 확률을 비교적 높게 예측하고(예: 30%, 15%), '산'이나 '바다'처럼 현 재 문맥과 관련 없는 단어가 등장할 확률은 낮게 예측한다(예: 0.7%, 0.5%). 
'영어'는 언어이긴 하지만 프로그래밍 언어는 아니기 때문에 중간 정도의 확률(예: 3%)로 예측한다.
![Screenshot-2025-06-08-at-6.41.33-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-6.41.33-PM.png)
다음 단어를 예측하는 언어 모델을 학습시킬 때는 그림 4.3과 같이 학습 데이터의 일부를 입력으로 넣고 바로 다음에 나오는 정답 토큰을 맞추도록 학습한다. 

여기서 맞추도 록 학습한다는 것은 예를 들어 "최고의 프로그래밍 언어는?"이라는 입력에 대해 다음으로 오는 정답 토큰이 '파이썬'이라면, 그림 오른쪽과 같이 언어 모델이 다음 단어로 '파 이썬'을 예측할 확률이 높아지도록 학습시키는 것을 말한다. 
이런 과정을 수많은 학습 데이터에 대해 수행하면서 어떤 단어가 다음에 올지 학습하게 된다.
![Screenshot-2025-06-08-at-6.42.20-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-6.42.20-PM.png)
### 연습문제 풀어보기 : 지도 미세 조정
프로그래밍 언어의 문법이나 자료구조와 알고리즘 같은 기본 개념을 익혔다면, 다음으 로 필요한 건 코딩 테스트 연습 문제를 풀어보는 것이다. 
먼저, 코딩 테스트 문제를 풀기 위해서는 간단한 예제 문제를 충분히 풀어보고, 자주 나오는 문제와 그 정답 코드를 보면서 문제 풀이 과정을 익혀야 한다. 
코딩 테스트는 일반적으로 문제의 상황과 필요한 코드를 설명하고 예시 케이스를 보여주는 정해진 형식이 있는데, 처음으로 문제를 접한 다면 그런 형식에 어색함을 느끼고 빠른 시간 안에 문제를 파악하지 못할 수도 있다. 

그림 4.4와 같이 코딩 테스트 서비스마다 정답을 작성하는 방식이 다른데, 서비스에 맞게 코드를 작성할 수 있는 연습도 필요하다.

![Screenshot-2025-06-08-at-6.42.50-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-6.42.50-PM.png)
LLM도 사용자의 요청에 적절히 응답하기 위해서는 사람이 코딩 테스트를 준비할 때와 비슷하게 요청의 형식을 적절히 해석하고, 응답의 형태를 적절히 작성하며, 요청과 응답 이 잘 연결되도록 추가로 학습한다. 이를 지도 미세 조정supervised fine-tuning이라고 한다. 지 도 미세 조정에서 '지도supervised'란 학습 데이터에 정답이 포함되어 있다는 의미다. 지 도 미세 조정을 통해 LLM은 사용자의 요청에 맞춰 응답하도록 학습하는데, 이를 정렬 alignment이라고 한다. 사람의 요청과 LLM의 응답이 정렬되도록 한다는 의미다.

지도 미세 조정에 사용하는 데이터셋을 지시 데이터셋™nstruction dataset 이라고 부른다. 사용 자의 지시에 맞춰 응답한 데이터셋이라는 의미다. 이와 같은 지시 데이터셋에 비해 사 전 학습 데이터셋은 형식이 너무나도 다양하고, 사용자의 요청에 응답하는 형식의 데이 터는 적다. 특히 양질의 데이터는 훨씬 더 적을 수밖에 없다. 딥러닝 모델은 기본적으로 학습 데이터에 있는 행동을 배우기 때문에 학습 데이터에 요청에 응답하는 데이터가 적 다면 그 행동은 잘 배우지 못한다.

이런 문제를 보완하기 위해 사용자의 요구사항과 그에 대한 응답(정답)을 구조화한 데 이터를 구축하고 언어 모델의 학습에 활용한다. 지시 데이터셋은 사용자의 요청을 형식 에 맞춰 작성하고, 그에 대해 적절한 형식의 응답을 하는 형태다. 

OpenAI는 2022년 공개한 챗GPT를 개발하면서, 그림 4.5 같이 데이터 레이블러(Labeler)를 고용해 13,000개 가 넘는 지시 데이터셋을 구축해 모델을 학습시켰다.
![Screenshot-2025-06-08-at-7.00.10-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.00.10-PM.png)
그렇다면 지시 데이터셋은 실제로는 어떤 형태일까? 

2023년 스탠퍼드대학교에서 오픈 소스 라마(Lama) 모델을 추가 학습한 알파카(Apaca)를 공개할 때 사용한 알파카 데이터셋을 살펴보자(예제 4.1). 

스탠퍼드대학교는 알파카 데이터셋을 구축하기 위해 OpenAI의 `text-davinci-003` 모델을 사용해 175개의 예시 태스크로부터 52,000개의 데이터셋을 구축했다.

지시사항(instruction)은 사용자의 요구사항을 표현한 문장이다. 

입력(input)에는 답변을 하는데 필요한 데이터가 들어간다. 
출력(output)은 지시사항과 입력을 바탕으로 한 정답 응답이다. 
마지막으로 텍스트(text)는 지시사항, 입력, 출력을 정해진 포맷으로 하나로 묶은 데이터다. 

예제 4.1에서 지시사항으로 "Create a classification task by clustering the given list of items."
(주어진 아이템 리스트에서 적절히 아이템을 묶어 분류를 만들어라)라는 요구가 있었고, 
입력에 아이템 리스트로 사과, 오렌지, 바나나, 딸기, 파인애플("Apples, oranges, bananas, strawberries, pineapples")을 줬다. 
이에 대한 출력은 사과와 오렌지를 클래스 1로, 바나나와 딸기를 클래스 2로, 파인애플을 클래스 3으로 묶었다.

```json
{
	"instruction": "Create a classification task by clustering the given list of items.",
	"input": "Apples, oranges, bananas, strawberries, pineapples",
	"output": "Class 1: Apples, Oranges\nClass 2: Bananas, Strawberries\nClass 3:Pineapples",
	"text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \n\n### Instruction: \nCreate a classification task by clustering the given list of items. \n\n### Input: \nApples, oranges, bananas, strawberries, pineapples) n\n### Response: \nClass 1: Apples, Oranges\nClass 2: Bananas, Strawberries\nClass 3:Pineapples",
}
```
예제 4.1의 데이터는 사용자의 요청과 응답 데이터가 포함돼 있지만 아직 특정한 형식 을 갖추지는 않았는데, 예제 4.2와 같은 템플릿에 예제 4.1의 데이터를 넣어 형식에 맞춘다. 
예제 4.2를 보면, 첫 두 줄에서 "작업을 설명한 지시사항과 맥락 정보인 입력을 바 탕으로 요청에 대한 적절한 응답을 작성하라"라는 안내를 하고 지시사항, 입력, 응답을 각각 넣어준다. 
이때 LLM이 데이터의 형식을 인식할 수 있도록 `###`으로 텍스트를 구분 하고 있다.

```python
f"""

Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request. \n\n 
### Instruction: \n{instruction}\n\n
### Input: \n{input}/\n\n 
### Response: \n{output}

"""
```

지금까지 어떤 데이터가 어떤 형식으로 만들어지는지를 알아봤다. 
그렇다면 이 데이터 셋을 LLM은 어떻게 학습할까? 

지도 미세 조정이라는 새로운 용어가 붙었지만, 사전 학 습 때와 동일하게 다음 단어를 예측하는 인과적 언어 모델링 (causal language modeling)을 사용해 학습한다. 
즉 지도 미세 조정이라는 이름은 LLM이 학습하는 방식이 달라서가 아니 라, 학습하는 데이터셋에 차이가 있기 때문에 지어진 것이다.

지시 데이터셋을 어떻게 구성하는 게 좋은지 아직 정답은 없다. 
메타, 마이크로소프트 등 여러 회사에서 어떻게 지시 데이터셋을 구성했을 때 LLM의 성능을 높일 수 있는지 연구를 진행했다. 
이제부터 사례를 바탕으로 좋은 지시 데이터셋이 갖춰야 할 조건을 살펴보자.

### 좋은 지시 데이터셋이 갖춰야 할 조건

지시 데이터셋을 준비한다고 했을 때 데이터의 양, 품질, 질문의 형식, 답변의 형식 등 다양한 측면에서 고민해 봐야한다. 
새로운 기술을 도입할 때 아무런 가이드가 없다면 모든 과정이 미궁처럼 느껴지고 막막할 수밖에 없다. 
이제부터 메타와 마이크로소프트 가 2023년 공개한 연구 결과를 바탕으로 좋은 지시 데이터셋이 갖춰야 할 몇 가지 조건 을 소개한다.

먼저, "얼마나 많은 지시 데이터셋이 필요한가?"라는 질문에 대해 메타는 2023년, [Less Is More for Alignment(정렬을 위해서는 적은 것이 더 낫다)](https://arxiv.org/abs/2305.11206)라는 논문을 발표하며 파라미터가 650억개인 라마LLaMa 모델을 정렬 하는 데 선별한 1,000개 정도의 지시 데이터셋으로도 가능했다고 밝혔다. 

이렇게 정렬 한 모델의 이름은 리마(LAMA)다. 
앞서 LLM의 사전 학습을 위해 대용량의 텍스트 데이터를 사용하고 라마-2 모델의 경우 약 10TB에 달하는 학습 데이터를 사용했다고 이야기했다.
일반적으로는 수백 GB에서 수 TB의 데이터로 사전 학습을 수행한다. 
그런데 지도 미세 조정에서는 불과 1,000개 정도의 데이터로도 모델이 사용자의 요청에 응답하도록 만들 수 있었다는 것이다. 
그뿐 아니라 약 300개의 요청에 대한 응답 결과를 사람이 직접 확인하면서 평가했을 때, 리마 모델은 앞서 52,000개의 지시 데이터셋으로 학습한 알파카 모델보다도 응답의 품질이 뛰어났고, 구글의 바드(Bard), (OpenAI)의 GPT-4와 비 교했을 때도 4~50%의 답변은 리마가 더 뛰어나거나 비슷한 수준이었다.

또한 "지시 데이터셋에서 지시사항이 다양한 형태로 되어 있고 응답 데이터의 품질이 높을수록 정렬한 모델의 답변 품질이 높아진다."고 밝혔다. 
메타가 지시 데이터셋을 구 성할 때 사용한 데이터셋은 크게 위키하우(wikiHow) 데이터와 스택익스체인지(Stack EXchange)데이터였다. 
두 데이터 모두 한 사용자가 질문을 하고 다른 사용자들이 그에 대해 답해 주는 마치 네이버 지식인과 같은 형태다. 

위키하우는 답변의 퀄리티가 높지만 요청 또 는 질문이 모두 '하는 방법How to' 형식이었고, 스택익스체인지는 질문의 형식은 다양하 지만 답변의 퀄리티가 낮았다. 
메타 연구진은 스택익스체인지 데이터 중 답변의 퀄리티 가 높은 데이터만 선별해 질문의 형식도 다양하면서 답변의 퀄리티가 높은 지시 데이터 셋을 구축해 높은 성능의 리마 모델을 만들 수 있었다.

메타에서는 이런 결과를 바탕으로 피상적인 정렬 가설(superficial alignment hypothesis')을 주장했다. 

피상적인 정렬 가설이란, 모델의 지식이나 능력은 사전 학습 단계에서 대부분 학습하고 정렬 데이터를 통해서는 답변의 형식이나 모델이 능력과 지식을 어떻게 나열 할지 정도만 추가로 배우기 때문에 적은 정렬 데이터로도 사용자가 원하는 형태의 답변 을 생성할 수 있다는 가설이다. 

리마 모델의 결과만으로 피상적인 정렬 가설이 맞다고 판단하긴 어렵지만, 지도 미세 조정을 통해 LLM을 정렬하고자 할 때 기초 모델(foundation model)을 잘 선택한다면 작은 지시 데이터셋으로도 정렬이 가능하다는 힌트는 얻을 수 있다.

다음으로 지시 데이터셋의 품질이 LLM의 성능에 얼마나 큰 영향을 미치는지 살펴보자. 

2023년 마이크로소프트는 [『Textbooks Are All You Need(텍스트북이면 충분하다)」](https://arxiv.org/abs/2306.11644)라는 연구 결과를 발표하며, 지시 데이터셋의 품질을 높이면 더 작은 데이터셋과 더 작은 모델로도 높은 성능을 달성할 수 있다고 주장했다.

마이크로소프트가 2023년 6월 공개한 파이썬 코드 생성 모델인 파이Pi는 파라미터가 13억 개인 작은 모델임에도 GPT-3.5나 2023년 마이크로소프트와 베이징대학교가 공개한 코드 생성 모델인 위자드코더WizardCoder와 견줄 수 있는 성능을 보였다. 

위자드코더가 160억 개의 파라미터를 갖고 GPT-3.5가 1,750억 개의 파라미터를 갖는 걸 감안 할 때 파이 모델의 성능은 꽤 놀랄 만했다.

마이크로소프트에서 파이 모델을 훈련하기 위한 학습 데이터를 구축할 때 공개된 코드 데이터셋을 그대로 사용하지 않고 선별해서 사용했는데, 그 이유는 다음과 같았다.

- ﻿﻿외부 모듈이나 파일을 사용하기 때문에 하나의 코드 파일 자체에서 의미를 이해하 기 어려운 경우가 많았다.
- ﻿﻿대부분의 파일은 의미 있는 연산보다는 보일러플레이트bollerplate 코드나 설정 파일 이었다.
- ﻿﻿알고리즘 로직을 담고 있는 코드도 복잡하거나 제대로 문서화되지 않은 함수들 사 이에 있어 의미를 파악하기 어려웠다.
- ﻿﻿특정 주제나 사례에 관련된 코드가 많아 특정 개념이나 스킬에 불균형한 데이터셋 분포를 보였다.
이런 이유로 코드 데이터셋에서 모델의 학습에 도움이 되는 교육적 가치가 높은 데이터 를 선별했다. 
교육적 가치가 낮은 코드와 높은 코드를 비교한 그림 4.6을 한번 살펴보자.

그림 4.6(a)의 코드부는 교육적 가치가 높은 코드로, 코드의 이름에 기능에 대한 의미가 충분히 담겨 있고 어떤 기능을 수행하는지 독스트링(Docstring)으로 잘 정리돼 있다. 
반면 그림 4.6(b)의 코드부는 코드의 대부분이 Default라는 클래스의 속성을 설정하는 내용으로 구성돼 있다.
![Screenshot-2025-06-08-at-7.27.11-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.27.11-PM.png)
이 외에도 사람이 새로운 개념을 배울 때 예제 문제를 공부하는 것처럼 GPT-3.5를 활 용해 코드 예제 데이터셋(CodeExercises)을 생성해 학습 데이터에 추가했다. 

예제 데이 터셋은 예제 4.3과 같이 함수의 독스트링 내용을 바탕으로 코드를 구현하는 형식이다.

1. 이 코드부의 독스트링에는 단어(word) 안에 있는 문자(Letter)를 맞추는 게임을 할 때 아직 정답으로 예측한 적 없는 문자 리스트를 반환한다는 함수의 목적과 입력 인자의 형식과 의미, 반환하는 데이터의 타입과 정보가 담겨 있다. 
2. 2에는 독스트링에서 설명한 함수의 동작을 수행하는 코드가 작성돼 있다. 

이런 형태의 데이터를 통해 파이 모델은 함수의 이름과 설명을 바탕으로 함수를 구현하는 방법을 학습하게 된다.

```python
def valid_guessing_letters(word: str, guesses: List[str]) → List[str]:
# ----- 1 -----
""" 
	Returns a list of valid guessing letters, which are letters that have not been guessed yet and are present in the word.
	Parameters:
	word (str): The word to guess.
	guesses (List[str]): A list of letters that have already been guessed.
	Returns:
	List[str]: A list of valid guessing letters.
"""
valid_letters = []
for letter in word:
	if letter not in guesses and letter not in valid_letters: 
		valid_letters.append (letter)
	return valid letters
```

교육적 가치가 높은 데이터를 필터링하고, 코드 예제 데이터셋을 추가했을 때 모델 성 능의 변화는 그림 4.7과 같다. 
모델의 평가에는 LLM의 코드 생성 정확도를 평가하는 벤치마크인 HumanEval 데이터셋을 사용했다. 

공개된 코드 데이터셋인 스택+(The Stack) 를 그대로 사용했을 때에 비해 교육적 가치가 높은 데이터를 선별한 코드 텍스트북(code textbook)을 학습 데이터로 사용했을 때 정확도가 17에서 29로 크게 상승했다. 

또한 GPT-3.5로 생성한 고품질의 예제 데이터셋으로 추가 학습시키자 정확도는 29에서 51로 또 한 번 크게 상승했다. 
학습 데이터의 품질이 높다면 더 많은 양의 데이터로 학습할 때보다도 더 높은 성능을 달성할 수 있다는 사실을 확인할 수 있다.

![Screenshot-2025-06-08-at-7.29.57-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.29.57-PM.png)
메타와 마이크로소프트의 연구를 통해 좋은 지시 데이터셋이 갖춰야 하는 조건을 정리 하면 다음과 같다.

- ﻿﻿지시 데이터셋을 작은 규모로 구축하더라도 모델이 지시사항의 형식을 인식하고 답변하도록 만들 수 있다.
- ﻿﻿지시사항이 다양한 형태이고 답변의 품질이 높을수록 모델의 답변 품질도 높아진다.
- ﻿﻿학습 데이터의 품질을 높이기 위해 모델의 목적에 맞춰 학습 데이터의 교육적 가 치를 판단하고 교육적 가치가 낮은 데이터를 필터링하는 방법을 사용할 수 있다.
- ﻿﻿교재의 예제 데이터와 같은 고품질의 데이터를 학습 데이터에 추가하면 성능을 크 게 높일 수 있다.

이와 같은 방식을 지시 데이터셋 구축에 활용한다면, 더 효율적인 데이터셋 구축이 가 능할 것이다. 
지금까지 LIM을 사전 학습하고 지시 미세 조정을 통해 지시사항에 맞춰 응답하도록 학습하는 과정을 살펴봤다. 
이제부터는 LIM이 사람이 더 선호하는 방식으로 답변할 수 있도록 조정하는 방식을 알아보자.

## 채점 모델로 코드 가독성 높이기
엔지니어스의 학습 자료와 예제 문제, 코딩 테스트 서비스를 통해 A는 원하는 회사에 취 업할 수 있었다. 
하지만 취업하고 보니 자신이 작성한 코드를 동료들과 공유하거나 코드 리뷰를 받는 일이 많았다. 

코드를 구현하는 것뿐만 아니라 코드를 남들이 읽기 쉽도 록 가독성 높게 작성하는 게 중요하다는 사실을 알게 되고 A는 엔지니어스에 관련 서비스를 만들 계획이 있는지 문의했다. 
엔지니어스는 A의 요구에 맞춰 사용자가 코드 가독 성을 높일 수 있도록 교육하는 방법을 고민하기 시작했다.

엔지니어스가 내놓은 해답은 '코드 가독성을 평가하는 딥러닝 모델을 코딩 테스트에 포함'하는 것이었다. 
기존의 코딩 테스트는 목적에 맞게 작동하는지, 효율적으로 작동하는 지를 중점으로 점수를 평가한다. 

여기에 코드 가독성을 평가 항목으로 추가할 수 있다면, 엔지니어스의 사용자들은 코딩 테스트에서 코드를 제출하고 받는 가독성 점수를 통해 어떤 코드가 가독성이 좋거나 나쁜 코드인지 함께 배울 수 있을 것이다.

### 선호 데이터셋을 사용한 채점 모델 만들기
코드 가독성을 평가하는 모델을 만들기 위해서는 모델을 학습시킬 학습 데이터를 구축 해야 한다. 
엔지니어스는 가독성 채점 모델을 학습시키기 위해 그림 4.8과 같은 가독성 비교 데이터셋을 구축했다. 

그림에서 코드 A와 코드 B를 비교해 더 가독성이 높은 코드 를 선택하고, 선택한 코드를 선호 데이터Chosen data, 선택하지 않은 코드를 비선호 데이터

rejected data라고 한다. 이런 형태로 두 데이터 중 사람이 더 선호하는 데이터를 선택한 데이터셋을 선호 데이터셋preference dataset 이라고 한다. 선호 데이터와 비선호 데이터는 정해진 것은 아니고 비 교하는 대상에 따라 달라질 수 있는데, 만약 코드 B가 새로운 코드 C 보다 코드 가독성이 높다면 그 관계에서는 코드 B가 선호 데이터가 된다.

그림 4.8을 보고 의문이 생긴 독자도 있을 것이다. 
왜 코드 가독성 채점 모델을 만들 때 그림 4.9 와 같이 코드와 그 코드에 대한 가독성 점수를 직접 수집해 사용하지 않고 두 코드를 비교한 데이 터셋을 사용할까? 

코드에서 직접 점수를 매긴 데이터셋은 구축하기가 어렵기 때문이다. 코드를 보고 그 가독성을 바로 평가하는 작업은 좀처럼 쉽지 않다.

예를 들어, 입력한 리스트에 있는 짝수를 모두 더하는 함수를 작성한 코드를 평가한다 고 하자. 
예제 4.4의 (1)만 주어지고 코드의 가독성을 점수로 평가하라고 하면, 쉽게 몇 점이라고 평가하기 어렵다. 

하지만 예제 4.4의 (2)를 함께 제공하면서 둘 중에 코드 가 독성이 더 높은 코드를 선택하라고 하면, 쉽게 (2)가 더 가독성이 높다고 선택할 수 있다. 
(2)의 코드는 독스트링을 통해 함수가 하려는 일을 더 잘 설명하고 있고, 코드도 먼 저 짝수를 찾아 `even_numbers` 변수에 저장하고 `even_numbers`를 더한 `total_sum`을 반환 하도록 단계별로 작성돼 있으며, 변수명도 데이터의 의미를 잘 반영하고 있다. 

하지만(1)은 어떤 함수인지 코드를 직접 살펴봐야만 확인할 수 있고, 입력 리스트에서 짝수 를 찾아 더하는 전체 과정을 한 줄의 코드로 수행하기 때문에 천천히 들여다봐야 이해 할 수 있다. 

이처럼 코드 자체에 대한 점수를 부여하는 것은 어렵지만, 두 코드를 비교해 가독성이 높은 코드를 선택하는 것은 비교적 쉽기 때문에 선호 데이터셋을 구축해 채점 모델을 만든다.

```python
# (1) 코드 가독성이 낮은 예시
def sum_even (nums):
	even_sum = sum(n for n in nums if n % 2 == 0)
	return even_sum
	
# (2) 코드 가독성이 높은 예시
def sum_of_even_numbers (numbers_list):
	"""
	Calculate the sum of all even numbers in a given list.
	
	Parameters:
	numbers_list (list): A list of integers.
	Returns:
	int: The sum of all even numbers in the list.
	"""
	
	even_numbers = [number for number in numbers_list if number & 2 == 0]
	total_sum = sum (even_numbers )
	return total_sum
```

여러 코드를 바탕으로 어떤 코드가 더 가독성이 높은지를 선택해 선호 데이터셋을 구축 하고 나면, 채점 모델이 선호 데이터에 비선호 데이터보다 높은 점수를 주도록 채점 모 델을 학습시킨다. 

이런 방식으로 코드 가독성을 평가하는 채점 모델을 만들 수 있다.

OpenAl도 챗GPT를 개발하는 과정에서 같은 학습 방식을 사용했다. 
지도 미세 조정을 마친 LLM은 사용자의 요청에 맞춰 응답하기 때문에 사용자에게 결과적으로 해가 될 수 있는 정보(예: 약물 제조 방법, 폭탄 제조 방법)도 제공하고 차별적인 답변도 생성하는 등 문제가 있었는데, 이런 부분을 줄이기 위해 그림 4.10과 같이 생성된 답변의 점수를 평가하는 리워드 모델「eward model을 만들었다. 

순서대로 살펴보면, 먼저 지도 미세 조정을 마친 LLM에 지시사항을 입력해 여러 응답(A, B, C)을 생성한다. 그리고 레이블러가 응답을 비교해 더 좋다고 판단하는 순서를 정해 선호 데이터셋을 구축한다. 

그림 4.10에 서는 레이블러가 A> C> B 순서로 좋다고 판단했는데, 이렇게 구축한 선호 데이터셋을 사용해 리워드 모델이 응답 A의 점수가 응답 C의 점수보다 높도록, 응답 A의 점수가 응 답 B의 점수보다 높도록 학습한다.

![Screenshot-2025-06-08-at-7.36.38-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.36.38-PM.png)
이와 같은 방식을 거쳐 엔지니어스는 코드 가독성 채점 모델을 만들었고 이를 코딩 테 스트 서비스에 반영해 사용자들이 가독성 높은 코드를 작성하는 방법을 배울 수 있도록 제공했다. 
이제부터는 사용자들이 엔지니어스의 서비스를 활용해 코드 가독성 훈련을 하는 과정을 살펴보자.

### 강화 학습 : 높은 코드 가독성 점수를 향해
코딩 테스트 지원자들인 엔지니어스의 사용자들은 코드 가독성 채점 모델의 점수를 받 으며 더 나은 코드를 작성하기 위해 노력하게 된다. 
만약 내가 작성한 코드가 코드 가독 성 평가에서 60점을 받았다면, 반복된 부분을 함수를 묶거나 변수의 이름이 데이터의 의미를 더 잘 담도록 변경하는 등 다양한 시도를 하면서 가독성이 높은 코드를 작성하는 방법을 익혀 나간다.

OpenAl는 2022년 [Training language models to follow instructions with human feedback(사람의 피드백을 사용해 언어 모델이 지시를 따르도록 학습하기)](https://arxiv.org/abs/2203.02155)이라는 논문에서 강화 학습을 사용해 LLM이 리워드 모델로부터 더 높은 점수를 받도록 학습시킨 과정을 공개했다. 

강화 학습을 사용했기 때문에 이 학습 방법을 '사람의 피드백을 활용한 강화 학습(Reinforcement Learning from Human Feedback' 이하 RLHF)이라고 부른다. 
지금부터 강화 학습의 개념을 살펴보면서 LLM에 강화 학습이 어떻게 접목되는지 확인해 보자.

강화 학습에서는 그림 4.11과 같이 에이전트(agent)가 환경(environment)에서 행동(action)을 한다.

행동에 따라 환경의 상태(state)가 바뀌고 행동에 대한 보상(reward)이 생기는데, 에이전트는 이 변화된 상태를 인식하고 보상을 받는다. 
에이전트는 가능하면 더 많은 보상을 받을 수 있도록 행동을 수정하면서 학습한다. 
이때 에이전트가 연속적으로 수행하는 행동의 모음을 에피소드(episode)라고 한다.

![Screenshot-2025-06-08-at-7.41.24-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.41.24-PM.png)
코딩 테스트 문제를 푸는 예시를 강화 학습의 관점에서 나타낸 그림 4.12를 한번 살펴 보자. 
엔지니어스의 사용자는 채점 시스템의 점수를 받으며 코드의 변수명을 바꾸거나 공통 로직을 함수로 만들거나 타입 힌트를 추가하는 등 여러 행동을 한다. 

이때 그림4.11과 연결하면 사용자는 에이전트이고, 채점 시스템은 환경, 채점 시스템이 매긴 점수 는 보상, 행동을 통해 변화된 코드가 변화된 상태라고 할 수 있다. 
사용자가 높은 점수를 받기 위해 한 여러 행동을 하나로 묶으면 에피소드가 된다.
![Screenshot-2025-06-08-at-7.41.54-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.41.54-PM.png)
언어 모델이 RLIF를 통해 학습하는 과정을 나타낸 그림 4.13도 살펴보자. 
언어 모델은 다음 단어를 예측하는 방식으로 토큰을 하나씩 생성하는데, 강화 학습 관점에서는 토큰 생성을 하나의 행동으로 볼 수 있다. 

언어 모델이 텍스트를 모두 생성하면, 리워드 모델 이 생성한 텍스트를 평가하고 점수를 매긴다. 
그림 4.12와 비교하면 언어 모델은 행동을 취할 때마다 보상을 받지 않고 전체 생성 결과에 대해 리워드 모델의 점수를 받는다는 차이가 있고 나머지는 동일하다.
![Screenshot-2025-06-08-at-7.42.23-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.42.23-PM.png)
이와 같은 방식으로 언어 모델은 생성한 문장의 점수가 높아지는 방향으로 학습한다.

그런데 이때 보상을 높게 받는 데에만 집중하는 보상 해킹(reward hacking) 이 발생할 수 있다.

예를 들어, 코드 가독성 점수를 높게 받는 방법으로 깔끔한 코드를 작성하는게 아니라 아예 코드를 작성하지 않거나 `print("hello world")` 같은 간단한 코드만 작성해서 코드 가독성 점수를 높게 받으려고 할 수 있다. 
OpenAI는 보상 해킹을 피하기 위해 PPO라 는 강화 학습 방법을 사용했다. 이제부터 PPO가 무엇인지 살펴보자.

### PPO: 보상 해킹 피하기
엔지니어스 팀은 코드 가독성 평가를 도입한 이후 한동안 사용자들의 가독성 높은 코드 작성 능력이 향상되는 것을 확인했다. 
하지만 얼마 후 사용자 A가 그림 4.14와 같이 학습을 통해 코드 가독성 점수는 60점에서 90점으로 높아졌지만, 기능 구현 점수와 코드 효율성 점수가 각각 80점에서 30점, 70점에서 40점으로 떨어진 것을 관찰했다. 

이처럼 평가 모델의 높은 점수를 받는 과정에서 다른 능력이 감소하거나 평가 점수만 높게 받을 수 있는 우회로를 찾는 현상을 보상 해킹이라고 한다. 
그렇다면 어떻게 보상 해킹을 피하면서 사용자 A가 코드 가독성 점수를 높이도록 할 수 있을까?
![Screenshot-2025-06-08-at-7.43.44-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.43.44-PM.png)
엔지니어스 팀은 사용자 A가 코드 가독성 점수를 낮게 받았을 때 코드를 수정하는 과정 을 살펴봤다. 
사용자 A는 코드 가독성 점수가 낮게 나오자, 지금까지 작성했던 코드를 완전히 지워 다시 작성하거나 한 번에 변수명 변경, 함수로 묶기, 타입 힌트 추가 등 여 러 방법을 적용해 코드 가독성 점수를 높이려고 시도했다. 

코드를 수정해 본 경험이 있 는 독자라면, 복잡한 코드를 완전히 지우고 새로 작성하거나 한 번에 여러 부분을 수정 했을 때 에러 없이 수정 하기가 대단히 어렵다는 사실을 알고 있을 것이다. 

엔지니어스 팀은 사용자 A에게 코드를 한 번에 많이 수정하기보다는 점진적으로 한 가지씩 수정하 면서 코드 가독성 점수를 높여볼 것을 조언했다. 
사용자 A는 이 조언을 받아들여 한 번 에 한 가지씩만 변경하고 코드 가독성 점수를 확인했고 코드 가독성 점수를 높이면서도 다른 점수가 떨어지지 않을 수 있었다.

리워드 모델을 통해 LIM을 학습할 때도 비슷한 문제가 발생한다. 
OpenAI는 이런 문제 를 피하기 위해 강화 학습 방법 중 '근접 정책 최적화Proximal Preference Optimization'(이하 PPO)라는 학습 방법을 사용했다. 

PPO의 Proximal은 '몸 쪽의, 가까운'이라는 뜻이 있다. 

그림 4.15에서 가운데에 있는 지도 미세 조정 모델을 기준으로 학습하는 모델이 너 무 멀지 않게 가까운 범위에서 리워드 모델의 높은 점수를 찾도록 한다는 의미다. 
이때 지도 미세 조정 모델을 기준으로 거리를 측정하기 때문에 참고 모델reterence model이라 고 한다. 

그림에서 학습 모델 A는 코드 가독성 점수가 90점이지만 참고 모델에서 멀어다른 능력이 떨어지는 보상 해킹이 발생했다.

B는 참고 모델에서 가깝지만 코드 가독성 점수가 30점으로 낮다. C는 참고 모델에서 가까우면서도 코드 가독성 점수가 80점으로 높다. 
PPO는 보상 해킹에 빠진 모델 A나 리워드 모델의 점수가 낮은 모델 B가 아닌 모델 C를 찾을 수 있는 학습 방법이다.
![Screenshot-2025-06-08-at-7.44.05-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.44.05-PM.png)

### RLHF: 멋지지만 피할 수 있다면
OpenAI는 RLF를 챗GPT 개발에 도입하면서 AI 서비스에서 자주 문제가 되는 편향성, 공격성 등 여러 문제를 효과적으로 제어했다. 
챗GPT를 사용하다 보면 자신은 AI 모델 이기 때문에 질문에 답변하기 어렵다는 응답을 종종 받을 수 있는데, 완벽하지는 않지만 충분히 조심스럽게 대화한다는 인상을 받을 수 있다. 
챗GPT가 처음 공개되던 당시 이런 능력은 많은 사용자에게 충격을 줬고 RLHP는 대화형 AI 모델을 개발하기 위한 필 수 기술로 여겨졌다.

하지만 RLHF는 멋진 결과물만큼이나 사용하기 어렵기로 악명이 높다. 
앞서 살펴본 대 로 RLHP를 사용하기 위해서는 리워드 모델을 학습시켜야 하는데, 리워드 모델의 성능 이 좋지 않으면 LLM이 일관성 없는 점수를 통해 학습하게 되면서 제대로 학습되지 않는다.

따라서 성능이 높고 일관성 있는(robust) 리워드 모델을 만들어야 한다. 
또한 모델을 학 습시킬 때 참고 모델, 학습 모델, 리워드 모델 총 3개의 모델이 필요하기 때문에 GPU와 같은 리소스가 더 많이 필요하다. 
마지막으로, 강화 학습 자체가 하이퍼파라미터에 민감 하고 학습이 불안정하기 때문에 많은 연구자와 개발자들이 RLIF를 활용해 LLM을 학습 하는 데 어려움을 겪었다.

이후로 강화 학습이나 리워드 모델을 사용하지 않고도 사람의 선호를 학습할 수 있는 기술이 개발됐고, 비교적 작은 LLM을 학습하는 연구자나 개발자의 경우 강화 학습을 사용하지 않는 선호 학습 방법을 많이 사용하고 있다. 
지금부터는 강화 학습을 사용하지 않고 사람의 선호를 학습할 수 있는 기술에 대해 살펴보자.

## 강화 학습이 꼭 필요할까?
엔지니어스가 코딩 테스트에 코드 가독성 채점 모델을 추가함으로써 엔지니어스 사용자들은 가독성 높은 코드를 작성하는 방법을 배워나갈 수 있다. 

하지만 이 과정에서 엔 지니어스는 코드 가독성 채점 모델을 만들기 위해 많은 노력을 들여야 하고, 사용자들은 직접 여러 번의 시행 착오를 겪으며 더 높은 가독성 점수를 받을 수 있는 방법을 찾아야 한다. 
만약 한 번에 두 가지 행동(예: 변수명 변경과 타입 힌트 추가)을 했을 때 가독성 점수가 높아졌다면, 정확히 어떤 행동으로 인해 점수가 높아졌는지 파악이 어렵고 여러 행동이 복합적으로 점수에 영향을 미친다면 사용자는 혼란에 빠질 수 있다.

그렇다면 다른 방법은 없을까? 
지금부터 강화 학습 없이 LLM이 사람들이 더 선호하는 답변을 생성할 수 있도록 학습시키는 여러 방법을 살펴본다. 
먼저, 여러 생성 결과 중 리 워드 모델이 가장 높은 점수를 준 결과를 LLM의 지도 미세 조정에 사용하는 기각 샘플링 (rejection sampling)방법을 알아본다. 
다음으로 선호 데이터셋을 직접 LLM이 학습하는 방식 으로 변경해 열풍을 일으킨 직접 선호 최적화 Direct Preference Optimization(이하 DPO)에 대해 살펴본다. 

마지막으로, DPO를 활용해 개발된 모델을 소개한다.

### 기각 샘플링: 단순히 가장 점수가 높은 데이터를 사용한다면?
코드 가독성 채점 기능을 사용해 사용자들이 더 쉽게 가독성 높은 코드를 작성하는 능 력을 키울 수 있는 방법은 없을까? 
앞서 언급한 대로 코드를 제출하고 받는 점수만으로 코드 가독성을 높이기 위해서는 사용자가 많은 시행착오를 겪어야 한다. 
엔지니어스 팀 은 사용자들이 참고할 수 있도록 제출된 코드 중 가장 가독성 점수가 높은 코드를 공개 하기로 했다. 
사용자들은 공개된 코드를 통해 가장 높은 점수를 받은 코드를 보고 참고 해 학습 방향성을 잡을 수 있다.

기각 샘플링은 이런 아이디어를 LLM의 선호 학습에 적용한 방법이다. 
지도 미세 조정을 마친 LLM을 통해 여러 응답을 생성하고 그중에서 리워드 모델이 가장 높은 점수를 준 응답을 모아 다시 지도 미세 조정을 수행한다. 
강화 학습을 사용하지 않기 때문에 학습 이 비교적 안정적이고 간단하고 직관적인 방법임에도 효과가 좋아 많이 활용된다.

2023년 공개된 가장 대표적인 오픈소스 LLM인 메타의 라마-2LlaMa-2도 학습 과정에서 기각 샘플링을 사용했다. 

그림 4.16은 라마-2의 학습 과정을 나타낸 그림인데, 먼저 왼 쪽 아래에서 대용량의 사전 학습 데이터를 사용해 자기 지도 학습(Self-supervised learning)을 수행한다. 

여기서 자기 지도 학습이란 데이터의 특성을 활용한 비지도 학습Unsupervised learing 방식을 말하는데, 다음 단어를 예측하는 언어 모델링을 사용했다고 이해하면 된다. 
사전 학습을 통해 라마-2 기본 모델을 만들었다면 다음으로 지도 미세 조정을 통해 라마-2-챗 모델을 만든다. 

그림 왼쪽 위에서 라마-2-챗 모델이 더 안전하고 사람들에 게 도움이 될 수 있도록 사람의 피드백을 반영한 선호 데이터셋을 구축하고 마지막으로 RLHF를 통해 사람들이 선호하는 답변을 하도록 학습한다.
![Screenshot-2025-06-08-at-7.48.21-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.48.21-PM.png)
그림 4.16에서 RLHF 과정에 기각 샘플링이 추가된 것을 확인할 수 있다. 
메타는 라마-2 를 개발하면서 바로 강화 학습을 사용하는 PPO를 통해 모델이 사람의 선호를 학습하도록 하지 않고, 먼저 기각 샘플링을 통해 언어 모델이 더 빠르고 안정적으로 사람의 선호 를 학습한 후 PPO를 사용했다. 

기각 샘플링은 그 자체로도 사람의 선호를 학습하는 데 효과적이고, 라마-2의 학습에서와 같이 강화 학습 전에 활용함으로써 학습을 더 안정적 으로 만드는 방식으로 활용할 수도 있다. 

코드 레벨에서 기각 샘플링에 대해 더 알고 싶 다면, 허깅페이스의 [TRL-Transformer Reinforcement Learning 라이브러리의 기각 샘플링 공식 문서](https://huggingface.co/docs/trl/main/en/best_of_n)를 확인해 보자.

### DPO: 선호 데이터 셋을 직접 학습하기
엔지니어스 팀은 코드 가독성 채점 기능과 코딩 테스트 문제별로 가장 높은 점수를 받은 코드를 공개하는 방식으로 엔지니어스 사용자들이 가독성 높은 코드를 작성하는 능력을 키울 수 있도록 도왔다. 
하지만 채점 기능은 개발하는 데 오랜 기간이 걸렸고 새로운 프 로그래밍 언어가 추가되거나 새로운 유형의 코딩 테스트 문제를 추가하면 잘 작동하는지 매번 확인하는 데 많은 노력이 들었다. 
이런 이유로 엔지니어스 팀은 채점 기능을 없애면 서도 사용자들에게 코드 가독성 훈련 기능을 제공할 방법을 고민하기 시작했다.

채점 모델을 개발할 때 사용하던 선호 데이터셋을 살펴보던 한 팀원이 아이디어를 냈다. 
"엔지니어스 회원들은 코드를 읽을 줄 알고 코딩 테스트 문제를 푸는 훈련도 되어 있으니, 가독성이 좋은 코드와 좋지 않은 코드를 비교해서 보여주면 충분히 배울 수 있지 않을까요?" 엔지니어스 팀은 이 아이디어를 받아들여 선호 데이터셋을 사용해 문제 와 함께 가독성이 좋은 코드와 좋지 않은 코드를 대비시켜 사용자들이 확인할 수 있는 기능을 출시한다. 
사용자들은 채점 기능을 통해 많은 시행착오를 겪지 않아도 됐고 엔 지니어스 팀은 채점 기능을 관리하기 위해 들이던 리소스를 줄일 수 있게 됐다.

위의 엔지니어스 팀의 사례는 LLM의 선호 학습에 RLHF를 사용하던 많은 연구자와 개 발자가 겪던 상황을 보여준다. 
좋은 리워드 모델을 만들고 관리하는 것도 쉽지 않은 일이고 리워드 모델을 만든다고 하더라도 리워드 모델에 강화 학습으로 사람의 선호를 반영한다는 것도 쉽지 않았다. 
그러던 중 2023년 5월 스탠퍼드대학교의 라파엘 라파 일로바Raael Ralailova, 아키트 샤르마Archit Sharma, 에릭 미첼Eric Mitchell이 리워드 모델과 강화 학습을 사용하지 않고 선호 데이터셋을 직접 학습하는 DPO를 발표했다[「Direct Preference Optimization: Your Language Model is Secretly a Reward Model(DPO: 당신의 언어 모델은 리워드 모델이기도 하다)』,](https://arxiv.org/pdf/2305.18290.pdf). DPO 방 법은 RLHF에 비해 훨씬 단순하면서 효과적이어서 많은 연구자와 개발자들이 환호했다.

RLHF와 DPO를 비교하면 그림 4.17과 같다. 
RLHF는 선호 데이터셋으로 리워드 모델을 만들고 언어 모델의 출력을 평가하면서 강화 학습을 진행한다. 
하지만 DPO에서는 선호 데이터셋을 직접 언어 모델에 학습시킨다. 

이제부터 DPO 학습 방법을 알아보자.
![Screenshot-2025-06-08-at-7.51.20-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.51.20-PM.png)
그림 4.2에서 언어 모델은 다음에 올 토큰의 확률을 예측한다고 설명했다. 
그리고 우리 에게는 특정 입력(프롬프트)에 대해 어떤 응답이 선호되고 선호되지 않는지 데이터를 수집한 선호 데이터셋이 있다. 
DPO에서는 이 두 가지를 결합해 그림 4.18과 같이 선호 데 이터셋을 직접 학습한다. 
입력 프롬프트가 "최고의 프로그래밍 언어는?"이고, 선호 데이터가 '파이썬', 비선호 데이터가 '자바'라면, '파이썬'을 예측할 확률이 높아지도록, '자바를 예측할 확률은 낮아지도록 학습한다.

DPO에서는 학습이 잘되고 있는지 확인하기 위해 학습하지 않는 학습 전 모델인 참고 모델과 비교한다. 
선호 학습을 할 때는 일반적으로 참고 모델로 지도 미세 조정을 마친 모델을 사용한다. 
DPO를 통해 선호 데이터셋을 학습하면, 그림 4.19와 같이 참고 모델은 "최고의 프로그래밍 언어는?"이라는 입력에 다음에 올 토큰으로 '파이썬'을 30%, '자 바를 15% 확률로 예측했지만, 학습 모델은 '파이썬'을 33%, 자바를 13%로 예측한다.

선호 데이터셋에 있는 여러 데이터에 대해 이런 과정을 반복하면, 언어 모델은 점차 선호 데이터를 자주 생성하고 비선호 데이터를 드물게 생성해, 선호 데이터에 가까운 결 과를 생성하는 언어 모델이 된다.
![Screenshot-2025-06-08-at-7.52.28-PM.png](/img/이미지 창고/Screenshot-2025-06-08-at-7.52.28-PM.png)
그림 4.19에서도 알 수 있듯이 DPO 학습을 위해서는 별도로 리워드 모델이 필요하지 않고 강화 학습을 사용하지도 않는다. 따라서 훨씬 쉽고 빠르게 모델에 사람의 선호를 반영할 수 있다. 

2024년을 기준으로 RLHF보다 더 많이 사용되는 선호 학습 방법이다.

DPO를 직관적으로 잘 설명한 문구는 앞서 언급한 논문의 부제인 '당신의 언어 모델은 리워드 모델이기도 하다'라고 할 수 있다.

우리는 엔지니어스의 비유를 통해 엔지니어스의 사용자들이 프로그래밍의 개념을 익 히고 연습문제를 풀면서 코딩에 대한 기초 체력을 갖췄다는 사실을 알고 있다. 그런 사 용자는 선호 데이터셋을 직접 보면서 더 가독성이 높은 코드의 특성을 익히고 가독성이 높은 코드를 생성하는 방법을 더 직접적으로 배울 수 있을 것이다. 이것이 리워드 모델 없이도 DPO가 잘 작동하는 이유이기도 하다.

이런 해석은 허깅페이스 팀이 2023년 DPO를 활용해 학습해 공개한 제퍼2ephyr 모델에 서도 드러난다. 

허깅페이스 팀은 [「Zephyr: Direct Distillation of IM Alignment(제퍼:언어 모델 정렬을 위한 직접 증류)」](https://arxiv.org/abs/2310.16944)라는 연구 발표를 통해 제퍼 모델을 학습시킨 과정을 설명했는데, DPO 학습 전에 지도 미세 조정을 하지 않은 경우에는 DPO 학습을 통해 성능이 증가하지 않았다. 
지도 미세 조정을 거치지 않은 언어 모델은 지시사항과 응답의 형식을 이해하기 어려워했기 때문이다.

### DPO를 사용해 학습한 모델들
2023년 5월 DPO가 처음 발표됐을 때는 비교적 작은 모델에서 실험한 결과만 포함돼 있었기 때문에 더 큰 모델에서도 DPO가 잘 동작하는지 의문을 가진 사람들도 많았다. 
또한 DPO를 사용하려면 여전히 선호 데이터셋을 구축하는 작업이 필요한데, 선호 데이터 셋 구축에 많은 시간과 노력이 필요하다. 
만약 더 효율적으로 선호 데이터셋을 구축할 수 있다면, 더 적은 비용으로 더 빠르게 사람의 선호를 반영한 LLM을 만들 수 있을 것이다.
이제부터는 DPO를 사용해 학습된 몇 가지 모델의 예시를 살펴보면서, 더 효율적으로 선호 데이터셋을 구축하기 위한 방법과 더 큰 모델에서도 DPO가 잘 동작하는지 확인해 본다.

먼저, 허깅페이스 팀이 2023년 공개한 [제퍼-7B-베타](https://huggingface.co/Hugging FaceH4/zephyr-7b-beta)는 AI 평가를 사용해 DPO 학습 데이터를 구축했다. 
제퍼 는 4개의 LLM이 생성한 결과를 AI(예: GPT-4)가 평가하고 가장 높은 점수를 받은 선호 데이터와 나머지 3개 중 랜덤으로 선택한 비선호 데이터쌍을 구축해 dDPO를 수행했다. 

이때 DPO 앞에 d가 붙었는데, 다른 LLM이 생성한 데이터를 활용했기 때문에 지식 증류(knowledge distillation)를 했다는 의미다. 
제퍼는 발표 당시 최고 성능의 사전 학습 모델로 DPO가 성공적으로 동작하고 더 나아가 사람의 평가가 아닌 AI의 평가로도 잘 동작한다는 사실을 확인시켜 줬다. 

지식 증류와 제퍼에 대한 더 자세한 내용은 7.3절에서확인할 수 있다.

다음으로 인텔이 발표한 [뉴럴-챗-7B](https://huggingface.co/Intel/neural-chat-7b-V3-1)는 DPO 학습을 수행한 모델로, 2023년 11월 최고 성능의 70억 파라미 터 모델이었다. 
뉴럴-챗-7B는 DPO 학습 데이터로 [Intel/orca_dpopairs](https:// huggingface.co/datasets/Intel/orca_dpo.pairs)를 사용했는데, 별도로 사람이나 AI가 선호를 판단하지 않고 GPT-3.5 또는 GPT-4가 생성한 답변을 선호 데이터로, 라마-2-13B 모델이 생성한 답변을 비선호 데이터로 사용했다. 
사람이 직접 선호 데이터를 구축 하거나 AI 모델을 활용해 평가를 수행한 이전 연구와 달리 단순히 더 성능이 뛰어난 모 의 생성 결과를 선호 데이터로 선택함으로써 학습 과정을 더 단순화했다.

마지막으로, 앨런 AAlen A의 루-2TULU 2 모델은 파라미터가 700억 개인 모델에서도 DPO를 통해 사람의 선호를 학습시킬 수 있다는 점을 확인했다. 
DPO 논문과 앞서 살펴 본 제퍼, 뉴럴-챗-7B 모두 파라미터가 70억 개 수준의 작은 크기에서 DPO의 학습 효과를 확인했기 때문에 더 큰 모델에서도 잘 작동하는지 의문이 남아 있었다. 
하지만 파 라미터가 700억 개에 달하는 루 2가 DPO를 통해 성공적으로 학습되면서 DPO의 확 장성에 대한 의문은 어느 정도 해결된 것으로 보인다[『Camels in a Changing Climate:Enhancing IM Adaptation with TULU 2(변화하는 기후 속의 낙타: 언어 모델 조정 개선하기)」.](https://arxiv.org/pdf/2311.10702.pdf).

더 큰 모델에서의 확장성에 대한 의문이 줄어들었고, 사람이 아닌 AI를 사용해 선호 데 이터셋을 구축하는 제퍼의 아이디어나 비교적 성능이 뛰어난 모델의 생성 결과를 선호 데이터, 성능이 낮은 모델의 생성 결과를 비선호 데이터로 사용하는 뉴럴-챗-7B 모델 의 아이디어를 활용한다면 더 쉽게 선호 학습을 수행할 수 있다. 

DPO는 LILM의 선호 학 습을 가속화하는 핵심 기술이 됐다.

