---
sticker: emoji//1f916
---
# 지도 학습

:::question 지도 학습이란?
- 지도 학습(Supervised Learning)은 입력 데이터($X$)와 이에 대응하는 정답 레이블($y$)을 사용해 모델을 학습 시키는 방법입니다. 
- 목표는 입력과 출력 간의 관계를 학습해 새로운 입력에 대해 정확한 출력을 예측하는 것.

:::

## 머신러닝 개발자 역할

- - 개발자는 데이터(X,y X, y X,y)를 기반으로 모델(예: 선형 회귀)을 설계하고, **손실 함수(Loss Function)** 를 정의해 모델이 예측($\hat\{y}$)과 실제 값($y$)의 차이를 최소화하도록 학습 시킵니다.
- 기준은 **모델의 성능**(예: 예측 오차 최소화)과 **일반화 능력**(새 데이터에서도 잘 작동). 
	- 이를 위해 수학적 원리(수식)와 최적화 알고리즘(예: 경사 하강법)을 사용.
- 선형 회귀는 지도 학습의 가장 단순한 형태로, 연속적인 출력(예: 집값, 온도)을 예측하는 **회귀 문제**에 적합.

:::tip 선형 회귀가 지도 학습에서 중요한 이유
- **직관적**: 입력과 출력의 선형 관계를 가정해 이해하기 쉬움.
- **기초**: 더 복잡한 모델(신경망, SVM 등)의 수학적 원리를 이해하는 첫걸음.
:::

 >- **실제 적용**: 집값 예측, 주식 가격 예측 등 연속적인 값을 예측하는 데 사용.


## 선형 회귀와 수식과 그 의미
선형 회귀는 입력 $X$와 출력 $y$ 간의 선형 관계를 모델링합니다. 수식과 각 요소의 의미를 단계별로 풀어보겠습니다.

### 모델 수식

**수식**: $\hat\{y} = wX + b$
-  $\hat\{y}$ : 예측값 (모델이 추정한 출력)
- $X$: 입력 데이터 (예: 집 크기, 공부 시간)
- $w$: 가중치 (입력의 중요도를 조절, 기울기 역할)
- $b$ : 편향 (절편, 기본 값 조정)

**의미**:
- 이 수식은 입력 $X$를 가중치 $w$로 스케일링하고, 편향 $b$를 더해 출력 $y$를 근사.
- 예: 집 크기($X$)로 집값($y$)을 예측하려면, $w$는 평당 가격, $b$는 기본 비용(예: 토지 비)로 해석 가능.

**왜 선형인가?**:
- 선형 관계는 수학적으로 간단하고, 미분 가능해 최적화에 적합.
- 현실 데이터가 완벽히 선형은 아니지만, 선형 회귀로 시작해 복잡한 모델로 확장 가능.

**근본 원리**:
- 선형 회귀는 데이터를 직선으로 가장 잘 설명하려는 시도. 직선은 $y=wx+b$로 표현되며, 이는 1차 함수의 형태. 
- 이 가정은 데이터가 대략적으로 선형 패턴을 따를 때 유효.

:::question 왜 $wX + b$  형태를 쓰나요?
- 이는 직선 방정식($y=mx+c$)의 일반화된 형태로, 
- 다차원 입력(벡터 $X$)에도 확장 가능(예: $\hat\{y} = w_1x_1 + w_2x_2 + \dots + b$)

:::

### 손실 함수: 평균 제곱 오차 (MSE)

**수식**: $L = \frac\{1}\{n} \sum_\{i=1}^n (y_i - \hat\{y}_i)^2$ 
- - $y_i$​: 실제 값.
- $\hat\{y}_i$i​: 예측값 ($wX_i + b$).
- $n$: 데이터 포인트 수.

**의미**:
- MSE는 예측값과 실제 값의 차이(오차)를 제곱해 평균낸 값.
- 제곱을 쓰는 이유:
    1. **양수화**: 오차가 양수/음수 상관없이 누적되도록.
    2. **큰 오차 강조**: 큰 오차를 더 강하게 패널티로 부여.
    3. **미분 가능**: 제곱 함수는 미분이 쉬워 최적화에 유리.

:::question **왜 MSE인가?**
- MSE는 오차의 "크기"를 직관적으로 측정.
- 통계적으로, MSE는 가우시안 노이즈를 가정한 데이터에 적합한 손실 함수(최대 우도 추정과 연결).

:::

**근본 원리**:

- MSE는 모델이 데이터에 얼마나 잘 맞는지 측정하는 **지표**. 
- 손실이 작을수록 모델이 데이터를 잘 설명한다고 가정.
- 수학적으로, MSE는 볼록 함수(`convex`)라서 최적화 시 글로벌 최솟값을 보장.
:::question 왜 제곱을 쓰지 않고 절댓값을 안 쓰나요?
- 절댓값(MAE)은 미분 불연속점이 있어 최적화가 까다로움. 제곱은 미분이 매끄럽고, 통계적 가정에 부합.

:::

### 최적화 : 경사 하강법

- **수식**:
    - 가중치 업데이트: $w\leftarrow w - \eta \frac\{\partial L}\{\partial w}$
    - 편향 업데이트:  $b \leftarrow b - \eta \frac\{\partial L}\{\partial b}$
    - 기울기 계산:
        - $\frac\{\partial L}\{\partial w} = -\frac\{2}\{n} \sum_\{i=1}^n (y_i - \hat\{y}_i)x_i$
        - $\frac\{\partial L}\{\partial b} = -\frac\{2}\{n} \sum_\{i=1}^n (y_i - \hat\{y}_i)$
    - $\eta$ : 학습률 (예: 0.01, 스텝 크기 조절).
- **의미**:
	- 경사 하강법은 손실 함수 $L$의 기울기를 따라 가중치와 편향을 조정해 손실을 최소화.
	- 기울기($\frac\{\partial L}\{\partial w}$​)는 손실이 $w$ 방향으로 얼마나 증가하는지를 나타냄.

:::question 왜 이런 방식인가?
:::

 >- 손실 함수는 $w,b$ 에 대한 함수로, 볼록한 형태라 기울기를 따라 내려가면 최솟값에 도달.
> - 체인 룰을 사용해 기울기를 계산: $L = \frac\{1}\{n} \sum (y_i - (w x_i + b))^2$를 $w$로 미분.

**근본 원리**:
- 경사 하강법은 **최적화 문제**를 푸는 방법. 손실 함수의 최솟값을 찾는 건 데이터에 가장 잘 맞는 직선을 찾는 것과 같음.
- 미분(기울기)은 손실 함수의 변화율을 알려주며, 이를 반대 방향으로 이동해 최적점을 탐색.


:::question 기울기 계산은 어떻게 나온 건가요?
-  $L = \frac\{1}\{n} \sum (y_i - \hat\{y}_i)^2$, $\hat\{y}_i = w x_i + b$
- $\frac\{\partial L}\{\partial w} = \frac\{1}\{n} \sum 2 (y_i - \hat\{y}_i) \cdot \frac\{\partial}\{\partial w} (y_i - \hat\{y}_i)$ 
- $\frac\{\partial}\{\partial w} (y_i - \hat\{y}_i) = \frac\{\partial}\{\partial w} (y_i - (w x_i + b)) = -x_i$
- 따라서 $\frac\{\partial L}\{\partial w} = -\frac\{2}\{n} \sum (y_i - \hat\{y}_i)x_i​.$

:::

### 선형 회귀의 실제 값과 예측값의 의미
- **실제 값 ($y$):**
	- 데이터에서 주어진 정답. 예: 실제 집값, 실제 시험 점수.
	- 현실의 관측값으로, 노이즈(잡음)가 포함될 수 있음
- **예측값 (y^ \hat\{y} y^​)**:
    - 모델이 입력 X X X를 기반으로 추정한 값.
    - 목표는 y^ \hat\{y} y^​가 y y y에 최대한 가까워지도록 w,b w, b w,b를 조정.
- **의미**:
    - **예측값의 역할**: 새로운 입력 데이터에 대해 미래 값을 추정(예: 새로운 집 크기로 집값 예측).
    - **오차 (y−y^ y - \hat\{y} y−y^​)**: 모델의 부정확성을 나타냄. MSE는 이 오차를 정량화.
    - **실제 값과의 비교**: 오차가 작을수록 모델이 데이터를 잘 설명. 하지만 과적합(overfitting)을 주의해야 함.
- **왜 중요한가?**:
    - 선형 회귀는 데이터를 이해하고, 패턴을 찾아 예측하는 첫걸음.
    - 실제 값과 예측값의 차이는 모델 개선의 단서(예: 오차가 크면 비선형 모델로 확장).

**근본 원리**:

- 선형 회귀는 데이터의 **패턴**을 수학적으로 표현하려는 시도. 실제 값은 현실의 복잡성을 반영하고, 예측값은 그 복잡성을 단순화한 모델의 추정. 이 둘의 차이를 줄이는 과정이 학습.

**대화 포인트**: "실제 값과 예측값의 차이가 왜 중요한가요?" → 차이(오차)는 모델이 얼마나 틀렸는지를 보여주고, 이를 줄이는 게 학습의 목표. 오차 분석으로 모델의 한계(예: 선형 가정의 부적합)도 파악 가능.