---
slug: "서브-도메인-찾기"
---
# 한 도메인에 여러 애플리케이션이 있는 경우
메가 뱅크라는 이름의 은행있다고 가정하자
`mega-bank.com` 는 전체 도메인을 포괄한다.

새로운 취약점은 `mega-bank.com` 에 나타나기 보단 다른 서버에 나타날 가능성이 크다.
![Diagram.svg](/img/이미지 창고/Diagram.svg)
가장 먼저 할 일은 정찰을 수행해 mega-bank.com 에 붙은 서브도메인의 목록으로 웹 애플리케이션 맵을 채우는 것이다.
www 는 외부에 공개된 웹 애플리케이션 자체를 가리키므로 우리는 그쪽에 관심 없다.

고객을 상대하는 큰 회사들은 실제로 기본 도메인에 붙은 여러 서브 도메인을 운영한다.
이러한 서브도메인들은 각각 이메일, 관리자 애플리케이션, 파일 서버 등의 용도로 사용된다.

서브도메인 데이터를 찾는 방법은 많이 있으며, 원하는 결과를 얻으려면 여러 방법을 시도 해봐야 할 것이다.
우선 가장 단순한 방법부터 알아보자

## 브라우저에 내장된 네트워크 분석 도구
처음에는 메가뱅크에서 눈에 보이는 기능을 따라가며, 백그라운드에서 일어나는 API 요청을 관찰할으로써 유용한 데이털르 수집할 수 있다.
이러한 방법으로 손쉬운 먹잇감이 되는 엔드포인트를 찾아낼 수도 있다.
어떤 요청이 이뤄지는지 확인하려면 웹 브라우저에 내장된 네트워크 도구를 사용할 수도 있고, Burp, PortSwigger, ZAP 같은 강력한 도구도 있다.

브라우저의 개발자 도구를 사용해 위키피디아에 대한 네트워크 요청을 조회, 수정, 재 전송, 기록하는 것을 나타낸다.
세가지 주요 브라우저에는 이와 같은 매우 강력한 개발자 도구가 포함되어 있다.

사실 서드파티 도구를 구입할 필요가 없이개발자 도구만 가지고도 실력 있는 해커가 될 수 있다.
현대적 브라우저에는 네트워크 분석과 코드 분석 기능이 있으며, 중단점과 파일 참조, 정확한 성능 측정(side-channel 공격에 해킹 도구로 사용할 수 있다.)
기능이 포함된 JS 런타임 분석을 위한 도구를 제공한다.
또한 간단한 보안 및 호환성 감시를 수행하는 도구도 있다.

브라우저에서 네트워크 트래픽을 분석하는 방법은 다음과 같다.
1. 네비게이션 막대 오른쪽 상단의 점 세 개 아이콘을 클릭한다.
2. <mark>도구 더 보기</mark> -> 개발자 도구를 선택한다.
3. 상단 메뉴에서 Network 탭을 클릭한다.
	1. 만약 보이지 않으면, 해당 탭이 보일 때까지 개발자 도구를 옆으로 늘린다.

 브라우저에서 Network 탭을 사용해 브라우저가 처리하는 모든 네트워크 트래픽을 볼 수 있다.
 큰 사이트에서는 요청이 너무 많아 필터링 하기 겁날 수 있다.
가장 흥미로운 결과는 Network 탭의 XHR 탭에 많다.

이 탭은 서버에 대한 HTTP POST, GET, PUT, DELETE 및 기타 요청을 보여주며 폰트, 이미지, 비디오, 의존성 파일은 걸러낸다.
왼쪽 영역에서 개별 요청을 클릭해 상세한 내용을 볼 수 있다.

이 요청들 중 하나를 클릭하면 원본 요청과 함께 요청 헤더와 본문을 포함하는 포매팅 된 버전도 볼 수 있다.
각 요청을 선택했을 때, 나타나는 Preview 탭에서는 API 요청을 보기 좋게 포매팅해 보여준다.

XHR 아래의 Response 탭은 원시 응답 페이로드를 보여주며 Timming 탭에서는 요청과 관련된 Queuing, 다운로드,
대기시간을 보여준다.
이러한 성능 지표들은 서버에서 어떤 코드가 실행되는지 알아내기 위해 응답 이외의 부차적 지표(예 : 같은 엔드포인트를 통해 호출된 두 스크립트의 로딩 시간 차이)에 의존하는 사이드 채널 공격에 사용할 수 있어 매우 중요하다

이제 브라우저 Network 탭에 친숙해졌으니, 정찰에 사용해보자. 처음에는 막막하겠지만, 배우기 어렵지 않다.

## 공개된 레코드를 이용하기
오늘날 웹에 공개된 정보는 너무나 많고, 실수로 누설된 데이터가 몇 년동안 방치되기도 한다.
뛰어난 해커는 이러한 점을 이용해 쉽게 공격할 수 있는 흥미로운 정보를 찾아낸다.

지금까지 침투 테스트를 수행하면서 다양한 정보를 웹에서 찾을 수 있다.
- 실수로 공개했다가 비공개로 되돌린 깃허브 저장소의 캐시된 사본
- SSH 키
- AWS 나 Script 같은 서비스의 여러 가지 키를 공개된 웹 어플리케이션에 임시로 게시했다가 삭제한 것
- 외부에 공개하지 않는 DNS와 URL 목록
- 출시 전인 제품의 세부 사항을 담은 비공개 페이지
- 웹에서 제공하되 검색 엔진의 크롤링을 허가하려고 하지 않았던 재무 기록
- 이메일 주소, 전화번호, 사용자 명
이러한 정보를 다음과 같은 곳에 구할 수 있다.
- 검색 엔진
- 소셜미디어 게시물
- archive.org 와 같은 아카이빙 애플리케이션
- 이미지 검색과 역 이미지 검색

공개된 레코드도 서브도메인을 찾으려고 시도할 때 좋은 정보다.
서브 도메인은 딕셔너리를 통해 쉽게 찾을 수 없지만, 앞에서 나열할 서비스 중 한 곳에 색인 되어 있을 수도 있기 때문이다.

## 검색 엔진 캐시
구글은 세계적으로 가장 널리 사용되는 검색 엔진이므로, 다른 어느 검색 엔진보다 많은 데이터가 색인 되어 있을 것이다.
구글 검색 결과 데이터는 너무나 많아서 그중 가치 있는 것을 수작업으로 찾아내려고 하면 정찰에 그리 유용하지 않을 것이다.

또한 구글은 자동화된 요청을 식별해 거부하므로 진짜 웹 브라우저와 거의 똑같이 요청을 하지 않으면 안된다.
다행히 구글에서는 검색 질의를 구체적으로 할 수 있는 특수한 검색 연산자를 제공한다.
`site:<사이트명>` 특정 도메인에 대한 질의할 수 있다.

유명한 사이트에 이와 같이 질의하면 주 도메인의 콘텐츠 페이지가 주로 반환 되는데 거기서 흥미로운 서브 도메인의 페이지가 아주 조금 섞여 있을 수도 있다.
흥미로운 것을 찾아내려면 검색 범위를 좁힐 필요가 있다.

특정 질의 문자열에 대한 부정 조건을 추가하는 마이너스 연산자를 사용한다.
예를 들어 `-inurl:<패턴>`은 주어진 패턴과 일치하는 URL을 제외한다.

구글 검색의 `site:`와 `--inurl:<패턴>` 연산자를 조합한 검색의 예를 나타낸다.
이 두 연산자를 조합함으로써 `wikipedia.org` 웹 사이트에서 URL에 'dog'가 포함된 페이지를 제외하고 강아지에 관련된 페이지만 찾도록 구글에 질의 할 수 있다.

이 기법을 사용해 검색 결과의 수를 줄이고 불필요한 키워드를 배제하면서 특정 서브 도메인을 찾을 수 있다.

구글과 같은 검색 엔진의 검색 연산자 사용법을 익힘으로써, 다른 방법으로 쉽게 찾을 수 없는 정보를 얻을 수 있다.
우리가 이미 알고 잇는 www 같은 서브도메인을 제외하는데도 `--inurl:<패턴>` 연산자를 사용할 수 있다.

이 연산자는 서브도메인에만 적용되는 것이 아니라, URL 문자열 전체에 적용 되므로, URL의 다른 부분에 포함된 www까지 검색 결과에서 제외될 수 있음에 유의해야 한다.
예를 들어 다음과 같은 연산자를 사용해 질의하면 결과에서 `https://admin.mega-bank.com/www`까지 제외되는 거짓 긍정이 발생한다.

> `site:mega-bank.com -inurl:www`

여러 사이트에 이 기법을 활용해 보면, 이전에 존재하는지 생각하지 못했던, 서브 도메인을 찾아낼 수 있을 것이다.
유명한 소셜 뉴스 사이트인 레딧을 조사해보자

> `site:reddit.com -inurl:www`

이 질의의 결과로 나오는 `code.reddit.com` 은 레딧의 이전 버전에서 사용한 소스 코드를 아카이브 한 것으로 이유가 있어서 대중에게 공개하기로 결정한 것이다.
reddit 같은 웹 사이트는 목적에 따라 이런 도메인을 공개한다.

메가 뱅크에 대한 침투 테스트를 수행하는 우리 입장에서는 공개된 도메인 중 흥미롭지 않은 것을 질의에서 제외할 수 있다.
만약 메가뱅크가 `mobile.mega-bank.com` 이라는 서브 도메인에서 모바일 버전을 서비스 한다면 다음과 같이 쉽게 제외할 수 있다.

> `site:mega-bank.com -inurl:www -inurl:mobile`

주어진 사이트의 서브도메인을 찾으려고 시도 할 때 이러한 과정을 반복함으로써, 적합한 결과를 추려낼 수 있다.
구글 뿐 아니라 빙과 같이 규모가 큰 검색 엔진은 모두 이와 비슷한 연산자를 지원하므로, 한번 시도해보면 도움이 될 것이다.
이 기법을 통해 흥미로운 점을 찾아낸 것이 있으면, 기록해두고 또 다른 서브 도메인 탐색 기법으로 넘어가쟈

## 의도하지 않은 아카이브
`archive.org` 같은 공개 아카이브 유틸리티는 웹 사이트의 스냅숏을 주기적으로 남기므로, 과거의 웹 사이트의 사본을 방문할 수 있어 유용하다.
`archive.org` 수많은 사이트가 사라지고, 그 도메인을 새로운 사이트가 차지하는 인터넷의 역사를 보존하는 것을 추구한다.
`archive.org` 에 저장된 웹 사이트의 과거 스냅 숏은 때로는 20년을 거슬러 올라가기도 하므로, 한때 공개되었다가

`archvie.org`에 저장된 웹 사이트의 과거 스냅숏은 때로는 20년을 거슬러 올라가기도 하므로 한때(의도적으로 혹은 실수로) 공개되었다가 나중에 사라진 정보를 찾을 수 있는 금광과도 같은 곳이다.

일반적으로 검색 엔진은 웹사이트 관련 데이터를 색인 할 뿐만 아니라 캐시를 최신으로 유지하기 위해 웹 사이트를 주기적으로 크롤링 할 것이다.
따라서 관련성이 있는 현재 데이터를 보려면 검색 엔진을 이용하는 것이 맞지만, 이력 데이터는 웹 사이트 아카이브에서 찾아보는 것이 더 낫다.

트래픽을 기준으로 가장 유명한 웹 기만 미디어 회사는 뉴욕 타임즈이다.
이회사는 주 웹사이트인 `https://nytimes.com`을 archive.org 에서 조회하면, 첫 페이지의 스냅 숏이 1996년 부터 현재까지 20만 건 넘게 저장되어 있는 것을 볼 수 있다.

이력 스냅숏은 웹 어플리케이션의 주요 릴리즈 시점이나 중요한 보안 취약점 공개 시점을 알고 있거나 유추할 수 있을 경우 특히 가치가 높다.
한때 HTML 이나 JS를 통해 노출되었다가 현재는 숨겨진 서브도메인의 하이퍼링크가 이력 아카이브가 남아있는 경우도 있다.

브라우저에서 `archive.org` 스냅숏을 오른쪽 클릭하고 View source 를 선택해 공통적인 URL 패턴을 재빨리 검색할 수 있다.

`https://` 나 `http://` 를 검색하면 모든 HTTP 하이퍼링크가 나타나며, `file://` 을 검색하면 과거의 다운로드 할 수 있었던 것들을 찾을 수 있다.

다음과 같은 방법으로 아카이브에서 서브 도메인을 자동으로 탐색 할 수 있다.

1. 충분한 시간 간격을 두고 서로 다른 10개 날짜를 선택해 각 아카이브를 연다
2. ‘View source’ 를 오른쪽 클릭하고 Ctrl-A 를 눌러 HTML을 선택한다.
3. HTML을 클립 보드에 복사한다.
4. 바탕화면에 legacy-soucrce.html 이라는 파일을 생성한다.
5. 아카이브에서 복사한 소스코드를 파일에 붙여넣는다.
6. 열려 있는 9개의 아카이브에 대해 반복한다
7. 이 파일을 선호하는 편집기(vm, atom, vscode)에서 연다.
8. 일반적엔 URL 스킴에 대해 검색을 수행한다.
	- `http://`
	- `https://`
	- `file://`
	- `ftp://`
	- `ftps://`
브라우저에서 지원하는 모든 URL 스킴의 목록을 명세(https://oreil.zhTcF)에서 찾을 수 있다.

## 소셜 스냅숏
주요 소셜 미디어 회사는 사용자의 비밀스러운 정보가 안전하게 처리 된다는 인식을 심는 데 상당한 노력을 기울인다.
이것은 종종 고객 데이터가 얼마나 안전한지 소개하는 마케팅을 통해 이뤄진다.
그러나 이것은 호감도를 높이고 활성 사용자를 유지하는 것을 돕기 위한 것일 뿐 일때가 많다.
이러한 주장이 정당성을 가질 만큼 강제력 있는 현대적인 법률을 제정한 국가는 그리 많지 않다.

이러한 사이트들의 수많은 사용자는 어떤 데이터가 어떤 수단에 의해 공유되며 어떤 목적으로 사용되는지 완전히 이해하지 못하는 것으로 보인다.
회사의 의뢰를 받아 침투 테스트를 수행할 때 소셜 미디어 데이터로부터 서브 도메인을 찾는 것은 대체로 윤리에 어긋나지 않는다.
그렇다 하더라도 이러한 API를 정찰에 이용할 때는 범위를 좁혀서 최종 사용자의 프라이버시 침해를 최소화해야한다.

여기서는 단순한 설명을 위해 트위터 API를 사요해 정찰을 하는 예를 들겠지만, 다른 주요 소셜 미디어 회사들도 비숫한 구조의 API를 제공한다.
트위터 API로 트윗 데이터를 질의 및 검색하는 개념을 다른 주요 소셜 미디어에도 적용할 수 있다.

### 트위터 API
트위터는 데이터를 검색 및 필터링 하는 여러 방법을 제공한다
각각의 방법으 범위, 기능, 데이터 세트에서 차이가 있다.
더 많은 데이터에 액세스를 원할 수록 또 데이터를 요청하고 필터링 하는 방법을 더 많이 원할 수록 더 많은 비용이 든다.
경우에 따라 검색은 로컬이 아닌 트위터 서버에서 수행될 수도 있다.

악의적 목적으로 이용하는 것은 트위터 이용약관에 위배되므로 이러한 사용은 화이트 햇에서만 허용됨을 기억하자

가장 낮은 티어의 시험용 검색 API는 30일 간의 트윗을 필터링할 수 있고, 질의당 100트윗까지만 요청하 수 있으며, 1분에 30회까지 질의 할 수 있다.
무료 티어 API를 가지고 수행할 수 있는 월간 총 질의는 250회가 상한이다.
이 티어에서 제공된 최대 월간 데이터 세트를 얻는데 10분에 해당하는 질의가 소요된다.

즉, 좀 더 높은 멤버십 티어를 유로로 이용하지 않고 분석 할 수 있는 트윗은 25,000개에 불과하다.

이러한 제약으로 코딩 도구에서 API를 분석하기가 쉽지 않다.
기업에서 의뢰받은 프로젝트를 위해 트위터를 정찰할위필요가 잇다면, 업그레이드를 하든지 다른 데이터 소스를 찾아봐야할 것이다.

서브도메인 정찰을 위해 이 API를 사용해서 `*.mega-bank.com`에 대한 링크를 포함하는 JSON 을 만들 수 있다.
트위터 검색 API를 질의 하려면 다음이 필요하다.
- 등록된 개발자 계정
- 등록된 앱
- 인증을 위한 bearer token
문서가 흩어져 있고, 예제가 부족해서 이해하기 어렵지만, 이 API에 질의하는 법은 단순하다.
```shell
curl —request POST \
	—url https://api.twitter.com/1.1/tweets/search/30day/Prod.json \
	—header ‘authoriztion: Bearer <MY_TOKEN>’ \
	—header ‘content-type: application/json’ \
	—data ‘{
		“maxResults”: “100”,
		“keyword”: “mega-bank.com”
	}’
```
이 API는 키워드에 대한 퍼지 검색을 디폴트로 수행한다.
정확히 일치하는 결과를 얻으려면 큰 따옴표로 문자열을 감싸서 보내야한다.
큰 따옴표는 `”keyword”: “\mega-bank.com\””`와 같은 형식이 유효한 JSON을 통해 보낼 수 있다.

이 API의 결과를 기록해 링크를 검색하다 보면 이전에 몰랐던 서브 도메인을 발견할 수도 있다.
메인 앱이 아닌 다른 서버에 링크된 마케팅 캠페인, 광고 추적기, 구인 공고 등을 통해 수확을 얻는 경우가 많다.

좀 더 실체적인 예를 위해 MS와 관련된 트윗을 요청하는 질의르 작성해보자.
트윗을 필터링 하면 다음과 같은 MS의 여러 서브도메인이 트위터를 통해 홍보되는 것을 알 수 있다.
- `careers.microsoft.com` (구인 공고)
- `office.microsoft.com`(MS office 홈페이지)
- `powerbi.microsoft.com`(PowerBI 제품 홈페이지)
- `support.microsoft.com`(고객 지원)
트윗이 충분히 유명해지면 주요 검색 엔진에서도 색인 된다.
그러므로 별로 인기를 끌지 못한 트윗을 찾을 때 트위터 API를 분석하면 좋다.
많은 인기를 끈 트윗은 인바운드 링크 수가 많기 때문에, 검색 엔진에도 색인된다.
그러므로 앞 장에서 한 것 처럼 검색 엔진에서 적절한 연산자를 사용해 질의 하는 것이 때로는 더 효율적일 수도 있다.

이 API의 검색 결과가 정찰 프로젝트에서 사용하기에 만족스럽지 않다면, 트위터에서 제공하는 스트리이밍 API와 firehose API를 사용할 수 있다.

트위터의 스트리밍 API는 트윗을 실시간으로 분석할 수 있는 라이브 스트림을 제공한다.
하지만, 실제 라이브 트윗은 실시간으로 처리해 개발자에게 보내기에는 규모가 너무 크기 때문에, 스트리밍 API를 통해 제공하는 것은 극히 일부이다.
때에 따라서 트윗의 99% 이상을놓칠 수도 있다.
만약 조사하려는 앱이 큰 인기를 끈다면, 스트리밍 API가 유용할 수 있다.
그렇지만 스타트업 정찰을 하는 경우라면 그다지 도움되지 않을 것이다.

트위터의 firehose API는 스트리밍 API와 비슷하지만, 검색 조건과 일치하는 트윗을 100% 제공함을 보장한다.
대부분의 정찰에서는 관련성을 중시하므로 firehose API가 스트리밍 API보다 유용할 것이다.
결론적으로, 트위터를 정찰 도구로 활용할 때 따를 규칙은 다음과 같다.
- 웹 애플리케이션 대부분에 대해 검색 API로 질의하면 정찰과 가장 관련성 있는 데이터를 얻을 수 있다.
- 대규모 앱이나 큰 인기를 끄는 앱에 대해서 firehose 또는 스트리밍 API를 사용하여 유용한 정보를 얻을 수 있을 것이다.
- 이력 정보만으로 충분한 상황이라면 대규모 이력 데이터 덤프를 다운로드해 로컬에서 질의하는 것을 고려한다.
주요 소셜 미디어 사이트의 거의 대부분은 정찰이라든지 다른 형태의 분석에 유용하게 사용할 수 있는 데이터 API들을 제공한다는 사실을 기억하자
그 중 한가지 API로 원하는 결과를 찾지 못하더라도 다른 API를 가지고 성공할 수도 있다.

## 존 전송 공격
지금까지는 일반에 공개된 웹 앱을 둘러보며 네트워크 요청을 분석했다.
공개 웹 앱에 링크되어 있지 않는 서브 도메인을 찾는 방법을 알아보자

존 전송 공격(zone transfer attack)은 올바로 구성하지 않은 DNS서버를 정찰하는 트릭이다.
이름에서 풍기는 느낌과 달리 이것은 해킹이 아니다.
그리 큰 노력을 들이지 않고, 수행할 수 잇는 정보 수집 기법이며, 성공할 경우 귀중한 정보를 얻을 수 있다.
마치 유효한 DNS 서버로부터 유효한 DNS 존 전송 요청을 한 것처럼 특수한 포맷의 요청을 하는 것이 DNS 존 전송 공격의 핵심이다.

DNS 서버는 사람이 읽을 수 없는 도메인 네임(예 : `https://mega-bank.com`)을 기계가 읽을 수 있는 IP 주소(예 : 195.250.100.195)로 변환하는 역할을 하며, 계층적이며 공통적인 패턴을 사용해 저장하므로, 쉽게 요청 및 탐색을 할 수 있다.

DNS 서버는 서버의 애플리케이션 사용자를 업데이트 하지 않고도, 서버의 IP 주소를 바꿀 수 있다는 점에서 가치 있다.
달리 말해 사용자는 요청을 서버가 처리하는지 걱정할 필요 없이 할상 `https://www.mega-bank.com`을 방문할 수 있다.

DNS 시스템에서는 다른 DNS 서버와 DNS 레코드를 동기화하는 능력이 매우 중요하다.
DNS 존 전송은 DNS 서버가 DNS 레코드를 공유하는 표준적 방식이다.
레코드는 텍스트 기반의 zone file을 통해 공유한다.

zone 파일에 종종 포함되는 DNS 구성 데이터는 쉽게 액세스 할 수 없도록 의도 된다.
그 결과 적절하게 구성된 기본 DNS 서버는 권한 있는 보조 DNS 서버의 존 전송 요청만 처리 할 수 있어야 한다.
만약 DNS 서버는 권한 있는 모조 DNS 서버의 존 전송 요청만 처리 할 수 있어야 한다.

그렇게 하려면 먼저 `https://www.mega-bank.com`의 DNS 서버를 찾아야 한다.
유닉스 기반 시스템의 터미널에서 다음과 같은 명령을 사용해 DNS 서버를 쉽게 찾을 수 있다.

```shell
host -t mega-bank.com
```

host 명령은 리눅스 배포판 대부분과 맥 OS 최신 버전에서 찾을 수 있는 DNS 조회 유틸리티를 참조 한다.
`-t` 플래그는 `mega-bank.com`을 담당하는 네임 서버를 요청하고 싶다고 지정한다.

이 명령의 출력은 다음과 같다.
```shell
mega-bank.com name server ns1.bankhost.com
mega-bank.com name server ns2.bankhost.com
```
결과에서 중요한 부분은 `ns1.bankhost.com`과 `ns2.bankhost.com`으로 `mega-bank.com`을 담당하는 네임 서버들이다.
존 전송 요청을 하는 방법은 매우 간단하다.

```shell
host -l mega-bank.com ns1.bankhost.com
```

`-l` 플래그는 레코드를 갱신하기 위해 `ns1.bankhost.com`으로부터 `meag-bank.com`에 대한 존 전송 파일을 얻고자 함을 나타낸다.
다음과 같이 요청이 성공한다면 DNS 서버의 보안이 부적절하다는 뜻이다.
```shell
Using domain server:
Name: ns1.bankhost.com
Address: 195.11.100.25

Aliases:
mega-bank.com has address 195.250.100.195
mega-bank.com name server ns1.bankhost.com
mega-bank.com name server ns2.bankhost.com
mail.mega-bank.com has address 82.31.105.140
admin.mega-bank.com has address 32.45.105.144
internal.mega-bank.com has address 25.44.105.144
```

`mega-bank.com` 도메인 아래의 웹 어플리케이션 공인 IP 주소 목록을 얻었다.
이러한 서브도메인 또는 IP주소에 대해서도 시도해볼 수도 있다.

약간의 운이 따라준다면, 공격면을 획기적으로 넓힐 수 있을 것이다.

안타깝게도 DNS 존 전송 공격이 항상 계획대로 되지 않는다.
올바로 구성된 서버는 존 전송 요청에 대해 다음과 같이 출력된다.

```shell
Using domain server:
Name: ns1.secure-bank.com
Address: 144.122.34.45
Aliases:

: Transfer Failed
```
존 전송 공격은 막기 쉬우며 많으며 애플리케이션이 이와 같이 적절히 구성되어 있어, 공격을 물리칠 것이다.
그렇지만, 셸에서 몇 줄만 타이핑 해서 존 전송 공격을 시도할 수 있으므로, 해볼 가치는 충분하다.

혹시 라도 성공한다면, 다른 방법으로는 찾을 수 없는 흥미로운 서브 도메인을 많이 알아낼지도 모른다.

## 서브도메인에 대한 브루트 포싱
서브도메인을 탐색하는 방법으로 마지막으로 소개할 것은 브루트 포스 전술이다.
브루트 포스는 보안 메커니즘을 미처 갖추지 못한 웹 애플리케이션에 대해서는 효과적이지만, 보안을 갖춘 웹 애플리케이션에 대해 시도할 때는 매우 정교한 구조가 필요하다.

브루트 포싱 시도는 쉽게 로깅되며, 그러한 유형의 스누핑을 방지할 목적으로 개발된 전송률 제한, 정규 표현식 등의 단순한 보안 메커니즘으로 매우 오랜 시간이 걸리곤 한다.
따라서 서브도메인에 대한 브루트 포스는 마지막 카드로 남겨두는 것이 좋다.

:::warning 브루트 포스 공격은 매우 쉽게 탐지되어 IP 주소가 로그에 남거나 서버 또는 관리자에 의해 차단당할 수 있다.

:::

브루트 포싱은 일치하는 것을 찾을 때까지 서브도메인의 가능한 모든 조합을 테스트함을 의미한다.
일치하는 서브도메인이 여러 개 있을 수 있으므로, 일치하는 것을 찾더라도 멈추지 말고, 계속 시도 할 것이다.

로컬 브루트 포스와 달리 타깃 도메인의 서브도메인에 대한 브루트 포스를 수행하려면 네트워크 연결이 필요하다.
브루트 포스를 원격에서 수행하므로 네트워크 지연으로 시도가 느려질 수 있다.
일반적으로 네트워크 요청당 50~250ms의 지연을 예상할 수 있다.

그러므로 요청의 응답이 올 때까지 기다렸다가 다음 요청을 하기 보다는 비동기 방식으로 최대한 빨리 수행해야 한다.
그렇게 하면 브루트 포스가 완료되는 데 소요되는 시간을 획기적으로 줄일 수 있다.

살아 있는 서브 도메인을 탐지하느넫 필요한 피드백 루프는 단순하다.
우리가 사용할 브루트 포스 알고리즘은 서브도메인으로 표시한다.
응답이 없으면 사용하는 서브도메인이 아닌 것으로 표시한다.

이 책에서 가장 중요하게 다루는 언어는 JS 이다.
JS는 웹 브라우저의 클라이언트 스크립팅에 사용되는 언어로, 강력한 백엔드 서버 측 언이이기도 하다.
JS를 가지고 두단계로 이뤄진 브루트 포스 알고리즘을 구현하자.

1. 잠재적 서브도메인의 목록을 생성한다.
2. 서브도메인 목록을 따라가면서 서브도메인이 살아있는지 하나하나 ping 요청을 해본다.
3. 살아 있는 서브도메인만 기록하고 사용하는 서브도메인이 아닌 것은 내버려둔다.
다음 코드는 서브도메인을 생성한다.
```js
/*
	각 서브도메인의 최대 길이를 받아 서브 도메인의 목록을 브루트 포싱하는 함수
*/
const generateSubdomains = function(length){
	/*
		생성할 서브 도메인을 구성하는 문자의 목록
		많이 사용되지 않는 '-' 문자 등을 포함하도록 바꿔도 딘다.
		브라우저에 따라 중국어, 아랍어, 라틴 문자열 등을 지원하는 경우도 있다.
	*/
	const sharset = 'abcdefghijklmnopqrstuvwxyz'.split('');
	let subdomains = charset;
	let subdomain;
	let letter;
	let temp;

	/*
		시간 복잡도 : o (n*m)
		n = 문자열 길이
		m = 유효한 문자의 개수
	*/
	for(let i = 1; i < length; i++){
		temp = []
		for(let k = 0; k < subdomains.length; k++){
			subdomain = subdomains[k];
			for(let m = 0; m < charset.lenght; m++){
				letter = charset[m];
				temp.push(subdomain + letter);
			}
		}
		subdomains = temp
	}
	return subdomains;
}
const subdomains = generateSubdomains(4);
```
이 스크립트는 chartset의 문자 목록으로부터 길이가 n 이하인 가능한 모든 조합을 생성하는 방법으로 서브도메인을 만들어낸다.
charset 문자열을 분할해 문자 배열을 만든 다음 초기 문자 집합을 문자의 배열에 할당한다.

다음으로 length 만큼의 횟수로 다음을 반복하는데 임시 저장 배열을 생성하는 문장이 있다.
그 다음으로 각 서브도메인에 대한 반복문이 있고, 그 내부에서는 사용 가능한 캐릭터 세트를 지정하는 charset 배열의 각 문자에 대한 반복문이 있다.

중첩된 반복문의 가장 안쪽에서는 서브도메인과 문자를 조합해 임시 배열을 작성한다.
이제 서브 도메인 목록을 사용해 `mega-bank.com` 같은 최상위 도메인(`.com`, `.org`, `.net` 등)에 대한 질의를 시작할 수 있다.

이를 위해 유명한 JS 런타임인 NodeJS 에서 제공하는 DNS 라이브러리를 이용하는 짧은 스크립트를 작성한다.
이 스크립트를 실행하려면 NodeJS의 최신 버전을 설치해야 한다.

```js
const dns = require('dns');
const promises = [];

/*
	앞에서 작성한 브루트 포스 스크립트를 사용하거나, 일반적인 서브 도메인의 딕셔너리를 사용해 목록을 채운다.
*/
const subdomains = [];

/*
	각 서브도메인에 대해 비동기 DNS 질의를 수행한다.
	이것은 일반적인 `dns.lookup()` 보다 성능이 높다.
	사용하는 운영체제의 getaddrinfo(3)은 동기식으로 구현되었기 때문이다. 
*/

subdomains.forEach((subdomain)=>{
	promises.push(new Promise((resolve, reject)=>{
		dns.resolve(`${subdomain}.mega-bank.com`, function(err, ip){
			return resolve({ subdomain: subdomain, ip: ip})
		})
	}))
})
// DNS 질의가 모두 완료되면 결과를 로깅
Promise.all(promise).then(function(result){
	results.forEach((result)=>{
		if(!!result.ip){
			console.log(result)
		}
	})
})
```

이 스크립트에서 브루트 포싱 코드의 명확성과 성능을 개선하기 위해 몇가지 일을 했다.
먼저 노드의 DNS 라이브러리를 import 한다.

그런 다음 promises 배열을 생성하는데 이 것은 promise 객체들을 저장하는 용도다.
Promise는 좀 더 단순한 방식으로 비동기 요청을 다루며 모든 주요 웹 브라우저와 NodeJS 에서 자체적으로 지원한다.

다음으로 subdomains 라는 배열을 생성하는데 첫 번째 스크립트에서 생성한 서브도메인을 담기 위한 것이다.
다음으로 forEach() 연산자를 사용해 subdomains 배열의 각 서브도메인에 대해 반복한다.
이것은 이터레이션과 동등하지만 구문이 좀 더 우아하다.

서브도메인 이터레이션의 각 수준에서 promise 배열에 새로운 promise 객체를 넣는다.
이 promise 객체에서 호출하는 `dns.resolve`는 NodeJS DNS 라이브러리에 있는 함수이며, 도메인 네임의 IP 주소를 찾으려고 시도한다.
우리가 promise 배열에 넣는 promise들은 DNS 라이브러리가 네트워크 요청을 마쳐야 처리된다.

끝으로 `Promise.all` 블록은 promise 객체의 배열을 취하며, 배열의 모든 promise가 처리(네트워크 요청을 완료)되어야 결과를 얻는다.(`.then()을 호출`)
result 앞의 느낌표 두개(`!!`) 연산자는 정의된 결과만 원한다는 것을 나타내므로 IP 주소를 반환하지 않는 시도를 무시한다.

`reject()`를 호출한 조건이 포함된 경우 오류를 처리하기 위한 `catch()` 블록이 끝에 필요하다.
DNS 라이브러리는 오류를 여러 개 던지는데 그 중 일부는 브루트 포스를 중단할 만큼 심각하지 않은 것들이다.

이 예제는 단순화를 위해 오류처리를 포함하지 않았지만, 직접 작성해보면 좋은 연습이 될 것이다.
또한 `dns.lookup`을 사용하지 않고, `dns.resolve`를 사용햇다는 점에 유의하자.

둘 다 js 관점에서는 비동기적(호출 순서에 무관)이지만, `dns.lookup`이 의존하는 네이티브 구현은 작업을 동기적으로 수행하는`libuv`를 사용하기 때문이다.

앞의 두 스크립트를 하나의 프로그램으로 합치는 것은 아주 쉽다.
먼저 잠재적 서브도메인 목록을 생성한 다음 서브도메인을 확인하는 비동기적 브루트 포스 시도를 수행해야한다.

```js
const dns = require('dns')
/*
	각 서브 도메인의 최대 길이를 받아 서브도메인의 목록을 브루트 포싱하는 함수
*/
const generateSubdomains = function(length){
	/*
		생성할 서브도메인을 구성하는 문자의 목록
		많이 사용되지 않는 '-' 문자 등을 포함하도록 바꿔도 된다.
		브라우저에 따라 중국어, 아랍어, 라틴 문자열 등을 지원하는 경우도 있다
	*/
	const charset = 'abcdefghiklmnpqrstuvwxyz'.split('');
	let subdomains = charset;
	let subdomain;
	let letter;
	let temp;

	/*
		시간 복잡도 : o(n*m)
		n = 문자열 길이
		m = 유효한 문자의 개수
	*/
	for(let i = 1; i < lenght; i++ ){
		temp = []
		for(let k = 0; k < subdomains.lenght; k++){
			subdomain = subdomains[k];
			for(let m = 0; m < charset.length; m++){
				letter = charset[m];
				temp.push(subddomain+letter);
			}
		}
		subdomains = temp
	}
	return subdomains;
}

const subdomains = generateSubdomains(4);
const promises = [];

/*
	각 서브도메인에 대해 비동기 DNS 질의를 수행한다.
	이것은 일반적인 `dns.lookup()` 보다 성능이 높다.
	JS에서는 `dns.lookup()`이 비동기인 것처럼 보여도 내부적으로 사용하는 운영체제의 getaddrinfo(3)
*/

subdomains.forEach((subdomain)=>{
	promise.push(new Promise((resolve, reject)=>{
		dns.resolve(`${subdomain}.mega-bank.com`, function (err, ip){
			return resolve( { subdomain: subdomain, ip: ip });
		});
	}));
});

// DNS 질의가 모두 완료되면 결과를 로깅
Promise.all(promise).then(function(result){
	results.forEach((result))
})
```

잠시 기다리면 유효한 서브 도메인 목록이 터미널에 나타난다.

```shell
{ subdomain : ‘mail’, ip : ’12.32.244.156’ }
{ subdomain : ‘admin’, ip : ’12.32.244.222’ },
{ subdomain : ‘dev’, ip : ’12.32.244.117’ },
…
```

## 딕셔너리 공격
가능한 모든 서브 도메인에 대해 시도하는 브루트 포스 공격 대신 딕셔너리 공격을 시도한다면, 속도를 훨씬 더 높일 수 있다.
잠재적 서브 도메인의 배열을 가지고 시도한다는 점에서 딕셔너리 공격은 브루트 포스 공격과 비슷하지만, 무작위로 생성하는 것이 아니라 공통적인 서브도메인의 목록으로부터 만든다는 점에서 차이가 있다.

딕셔너리 공격의 장점은 속도가 매우 빠르다는 것이다.
매우 유별나고 비표준적인 서브 도메인이 아니라면, 딕셔너리 공격으로 찾아 낼 수 있다.

유명한 오픈소스 DNS 스캐너인 dnscaan에는 인터넷에서 가장 많이 사용되는 서브 도메인 목록이 포함돼 있다.
이 목록은 86,000건의 DNS 존 레커ㅗ드로부터 얻은 수백만개의 서브도메인을 가지고 작성한 것이다.
dnscan의 서브도메인 스캔 데이터에 따르면 가장 많이 사용되는 25가지 서브도메인은 다음과 같다.

```shell
www
mail
ftp
localhost
webmail
smtp
pop
ns1
webdisk
ns2
cpanel
autodiscover
autoconfig
m
imap
test
ns
blog
pop3
dev
www2
admin
forum
news
```

Github의 dnscan 저장소에는 가장 많이 사용되는 서브 도메인 1만 개를 포함해 여러 파일이 GNU v3 라이선스로 제공되어 정찰에 활용할 수 있다.
dnscan의 서브도메인 목록과 소스코드를 깃헙에서 찾을 수 있다.

직접 작성한 스크립트에서 dnscan 같은 딕셔너리를 이용할 수 있다.
짧은 목록은 복사해서 스크립트에 바로 붙여 넣어 하드코딩 해도 된다.
dnscan의 서브도메인 1만 건 목록과 같이 큰 것을 이용하려면 별도 파일로 저장해서 스크립트가 실행될때 읽어 들이게 하는 것이 좋다.
그렇게 하면, 서브도메인 목록을 수정하거나 다른 목록으로 대체하기 쉽다.
이 리스트는 `.csv` 포맷으로 되어 있어 서브도메인 정찰 스크립트에서 쉽게 사용할 수 있다.

```javascript
const dns = require(‘dns’)
const csv = require(‘csv-parser’)
const fs = require(‘fs’)

const promises = []
/*
	디스크의 서브도메인 데이터를 읽는 스트리밍(큰 파일을 메모리에 한번에 올리지 않고, 조금씩 처리)을 시작한다.

	각 행에서 서브도메인을 질의하는 ‘dns.resolve’ 를 호출해 서브도메인이 존재하는지 확인한다.
	이 프로미스들은 ‘promises’ 배열에 저장한다.

	성능 개선 : 서브도메인 리스트가 매우 큰 경우, 별도 출력 파일을 열고 프로미스가 처리 될때마다 결과를 스트리밍 한다.
	
*/
fs.createReadStream(‘subdomains-10000.txt’).pipe(csv()).on(‘data’,(subdomain)=>{
	promises.push(new Promise((resolve, reject)=>{
		dns.resolve(`${subdomain}.mega-bank.com`, function (err, ip){
			return resolve({ subdomain: subdomain, ip: ip })
		})
	}))
})
```
공개된 서브도메인 딕셔너리를 검색해서 브루트 포스 스크립트에 붙여 넣으면 딕셔너리 공격 스크립트가 만들어진다.
이와 같이 딕셔너리 접근이 브루트 포스보다 훨씬 효율적이므로, 서브도메인을 찾을 때 딕셔너리를 먼저 시도해보고, 결과가 만족스럽지 않았을때 브루트 포스를 사용하는 순서로 접근하면 된다.

