---
slug: "6.-Stream-Coding"
---
스트림은 NodeJS의 가장 중요한 컴포넌트이자 패턴 중의 하나입니다.
커뮤니티에는 모든 것을 스트리밍 하십시오 라는 모토가 있으며, 이것만으로도 NodeJS 에서 스트림의 역할 설명을 하기엔 충분합니다.

NodeJS 커뮤니티의 최우수 기여자는 스트림을 Node에서 최고이자 가장 오해 받는 개념으로 정의하고 있다.
NodeJS 스트림이 매력적인 데는 성능이나, 효율성과 같은 기술적 속성과 관련한 부분뿐만 아니라, 우아함과 NodeJS 철학에 완벽하게 맞는가 하는 방식 등의 요인이 있습니다.

이 장은 NodeJS 스트리멩 대한 완전한 이해를 제공하는 것을 목표로 하고 있습니다.
이 장의 전반부는 NodeJS 스트림의 주요 개념과 용어 및 라이브러리를 소개합니다.

후반부에서는 더 고급적인 주제를 다룰 것이며, 가장 중요한 것은 여러 상황에서 코드를 더 우아하고 효과적으로 만들 수 있는 유용한 스트리밍 패턴을 살펴 볼 것입니다.

- NodeJS 에서 스트림이 중요한 이유
- 스트림 이해와 사용 및 생성
- 프로그래밍 패러다임으로서 스트림 : I/O 뿐만 아니라, 다양한 상황에서 강력한 기능의 활용
- 여러 환경에서의 스트리밍 패턴 및 스트림 연결

서론은 이쯤하고, 스트림이 NodeJS 에서 초석의 역할을 하는 이유를 함께 알아보겠습니다.

# 스트림의 중요성 발견
NodeJS 와 같은 이벤트 기반 플랫폼에서 I/O 를 처리하는 가장 효율적인 방법은 실시간으로 입력을 사용할 수 있게 되는 즉시 사용하고 애플리케이션이 처리하는 즉시 출력을 내보내는 것 입니다.

이 섹션에서는 NodeJS 스트림과 그 강점에 대한 소개를 제공합니다.
스트림을 사용하고 구성하는 방법에 대한 자세한 사항은 뒷부분에 나옵니다.

## 버퍼링 대 스트리밍
지금가지 이 책에서 본 거의 모든 비동기 API 는 버퍼 모드를 사용하여 동작합니다.
입력 작업의 경우 buffer모드에서 작업이 완료될 때까지 리소스에서 들어오는 모든 데이터를 버퍼에 수집합니다.
그런 다음 단일 데이터 blob으로 데이터를 소비하는 곳으로 전달합니다.


![Pasted-image-20250403133008.png](/img/이미지 창고/Pasted-image-20250403133008.png)

위 그림에서 t1 시간에서 일부 데이터가 리소스에 수신되어 버퍼에 저장되는 것을 볼 수 있습니다.
시간 t2에 다른 데이터 청크(최종 데이터 청크)를 수신하여 읽기 작업을 완료하므로, t3에서 전체 버퍼가 데이터를 소비하는 곳으로 전송됩니다.

반면에 스트림을 사용하면 리소스에서 데이터가 도착하자마자 데이터를 처리할수 잇습니다.
다음 다이어그럼은 이것을 나타냅니다.
![Pasted-image-20250403133327.png](/img/이미지 창고/Pasted-image-20250403133327.png)
위 그림에서 리소스에서 각각의 새로운 데이터 청크가 수신되는 즉시 모든 데이터가 버퍼에 수집될 때까지 기다리지 않고, 처리할 수 있는 소비자에게 즉시 전달됨을 보여줍니다.

그렇다면, 두가지 접근 방식의 차이점은 무엇일까요?
순전히 효율성 관점에서 스트림 공간(메모리 사용량)과 시간(계산 클락 시간)측면에서 더 효율적일 수 있습니다.
그러나 NodeJS스트림에는 또 다른 중요한 **결합성** 이라는 것이 있습니다.
이제 이러한 특성이 애플리케이션을 설계하고 작성하는 방식에 어떤 영향을 미치는지 살펴보겠습니다.

## 공간 효율성
우선 스트림을 사용하면, 데이터를 버퍼링하고 한번에 처리할 때 불가능한 일을 할 수있습니다.
예를 들어, 수백메가바이트 또는 심지어 기가바이트 정도의 매우 커다란 파일을 읽어야 하는 경우를 생각해봅시다.
분명히 파일을 완전히 읽을 때 큰 버퍼를 반환한다는 것은 좋은 생각이 아닙니다.
이러한 큰 파일 몇개를 동시에 읽는 다고 가정해봅시다.

우리 애플리케이션은 쉽게 메로리가 부족해질 것입니다.

그외 V8의 버퍼는 크기에 제한이 있습니다.
몇 기가 바이트 이상의 데이터를 할당할 수 없으므로, 실제 메모리가 부족해지기 전에 벽에 부딪힐 수 있습니다.
:::tip 버퍼의 실제 최대 크기는 NodeJS의 플랫폼과 버전에 따라 달라집니다.
- 주어진 플랫폼의 제한을 알고 싶다면 다음의 코드를 실행해보십시오
:::

```js
import buffer from 'buffer'
console.log(buffer.constansts.MAX_LENGTH)
```

### 버퍼링 API를 사용한 Gziping
구체적인 예제를 만들기 위해서 GZIP 형식을 사용하여, 파일을 압축하는 간단한 커맨드라인 애플리케이션을 생각해보겠습니다.
NodeJS 에서 버퍼링된 API 를 사용하면 다음과 같습니다.

```js
import { promise as fs } from 'fs'
import { gzip } from 'zlib'
import { promisify } from 'util'
const gzipPromise = promisify(gzip)

const filename = process.argv[2]

async function main(){
	const data = await fs.readFile(filename)
	const gzippedData = await gzipPromise(data)
	await fs.writeFile(`${filename}.gz`, gzippedData)
	console.log('File successfully compresse')
}

main()
```

앞의 코드를 `gzip-buffer.js` 라는 파일에 넣은 후 다음 명령을 사용하여 실행할 수 있다.
```shell
node gzip-buffer.js <path to file>
```

이때 충분히 큰 파일(약 8GB)을 선택하면 읽으려는 파일이 최대 버퍼 크기보다 크다는 오류 메시지가 발생할 가능성이 큽니다.

이것은 정확히 우리가 의도한 것이며, 우리가 잘못된 접근 방식을 사용하고 있다는 사실을 보여주기 위한 것입니다.

### 스트림을 사용한 Gzipping
Gzip 애플리케이션을 수정해서 큰 파일로도 작동하게 만드는 가장 간단한 방법은 스트리밍 API를 사용한 것입니다.
이것은 어떻게 달성할 수 있는지 살펴보겠습니다.
다음 코드로 새로운 모듈을 작성해보겠습니다.
```js
// gzip-stream.js
import { createReadStream, createWriteStream } from 'fs'
import { createGzip } from 'zlib'

const filename = process.argv[2]

createReadStream(filename)
	.pipe(createGzip())
	.pipe(createWriterStream(`${filename}.gz`))
	.on('finish', ()=> console.log('File successfully compressed'))
```

이게 전부인가? 여러분은 의아할 수 있을 것입니다.
말했듯이 스트림이 인터페이스와 결합성으로 인해 놀라우면서 우아하며 깔끔하고 간결한 코드가 가능합니다.
그러나 지금 중요한 프로그램이 모든 크기의 파일에 대해 일정한 메모리를 사용하여 원할하게 실행된다는 것입니다.

:::note 이전 예에서는 간결함을 위해 오류 처리를 생략했었습니다.
- 이 장의 뒷부분에서 스트림을 사용한 적절한 오류 처리에 관련해 설명합니다.
- 그때까지 대부분의 예제에는 적절한 오류 처리가 부족하다는 것을 염두에 두십시오

:::

### 시간 효율성
이제 파일을 압축하고 원격의 HTTP 서버에 업로드한 다음, 파일을 압축해제하고 파일 시스템에 젖아하는 애플리케이션의 경우 생각해보겠습니다.
애플리케이션의 클라이언트 컴포넌트가 버퍼링된 API 를 사용하여 구현된 경우, 전체 파일을 읽고 압축한 경우에만 업로드가 시작됩니다.

반면에 압축 해제는 모든 데이터가 수신된 경우에만 서버에서 시작됩니다.
동일한 결과를 얻기 위한 더 나은 솔루션은 스트림을 사용하여 파일 시스템에서 읽은 즉시 데이터 청크를 압축하고 전송할 수 있는 반면, 서버에서 원격 피어에서 수신하는 즉시 모든 청크를 압축 해제할 수 있습니다.

서버 측에서 시작해서 앞서 언급한 애플리케이션 빌드하여 이를 시연해보겠습니다.

다음 코드를 `gzip-receive.js` 이라는 파일에 작성해서 만들어보겠습니다.
```js
import { createServer } from 'http'
import { createWriteStream } from 'fs'
import { createGunZip } from 'zlib'
import { basename, join } from 'path'

const server = createServer((req, res)=>{
	const filename = basename(req.headers['x-filename'])
	const destFilename = join('received_files', filename)
	.on('finish', ()=>{
		res.writeHead(201,{ 'Content-Type': 'text/plain'})
		res.end('OK\n')
		console.log(`File request received: ${filename}`)
	})
})

server.listen(3000,()=>console.log('Listening on http://localhost:3000'))
```

앞의 예에서 `req`는 네트워크에서 청크 단위로 요청 데이터를 수신하기 위해 서버가 사용하는 스트림 객체입니다.
NodeJS 스트림 덕분에 모든 데이터 청크는 수신되는 즉시 압축 해제되어 디스크에 저장됩니다.

:::note 서버 애플리케이션에서 수신된 파일이름에서 경로를 제거하기 위해 `basename()`  을 사용하여 있음을 알수 있습니다.
- 수신된 파일이 `recevice_files` 폴더 내에 정확히 저장되도록 하려는 것입니다.
- `basename()` 이 없으면 악의적인 사용자가 시스템 파일을 덮어쓰도록 재 작성하여 악성 코드를 삽입할 수 잇는 요청을 만들 수 있습니다.
- 예를 들어, 파일이름이 `/usr/bin/node`로 설정되면 어떻게 되게습니까
- 이러한 공격자는 NodeJs 인터프리터를 임의의 파일로 효과적으로 덮어 쓸수 있습니다

:::

애플리케이션의 클라이언트 측은 `gzip-send.js`라는 모듈이며 코드는 다음과 같습니다.

```js
import { request } from 'http'
import { createGzip } from 'zlib'
import { createReadStream } from 'fs'
import { basename } from 'path'

const filename = process.argv[2]
const serverHost = process.argv[3]

const httpRequestOptions = {
	hostname: serverHost,
	port: 3000,
	path: '/',
	method: 'PUT',
	headers:{
		'Content-Type' : 'application/octet-stream',
		'Content-Encoding' : 'gzip',
		'X-Filename' : basename(filename)
	}
}

const req = request(httpRequestOptions, (res)=>{
	console.log(`Server response: ${res.statusCode}`)
})

createReadStream(filename)
	.pipe(createGzip())
	.pipe(req)
	.on('finish',()=>{
		console.log('File successfully sent')
	})
```

앞의 코드에서 다시 스트림을 사용하여 파일에서 데이터를 읽은 다음 시스템에서 읽은 즉시 각청크를 압축하여 보냅니다.
이제 애플리케이션을 사용해보기 위해 먼저 다음 명령을 사용하여 서버를 시작하겠습니다.
```shell
node gzip-receive.js
```

그런 다음, 보낼 파일과 서버주소(예: `localhost`)를 지정하여 클라이언트를 시작합니다.
```shell
gzip-send.js <path to file> localhost
```

충분히 큰 파일을 선택하면 데이터가 클라이언트에서 서버로 어떻게 흐르는지 알 수 있습니다.
그렇다면, 이 패러다임이 버퍼링된 API 를 사용하는 것보다 더 효율적인 이유는 무엇일까요?


![Pasted-image-20250403164746.png](/img/이미지 창고/Pasted-image-20250403164746.png)

파일이 처리되면 다음과 같은 여러 단계를 거치게 됩니다.
1. 클라이언트 파일 시스템에서 읽기
2. 클라이언트 데이터 압축
3. 클라이언트 서버로 전송
4. 서버 클라이언트에서 수신
5. 서버 데이터 압축 해제
6. 서버 디스크에 데이터 쓰기

처리를 완료하려면 조립 라인에서처럼 각 단계를 순서대로 끝까지 거쳐야 합니다.
위 그림에서 버퍼링된 API를 사용하면 프로세스가 완전히 순차적임을 알 수 있다.

데이터를 압축하려면 먼저 전체 파일을 읽을때 까지 기다려야 하며, 데이터를 전송하기 위해서는 전체 파일을 읽고 압축 될 때까지 기다려야합니다ㅣ.

스트림을 사용하면 전체 파일을 읽을 때까지 가디리지 않고, 첫 번째 데이터 청크를 수신하자마자 조립라인이 시작됩니다.

더 놀라운 것은 다른 데이터 청크를 사용할 수 잇을때 이전 데이터 청크의 작업이 완료 될때까지 기다릴 필요가 없습니다.
대신 다른 조립라인이 병렬로 시작됩니다ㅣ.

이것은 우리가 실행하는 각 작업이 비동기적이므로 NodeJS 에 의해 병렬화 될 수 있기 때문에 완벽하게 동작합니다.

유일한 제약은 청크가 각 단꼐에 도착하는 순서를 유지해야한다는 것이비다.
NodeJS 스트림의 내부 구현이 우리를 위해 순서를 유지해줍니다.

위의 그림에서 볼 수 있듯이, 스트림을 사용한 결과는 모든 데이터를 한번에 읽고 처리하는데, 시간 낭비하지 않기 때문에, 전체 프로세스에 소요되는 시간이 줄어듭니다.

## 조립성

지금까지 살펴본 코드는 `pipe()` 함수 덕분에 스트림을 구성하는 방법에 대한 개요를 제공했습니다.
이 방식은 완벽한 NodeJs 스타일로 각각 하나의 단일 기능을 담당하는 여러 프로세스들을 연결 할 수 있게 해줍니다.

이는 스트림이 균일한 인터페이스를 가지고 있고 API 측면에서 서로 이해할 수 있기 때문에 가능합니다.
유일한 전제 조건은 파이프라인의 다음 스트림이 이전 스트림에 의해 생성된 데이터 형태를 지원해야하 한다는 것입니다.

이 장의 뒷 부분에서 볼 수 잇듯이 이것이 바이너리, 텍스트 또는 객체일 수도 있습니다.
이 속성의 힘에 대한 또 다른 예시를 살펴보기 위해 이전에 만든 `gzip-send/gzip-receive` 애플리케이션에 암호화 계층을 추가해보겠습니다.

이렇게 하면, 클라이언트와 서버 모두의 약간의 변경을 적용해야합니다.

### 클라이언트 암호화 추가
```js
// ...
import { createCipheriv, randomBytes } from 'crypto'
const filename = process.argv[2]
const serverHost = process.argv[3]
const secret = Buffer.from(process.argv[4], 'hex')
const iv = randomBytes(16)
```

여기서 변경한 내용을 검토해보겠습니다.
1. 먼저 `crypto` 모듈에서 `createCipheriv()` Transform 스트림과 `randomBytes()` 함수를 import 합니다.
2. 커맨드라인에서 서버의 암호화를 위한 암호화를 얻습니다.
	- 문자열이 16진수 문자열로 전달될 것으로 예상되므로, 이 값을 읽어 16진수 모드로 설정된 버퍼를 사용하여 메모리에 로드 합니다.
3. 마지막으로, 파일 암호화를 위한 초기화 벡터로 사용할 임의의 바이트들의 시퀀스를 생성합니다.

이제 HTTP 요청을 생성하는 부분의 코드를 수정해보겟습니다.

```js
const httpRequestOptions = {
	hostname: serverHost,
	headers:{
		'Content-Type' : 'application/octet-stream',
		'Content-Encoding' : 'gzip',
		'X-Filename' : basename(filename),
		'X-Initialization-Vector' : iv.toString('hex')
	}
}

//..

const req = request(httpRequestOptions, (res)=>{
	console.log(`Server response: ${res.statusCode}`)
})

createReadStream(filename)
	.pipe(createGzip())
	.pipe(createCipheriv('aes192', secret, iv))
	.pipe(req)
```

주오 변경 사항은 다음과 같습니다.
1. 초기화 벡터를 HTTP 헤더로 서버에 전달합니다.
2. Gzip 단계 이후, 데이터를 암호화 합니다.

지금까지 클라이언트 측에 대한 것이었습니다.

### 서버 측 복호화 추가
이제 서버를 리팩토링 해보겟습니다.
가장 먼저 해야 할 일은 핵심 암호화 모듈에서 일부 유틸리티 함수를 가져오는 것입니다.
이 모듈을 사용하여, 임의의 암호화 키(암호)를 생성할 수 있습니다.

```js
// ..
import { createDecipheriv, randomBytes } from 'crypto'
const secret - randomBytes(24)
console.log(`Generated secret: ${secret.toString('hex')}`)
```

생성된 암호는 클라이언트와 공유하기 위해 16진수 문자열로 커맨드 라인에 인쇄됩니다.
이제 파일 수신 로직을 업데이트 합니다.

```js
const server = createServer((req, res)=>{
	const filename = basename(req.header['x-filename'])
	const iv = Buffer.from(
		req.headers[`x-initialzation-vector`], 'hex'
	)
	const destFilename = join('received_files', filename)
	console.log(`File request recevice: ${filename}`)

	req
		.pipe(createDecipheriv('aes192', secret, iv))
		.pipe(createGunzip())
		.pipe(createWriteStream(destFilename))
})
```

여기에서는 두 가지 변경 사항을 적용합니다.
1. 클라이언트가 보낸 암호화 초기화 [벡터](nodejsdp.link/iv) 를 읽습니다.
2. 스트리밍 파이프라인의 첫 번째 단계는 crypto 모듈에서 createDecipheriv Transform 스트림을 사용하여 들어오는 데이터 암호를 복호화 합니다.

최소한의 노력으로(단지 몇 줄의 코드), 우리는 애플리케이션에 암호화 계층을 추가하였습니다.
이미 사용 가능한 일부 Transform 스트림(createCipheriv와 createDecipheriv)을 사용하여, 클라이언트와 서버를 위한 스트림 처리 파이프라인에 포함하기만 하면 됩니다.
비슷한 방법으로 레고 블록을 가지고 노는 것처럼 다른 스트림을 추가하고 결합할 수 있습니다.

이 접근 방식의 주요 장점은 재 사용성이지만, 지금까지 코드에서 볼 수 있듯이, 스트림은 더 깔끔하고 모듈화된 코드를 가능케 합니다.
이러한 이유로 스트림은 순수한 `I/O`를 처리할 뿐만 아니라 코드를 단순화하고 모듈화하는 수단으로 자주 사용됩니다.

이제 스트림을 소개했으므로 NodeJS 에서 사용할 수 있는 다양한 유형의 스트림을 보다 구조화된 방식으로 알아볼 준비가 되었습니다.

## 스트림 시작하기
이전 섹션에서 스트림이 왜 그렇게 강력하지 배웠지만, 스트림은 핵심 모듈부터 시작하여, NodeJS 의 모든 곳에 존재합니다.
예를 들어, fs 모듈에는 파일 읽기용 `createReadStream()` 및 파일 쓰기용 `createWriteStream()` 이 있으며, HTTP 요청 및 응답 객체는 기본적으로 스트림이며, `zlib` 모듈을 사용하면, 스트림 인터페이스를 사용하여 데이터를 압축 및 압축 해제 할 수 있습니다.

마지막으로, 암호화 모듈 조차도 `createCipheriv`와 `createDecipheriv`와 같은 몇몇 유용한 스트리밍 기본 요소들을 노출 시키고 있습니다.
이제 스트림이 중요한 이유를 알앗으므로, 한걸음 물러서서 더 자세히 알아보겠습니다.

### 스트림 해부
NodeJS의 모든 스트림은 스트림 코어 모듈에서 사용할 수 있는 네가지 기본 추상 클래스 중 하나의 구현입니다.
- Readable
- Writable
- Duplex
- Transform

각 스트림 클래스 EventEmitter의 인스턴스이기도 합니다.
실제로 스트림은 읽기 가능한 스트림이 읽기를 마쳤을 때 `end`, 쓰기 스트림이 쓰기를 완료 햇을 때 `finish`, 무언가 잘못되었을 때 `error`와 같은 여러 유형의 이벤트를 생성할 수 있습니다.

스트림이 매우 유연한 이유 중 하나는 바이너리 데이터 뿐만 아니라 거의 모든 JavaScript의 값을 처리할 수 있다는 사실입니다.
- `Binary` 모드 : 버퍼 또는 문자열과 같은 청크 형태로 데이터를 스트리밍 합니다.
- 객체 모드 : 데이터를 일련의 개별 객체로 스트리밍합니다(거의 모든 JavaScript 값을 사용할 수 있음)

이 두 가지 작동 모드를 통해 I/O 뿐만 아니라 이 장의 뒷부분에서 보게 되겠지만, 함수 방식으로 처리 단위를 우아하게 구성할 수 있는 도구로도 사용할 수 있습니다.

Readable 스트림 클래스의 소개를 시작으로  NodeJS 스트림에 대해 자세히 알아보도록 하겠습니다.

### Readable 스트림
Readable 스트림은 데이터 소스를 나타냅니다.
NodeJS 에서는 스트림 모듈에서 사용할 수 있는 Readable 추상 클래스를 사용하여 구현됩니다.

### Stream에서 읽기
Readable 스트림에서 데이터를 수신하는 방법에는 noe-flowing 모드와 flowing 모드, 이렇게 두 가지가 있습니다.

### non-flowing 모드
non-flowng 또는 pause 모드는 Readable 스트림에서 읽기를 위한 기본 패턴입니다.
이것은 스트림에 읽을 수 있는 새로운 데이터가 있다는 것을 나타내는 readable 이벤트에 대해 리스너를 연결하는 작업이 포함됩니다.

그런 다음 루프에서 내부 버퍼가 비워질 때까지 데이터를 계속 읽습니다.
이는 내부 버퍼에서 동기적으로 데이터를 읽어 데이터 청크를 나타내는 Buffer 객체를 반환하는 `read()` 함수를 사용하여, 수행 할 수 있습니다. `read()` 함수는 다음과 같은 특징이 있습니다.

```js
readable.read([size])
```

이 접근 방식을 사용하면, 요청 시 스트림에서 데이터를 강제로 가져옵니다.

이것이 어떻게 작동하는지 보여주기 위해 , `read-stdin.js` 라는 새로운 모듈을 만들어 보겠습니다.
이것은 표준입력(`Readable` 스트림이기도 함) 에서 읽고 모든 것을 표준 출력으로 다시 에코 하는 간단한 프로그램을 구현합니다.

```js
process.stdin
	.on('readable', ()=>{
		let chunk
		console.log('New data available')
		while((chunk = process.stdin.read() !== null)){
			console.log(
				`Chunk read((${chunk.length} bytes): "${chunk.toString()}"`
			)
		}
	})
	.on('end', ()=> console.log('End of stream'))
```

`read()` 함수는 `Readable` 스트림의 내부 버퍼에서 데이터 청크를 가져오는 동기 작업입니다.
스트림이 바이너리 모드에서 작동하는 경우 반환된 청크는 기본적으로 Buffer 객체 입니다.

:::note 바이너리 모드에서 작동하는 Readable 스트림에서는 스트림에서 `setEncoing(encodeing)` 을 호출하여 유효한 인코딩 형식(예 : utf8)을 제공함으로써 버퍼 대신 문자열을 읽을 수 있습니다.
- 이 접근 방식은 UTF-8 텍스트 데이터 스트리밍 할 때 권장됩니다.
- 스트림이 멀티 바이트 문자를 적절하게 처리하므로 필요한 버퍼링을 수행하고 문자와 별도의 청크로 분할되지 않도록 합니다.
- 즉, 스트림에 의해 생성된 모든 청크는 유효한 UTF-8 바이트 시퀀스 입니다.
- 스트림에서 데이터 사용을 시작한 후에도 Readable 스트림에서 원하는 만큼 `setEncodeing()` 을 호출할 수 있습니다.
- 인코딩은 스트림에 의해 방출된 이전 데이터에 대한 표현일 뿐입니다.


:::

데이터는 새로운 데이터가 사용할 수 있는 즉시 호출되는 readable 리스너에만 읽힙니다.
`read()` 함수는 내부 버퍼에 더 이상 사용 가능한 데이터가 없을 때 `null`을 반환합니다.
이 경우 다시 읽을 수 있음을 알려주는 다른 readable 이벤트가 시작될 때까지 기다리거나 스트림의 끝을 알리는 end 이벤트를 기다려야 합니다.

스트림이 바이너리 모드에서 작동할 때, `read()` 함수에 크기 값을 전달하여, 특정 양의 데이터를 읽을 수 있도록 지정할 수도 있습니다.
이는 네트워크 프로토콜을 구현하거나 특정 데이터 형식의 구문을 분석할 때 특히 유용합니다.

이제 `read-stdin.js` 모듈을 실행하고 테스트 해볼 준비가 되었습니다.
콘솔에 몇가지 문자를 입력한 다음 Enter 를 눌러 데이터가 표준 출력에 다시 표시되는지 확인합니다.
스트림을 종료하여 정상적인  종료(end) 이벤트를 생성하려면 EOF 파일의 끝 문자를 삽입해야합니다ㅣ.


:::tip 우리 프로그램을 다른 프로세스와 연결해 볼 수 있습니다.
- 이는 프로그램의 표준 출력을 다른 표준 입력으로 리다이렉션 하는 파이프 연산자(`|`)를 사용하여 가능합니다.
- 이것은 스트리밍 패러다임이 작성된 언어에 관계없이 프로그램이 통신할 수 잇게 해주는 범용 인터페이스의 훌륭한 예시입니다.


:::

### Flowing 모드
스트림에서 읽는 또 다른 방법은 데이터 이벤트에 리스터를 연결하는 것입니다.
이렇게 하면 스트림이 flowing 모드를 사용하도록 전환합니다.
여기서 데이터는 `read()` 를 사용하여, 가져오지 않고, 대신 도착하자마자 데이터 리스터로 바로 전달됩니다.

예를 들어 앞서 만든 `read-stdin.js` 애플리케이션은 `flowing` 모드를 사용하면 다음과 같이 보입니다.

```js
process.stdin
	.on('data',(chunk)=>{
		console.log('New data available')
		console.log(`Chunk read(${chunk.length} bytes) : "${chunk.toString()}"`)
	})
	.on('end',()=>{
		console.log('End of stream')
	})
```

Flowing 모드는 non-flowing 모드에 비해 데이터 흐름을 제어하는 유연성이 떨어집니다.
스트림의 기본 동작모드가 `non-flowing` 모드이므로, `flowing` 모드를 활성화하려면 리스너 데이터 이벤트에 연결하거나 `resume()` 함수를 명시적으로 호출해야한다.

스트림이 데이터 이벤트를 보내는 것을 일시적으로 중단하려면 `pause()` 함수를 호출하여 들어오는 데이터 내부 버퍼에 캐시하도록 할 수 있습니다.
`pause()` 를 호출하면 스트림이 다시 `non-flowing` 모드로 전환됩니다.

### 비동기 반복자
Readable 스트림은 비동기 반복자(`Iterator`) 이기도 합니다.
따라서 다음과 같이 `read-stdin.js` 예제를 다시 작성할 수 있습니다.

```js
async function main() {
	for await (const chunk of process.stdin){
		console.log('New data available')
		console.log(`Chunk read (${chunk.length} bytes) : "${chunk.toString()}"`)
	}
	cosnsole.log('End of stream')
}

main()
```

알아야 할 중요한 것은 전체 Readable 스트림을 소비하고 프라미스 반환하는 함수를 작성해야하는 경우 이 구문이 매우 유용할 수 있다는 것입니다.

### Readable 스트림 구현
이제 스트림에서 읽은 방법을 알았으므로 다음 단계는 새로운 사용자 지정 Readable 스트림을 구현하는 방법을 배우는 것입니다.
이를 위해 스트림 모듈에서 Readable 프로토타입을 상속하여, 새로운 클래스를 만들어야 합니다.
구현된 클래스는 다음과 같은 특징을 가진 `_read()` 함수의 구현을 제공해야 합니다.

```shell
readable._read(size)
```

Readable 클래스는 내부적으로 `_read()` 함수를 호출하는데 이 함수는 `push()` 함수를 사용하여, 내부 버퍼를 채우기 시작합니다

```shell
readable.push(chunk)
```

:::note `read()`는 스트림 소비자가 호출하는 함수고, `_read()`는 스트림 하위 클래스에 의해 구현되는 함수이므로 직접 호출해서는 안됩니다.
- `_` 은 일반적으로 공용함수가 아니므로 직접 호출하면 안된다는 것을 나타냅니다.

:::

새로운 Readable 스트림을 구현하는 방법을 살펴보기 위해 임의의 문자열을 생성하는 스트림을 구현해 보겠습니다.
무작위 문자열을 생성하는 코드를 포함하는 `random-stream.js` 라는 새로운 모듈을 만들어 보겠습니다.
```js
import { Readable } from 'stream'
import Chance from 'chance'

const chance = new Chance()

export class RandomStream extends Readable{
	constructor(options){
		super(options)
		this.emiitedBytes = 0
	}
	_read(size){
		const chunk = chance.string({ length: size})
		this.push(chunk, 'utf8')
		this.emiitedBytes += chunk.length
		if(chance.bool({likelihood: 5})){
			this.push(null)
		}
	}
}
```

파일 맨 위에서 종속성을 로드합니다.
숫자에서 문자열, 전체 문장에 이르기까지 모든 종류의 임의 값을 생성하는 라이브러리인 [change](nodejsdp.link/chance) 라는 npm 모듈을 로드한다는 점을 제외하면, 특별한 것은 없습니다.

다음 단계로 `Readable`을 부모로 하는 RandomStream 이라는 새로운 클래스를 만들 것입니다.
앞의 코드에서, ReadomStream 생성자에서 super(options)를 호출하면, 부모 클래스의 생성자를 호출하여 스트림의 내부 상태를 초기화 할 수 있습니다.

:::tip super(options) 만 호출하는 생성자가 있는 경우 부모 생성자를 상속하므로 생략 할 수 있습니다.
- 사용자 지정 생성자를 작성해야 할 때 마다 super(options) 를 호출하는 것을 잊지 마십시오

:::

options 객체를 통해 전달할 수 있는 매개 변수들은 다음과 같습니다.
- 버퍼를 문자열로 변환하는데 사용되는 인코딩 인자(기본 값은 null)
- 객체 모드를 활성화하는 플래그(objectMode, 기본 값 false)
- 내부 버퍼에 저장된 데이터의 상한, 설정된 상한 이상의 데이터는 더 이상 읽지 않아야 합니다.(highWaterMark, 기본값은 16KB)

자 이제 `_read()` 함수를 설명하겠습니다.
1. 함수는 `chance`를 사용하여 `size`와 동일한 길이의 임의의 문자열을 생성합니다.
2. 문자열을 내부 버퍼로 밀어 넣습니다.
	- 문자열을 푸시하기 때문에, 인코딩 `utf8` 도 지정해야합니다.
	- 청크가 단순히 바이너리 버퍼인 경우에는 필요하지 않음
3. EOF 상황
	- 즉 스트림의 끝을 나타내기 위해 null 을 내부 버퍼로 푸시하여 5%의 가능성을 가지고 무작위로 스트림을 종료합니다.

`_read()` 함수의 `size` 인자는 권고사항입니다.
필수 사항은 아니지만, 이를 존중하고 호출자가 요청한 데이터의 크기만큼 푸시하는 것이 좋습니다

:::note `push()` 를 호출할 때 false 를 반환하는지 확인해야 합니다.
- 이 경우 수신 스트림의 내부 버퍼가 `highWaterMark` 한계에 도달했음을 의미하며, 더 많은 데이터를 추가하지 말아야합니다ㅣ.
- 이를 배압(backpressure) 이라고 하며, 이 장의 다음 섹션에서 더 자세히 설명하겠습니다.

:::

이것으로 RandomStream을 사용할 준비가 되었습니다.
RandomStream 객체를 인스턴스화 하고, 여기에서 데이터를 가져오는 방법을 살펴보겠습니다.

```js
// index.js
import { RandomStream } from './random-stream.js'

cosnt randomStream = new RandomStream()
randomStream
	.on('data', (chunk)=>{
		console.log(`Chunk received (${chunk.length}: ${chunk.toString()})`)
	})
	.on('end',()=>{
		console.log(`Produced ${randomStream.emittedBytes} bytes of random data`)
	})
```

이제 새로운 사용자 정의 스트림을 테스트 해 볼 모든 준비가 되었습니다.
항상 그랬듯이 `index.js` 모듈을 실행하고 화면에 흐르는 일련의 임의의 문자열을 확인하면 됩니다

### 단순화된 생성자
간단한 사용자 정의 스트림의 경우 Readable 스트림의 단순화된 생성자 접근 방식(simplified construction)을 사용하여, 사용자 정의 클래스를 만드는 것을 피할 수 있습니다.

이 접근 방식을 사용하면, new Readable(options)을 호출하고 옵션들에 일련의 options 내에 `read()` 함수를 전달하기만 하면 됩니다.

여기서 `read()` 함수는 클래스를 확장하는 접근법에서 본 `_read()` 함수와 정확히 동일한 의미를 가집니다.
단순화된 생성자 접근 방식을 사용하여 RandomStream을 다시 작성해보겠습니다.

```js
import { Readable } from 'stream'
import Chance from 'chance'

const chance = new Chance()

let emiitedByutes = 0

const randomStream = new Readable({
	read(size){
		const chunk = chance.string( { length: size } )
		this.push(chunk, 'utf8')
		emittedBytes += chunk.length
		if(chance.bool({ likelihood: 5 })){
			this.push(null)
		}
	}
})

// 이제 randomStream instance를 직접 사용합니다.
```

이 접근 방식은 복잡한 상태를 관리할 필요가 없을 때 유용하며, 보다 간결한 구문을 활용할 수 있습니다.
앞의 예에서 사용자 지정 스트림의 단일 인스턴스를 만들었습니다.

단순화된 생성자 접근 방식을 채택하고 싶지만, 사용자 지정 스트림의 여러 인스턴스를 만들어야 하는 경우 해당 인스턴스를 만들기 위해서는 여러 번의 호출을 할 수 있는 팩토리 함수에 초기화 로직을 감쌀 수도 있습니다ㅣ.

### 반복가능자(Iterables)에서 Readable 스트림 얻기
`Readable.from()` 을 사용하여 배열 또는 기타 **반복가능자** 객체(**제네레이터**, **반복자**와 **비동기 반복자**)에서 Readable 스트림의 인스턴스를 쉽게 만들 수 있습니다.
이 도우미 함수에 익숙해지기 위해 배열의 데이터를 Readable 스트림으로 변환하는 간단한 예제를 살펴보겠습니다.

```js
import { Readable } from 'stream'

const mountains = [
	{ name: 'Everest', height: 8848 },
	{ name: 'K2', height: 8611 },
	{ name: 'Kangchenjunga', height: 8586 },
	{ name: 'Lhoste', height: 8516 },
	{ name: 'Makalu', height: 8481 },
]

const mountainsStream = Readable.from(mountains)
mountainsStream.on('data', (mountain)=>{
	console.log(`${mountain.name.padStart(14)\t${mountain.height}m}`)
})
```

이 코드에서 볼 수 있듯이 `Readable.from()` 함수는 사용하기가 매우 간단합니다.
첫번째 인자는 반복 가능자 인스턴스(이 경우에는 산들의 배열)입니다.
`Readable.from()` 은 `objectMode`와 같은 스트림 옵션을 지정하는데 사용할 수 있는 추가적인 인자를 허용합니다.

:::tip `objectMode` 를 명시적으로 `true`로 설정할 필요가 없습니다.
- `Readable.from()` 은 명시적으로 `objectMode` 를 설정하지 않는 한, `objectMode` 가 `true` 입니다.

:::

:::tip 메모리에서 큰 배열을 인스턴스화 하지 마십시오
- 이전의 예에서 세계의 모든 산들을 나열한다고 상상해봅시다.
- 약 백만개의 산이 있으므로, 모든 산을 배열에 미리 로드하면 상당한 양의 메모릴르 할당하게 됩니다.
- 그런 다음 Readable 스트림을 통해 배열의 데이터를 소비하더라도, 모든 데이터가 이미 로드되어 있게 되므로, 스트림의 메모리 효율성을 무효화 시킵니다ㅣ.
- 데이터를 청크 단위로 로드하고 사용하는 것이 항상 바람직 하며, `fs.createReadStream` 과 같은 네이티브 스트림을 사용하거나 사용지 지정 스트림을 만들거나 제네레이터, 반복자 또는 비동기와 반복자와 함께 `Readable.from` 을 사용하여 그렇게 할 수 있습니다.

:::

### Writable 스트림
Writable 스트림은 대상 데이터의 목적지를 나타냅니다.
예를 들어, 파일 시스템의 파일, 데이터 베이스 테이블, 소켓, 표준 오류 또는 표준 출력 인터페이스를 생각해볼 수 있습니다.

NodeJS 에서는 스트림 모듈에서 사용할 수 있는 Writable 추상 클래스를 사용하여 구현합니다.

### 스트림에 쓰기
일부 데이터를 Writable 스트림으로 밀어 넣는 것은 간단합니다.
우리가 해야할 일은 다음과 같은 특징의 `write()` 함수를 사용하는 것입니다.

```js
writable.write(chunk, [encoding], [callback])
```

인코딩 인자는 선택 사항이며, 청크가 문자열일 경우 지정할 수 있습니다.(기본 값은 utf8이고, chunk가 버퍼(buffer)인 경우 무시됩니다.)
반면 callback 함수는 청크 가 기본 리소스로 플러시 될 때 호출되며 선택 사항이기도 합니다.

더 이상 스트림에 기록할 데이터가 없다는 신호를 보내려면 `end()`함수를 사용합니다.

```js
writeable.end([chunk], [encoding], [callback])
```

`end()` 함수를 통해 최종 데이터 청크를 제공할 수 있습니다.
이 경우 callback함수는 스트림에 기록된 모든 데이터가 플러시 될 때 실행되는 리스너를 `finish` 이벤트에 등록하는 것과 같습니다.

이제 임의의 문자열 시퀀스를 출력하는 작은 HTTPS 서버를 만들어 어떻게 동작하는지 보여드리겠습니다.

```js
// entropy-server.js
import { createServer } from 'http'
import Chance from 'chance'

const chance = new Chance()
const server = createServer((req, res)=>{
	res.writeHead(200, { 'Content-Type': 'text/plain'})
	while(chance.bool( { likelihood: 95 } )){
		res.write(`${chance.string()}\n`)
	}
	res.end('\n\n')
	res.on('finish', ()=> console.log('All data sent'))
})
server.listen(8080,()=>{
	console.log('listening on http://localhost:8080')
})
```

우리가 만든 HTTP 서버는 `http.ServerResponse`의 인스턴스이자, Writable 스트림인 res 객체에 문자열을 씁니다.
1. 먼저 HTTP 응답의 헤더를 작성합니다.
	- `writeHead()` 는 Writeable 인터페이스의 일부가 아닙니다.
	- 실제로 이것은 `http.ServerResponse` 클래스에 의해 노출되는 보조 함수로 HTTP 프로토콜에 한정 됩니다.
2. 5%의 확률(likelihood)로 종료되는 루프를 시작합니다.
	- 우리는 `chance.boo()` 이 95%를 참으로 반환하도록 설정합니다.
3. 루프 내에서 임의의 문자열을 스트림에 씁니다.
4. 루프를 벗어나면 스트림에서 `end()`를 호출하여 더 이상 기록할 데이터가 없음을 나타냅니다.
	- 또한 스트림을 종료하기 전에 스트림에 기록할 두 개의 개행 문자를 가진 최종 문자열을 전달합니다.
5. 마지막으로 모든 데이터가 기본 소켓으로 플러시 될 때 발생하는 `finish` 이벤트에 대한 리스너를 등록합니다.

이 시점에서 서버는 사용자가 선택한 HTTP 클라이언트에 임의의 문자열을 보내기 시작합니다.

### 배압(Backpressure)
실제 배관 시스템에서 흐르는 액체와 유사하게 NodeJS 스트림은 스트림이 소비할 수 있는 것보다 더 빨리 데이터가 기록되는 병목 현상을 겪을 수 있습니다.

이 문제에 대처하는 메커니즘은 들어오는 데이터를 버퍼링하는 것입니다.
그러나 스트림이 데이터 생성자에게 피드백을 제공하지 않는 한, 내부 버퍼에 점점 더 많은 데이터가 축적되어 원하지 않는 수준의 메모리 사용량이 발생하는 상황이 발생할 수 있습니다.

이를 방지하기 위해 `writeable.write()` 는 내부 버퍼 `hightWaterMark` 제한을 초과하면서 `false` 를 반환합니다.
`Writable` 스트림에서 `highWaterMark` 속성은 `write()` 함수가 false를 반환하기 시작하는 내부 버퍼 크기의 제한으로, 애플리케이션이 쓰기를 중지해야 하는 제한 한도를 나타냅니다.
버퍼가 비워지면 drain 이벤트갑 ㅏㄹ생하여 다시 쓰기를 시작해도 안전함을 알립니다.

이러한 매커니즘을 배압(Backpresure)이라고 합니다.

배압은 권고 매커니즘입니다.
`write()`가 false 를 반환하더라도 이 신호를 무시하고 쓰기를 계속할 수 있으므로, 버퍼가 무한정으로 커집니다.
highWaterMark 임계값에 도달한다고 스트림이 자동으로 차단되지 않습니다.
따라서 항상 주의를 기울이고 배압을 처리해주는 것이 좋습니다.

:::tip 이 섹션에서 설명하는 매커니즘은 `Readable` 스트림에도 유사하게 적용됩니다ㅣ.
- 실제로 배압은 `Readable` 스트림에도 존재하며, `_read()` 내부에서 호출되는 `push()` 함수가 `false`를 반환할 때 트리거 됩니다.
- 그러나 이는 스트림 구현자에게 특정한 문제이므로 일반적으로 덜 자주 다뤄집니다.

:::

이전에 만든 `entropy-server.js` 모듈을 수정하여 Writable 스트림의 배압을 고려하는 방법을 빠르게 보여줄 것입니다.

```js
// ...
const server = createServer((req, res)=>{
	res.writeHead(200,{ 'Content-Type' : 'text/plain' })
	function generateMore(){
		while(chance.bool( { likelihood: 95 } ))
		const randomChunk = chance.string({
			length: (16*1024) - 1
		})
		const shouldContiune = res.write(`${randomChunk}\n`)
		if(!shoudeContinue){
			console.log('back-pressure')
			return res.once('drain', generateMore)
		}
		res.end('\n\n')
	}
	generateMore()
	res.on('finish', ()=> console.log('All data sent'))
})
```

앞의 코드의 가장 중요한 단계는 다음과 같이 요약할 수 있습니다.
1. 우리는 `generateMore()` 라는 함수로 메인 로직을 감쌋습니다.
2. 일부 매압을 받을 가능성을 높이기 위해 데이터 청크의 크기를 16KB에서 1바이트 뺀 값을 설정했습니다.
	- 이는 기본 `highWaterMark` 한계에 매우 근접한 값입니다.
3. 데이터 청크를 작성한 후 `res.write()`의 반환값을 확인합니다.
	- 거짓을 받으면 내부 버퍼가 가득 차서 더 많은 데이터 전송을 중지해야 함을 의미합니다.
	- 이런 일이 발생하면, 함수를 종료하고 `drain` 이벤트가 발생했을 때, `generateMore()` 를 호출하는 식으로 버퍼가 비워지기를 기다립니다.
이제 서버를 다시 실행한 다음 curl을 사용하여, 클라이언트 요청을 생성하면 서버가 기본 소켓이 처리할 수 있는 것보다 빠른 속도로 데이터를 생성하므로 약간의 배압이 발생할 가능성이 높습니다.

### Writable 스트림 구현
Writeable 클래스를 상속하고 `_write()` 함수에 대한 구현을 제공함으로써 새로운 `Writable` 스트림을 구현할 수 있습니다.
순차적으로 세부 사항들을 논의 하면서 바로 구현해보겠습니다.

다음 형식의 객체를 수신하는 Writable 스트림을 만들어 보겠습니다.

```js
{
	path: <path to a file>
	content: <string or buffer>
}
```

이러한 각각의 객체에 대해 스트림은 주어진 경로에 생성된 파일에 content 속성 값을 저장해야 합니다.
스트림의 입력이 문자열이나 버퍼가 아니라, 객체라는 것을 즉시 알 수 있습니다.
이는 스트림이 객체 모드에서 작동해야 함을 의미합니다.

모듈을 `to-file-stream.js` 라고 명명하겠습니다.

```js
import { Writable } from 'stream'
import { promise as fs } from 'fs'
import { dirname } from 'path'
import mkdirp from 'mkdirp-promise'

export class ToFileStream extends Writable{
	construtor(options){
		super({...options, objectMode: true})
	}

	_write(chunk, encoding, cb){
		mkdirp(dirname(chunk.path))
			.then(()=> fs.writeFile(chunk.path, chunk.content))
			.then(()=>cb())
			.catch(cb)
	}
}
```

스트림 모듈에서 Writable 을 확장한 새로운 스트림 클래스를 만들었습니다.

내부 상태를 초기화하기 위해 부모 생성자를 호출해야 합니다.
또한 이 함수는 작업이 완료될 때 호출 해야하는 콜백(cb)를 받습니다.
작업 결과를 전달할 필요는 없지만, 필요한 경우 스트림에서 error 이벤트를 발생시키는 오류를 전달 할 수 있습니다.

이제 방금 만든 스트림을 테스트 하기 위해 새로운 모듈을 만들고, 스트림에 대해 몇가지 쓰기 작업을 수행해보겠습니다.

```js
import { join } from 'path'
import { ToFileStream } from './to-file-stream.js'
const tfs = new ToFileStream()

tfs.write({ path: join('files', 'file1.txt'), content: 'Hello' })
tfs.write({ path: join('files', 'file2.txt'), content: 'Node.js' })
tfs.write({ path: join('files', 'file3.txt'), content: 'streams' })
tfs.end(()=>console.log('All files created'))
```

여기서는 첫 번째 사용자 지정 Writable 스트림을 만들고 사용했습니다.
평소대로 새로운 모듈을 실행하고, 출력을 확인하십시오

실행 후 files 라는 새 폴더에 세 개의 새 파일이 생성되는 것을 볼 수 있을 것입니다.

### 단순화된 생성자(Simplified construction)
Readable 스트림에 대해 살펴봤던 것처럼 Writable 스트림도 단순화된 생성자 매커니즘을 제공합니다.
Writable 스트림에 대해 단순화된 생성자를 사용하여, `ToFileStream`을 다시 작성하면 다음과 같습니다.

```js
//...
const fs = new Writable({
	objectMode: true,
	write(chunk, encoding, cb){
		mkdirp(dirname(chunk.path))
			.then(()=> fs.writeFile(chunk.path, chunk.content))
			.then(()=>cb())
			.catch(cb)
	}
})
// ...
```

이 접근 방식에서는 단순히 `Writable` 생성자를 사용하여 Writable 인스턴스의 사용자 정의 로직을 구현한 `write()` 함수를 전달합니다.

여기서 `write()` 함수의 이름 앞에 밑줄(`_`) 이 없음에 유의하십시오,
objectMode와 같은 다른 설정 옵션들도 전달 할 수 있습니다.

## Duplex 스트림
Deplex(이중) 스트림은 읽기 및 쓰기가 가능한 스트림입ㄴ디ㅏ.
예를 들어, 네트워크 소켓과 같이 데이터 소스이자 데이터 목적지인 엔티티를 설명하려는 경우 유용합니다.

Deplex 스트림은 `stream.Readable` 과 `stream.Writable` 이렇게 두 스트림의 함수를 상속하는데, 이것은 이제 우리에게 새로울 것이 없습니다.

이것은 우리가 데이터를 `read()` 또는 `write()` 할 수 있거나 `read` 및 `drain` 이벤트 모두를 수신할 수 있음을 말합니다.

사용자 정의 Deplex 스트림을 생성하려면 `_read()` 및 `_write()` 모두에 대한 구현을 제공해야 합니다.
`Duplex()` 생성자에 전달되는 `options` 객체는 내부적으로 `Readable` 과 Writable 생성자에 모두 전달됩니다.
이 옵션들은 이전 섹션에서 이미 논의한 것과 동일하며, `allowHalfOpen(기본값 true)` 라는 새로운 옵션이 추가되어 `false` 로 설정하면 `Readable` 쪽이 끝날 때 스트림이 Writable 쪽을 자동으로 종료하며, 그 반대의 경우도 마찬가지 입니다.

:::tip 한쪽에서는 객체 모드로, 다른 쪽에서는 바이너리 모드로 작동하는 Duplex 스트림이 필요한 경우 `ReadableObjectMode`와 `writableObjectMode` 옵션을 독립적으로 사용할 수 있습니다.

:::

### Transform 스트림
`Transform` 스트림은 데이터 변환을 처리하도록 특별히 설계된 특수한 종류의 Duplex 스트림 입니다.
몇 가지 구체적인 예제로 이 장의 시작 부분에서 논의한 `zlib.createGzip()` 및 `cryto.createCipheriv()` 함수는 각각 압축 및 암호화를 위한 `Transform` 스트림을 생성합니다.

단순 Duplex 스트림에서는 스트림에서 읽은 데이터와 스트림 안에 기록된 데이터 사이에 즉각적인 관계가 없습니다.(최소한 스트림은 이러한 관계와 무관합니다.)
원격 피어와 데이터를 주고 받는 TCP 소켓을 생각해보십시오. 소켓은 입력과 출력 사이의 관계를 인식하지 못합니다.
![Pasted-image-20250404153204.png](/img/이미지 창고/Pasted-image-20250404153204.png)

위 그림은 Duplex 스트림의 데이터 흐름을 보여줍니다.

반면에 Transform 스트림은 쓰기 가능한 쪽에서 받은 각 데이터 청크에 일종의 변환을 적용한 다음, 변환된 데이터를 읽기 가능한 쪽에서 사용할 수 있도록 합니다.

![Pasted-image-20250404153316.png](/img/이미지 창고/Pasted-image-20250404153316.png)

외부에서 Transform 스트림의 인터페이스는 Duplex 스트림의 인터페이스와 똑같습니다.
그러나 새로운 Duplex 스트림을 빌드하려면 `_read()` 및 `_write()` 함수를 모두 제공해야 하는데, 새로운 Transform 스트림을 구현하려면 다른 함수의 쌍인 `_transform()`과 `_flush()`를 제공해야합니다.

### Transform 스트림의 구현
주어진 문자열의 모든 항복을 대체하는 Transform 스트림을 구현해 봅시다.
이를 위해 `replaceStream.js` 라는 새로운 모듈을 만들어야 합니다.

```js
import { Transform } from 'stream'

export class ReplaceStream extends Transform{
	constructor(searchStr, replaceStr, options){
		super({ ...options })
		this.searchStr = searchStr
		this.replaceStr = replaceStr
		this.tail = ''
	}

	_transform(chunk, encoding, callback){
		const piecs = (this.tail + chunk).split(this.searchStr)
		const lastPiece = pieces[pieces.length - 1]
		const tailLen = this.searchStr.length - 1
		this.tail = lastPiece.slice(-tailLen)
		pieces[pieces.length - 1] = lastPiece.slice(0, -tailLen)

		this.push(pieces.join(this.replaceStr))
		callback()
	}

	_flush(callback){
		this.push(this.tail)
		callback()
	}
}
```

이 예제에서는 Transform 기본 클래스를 확장하여 새로운 클래스를 만들었습니다.
클래스의 생성자는 searchStr, replaceStr 및 options 이렇게 세 가지 인자를 받습니다
상상할 수 있듯이 찾을 텍스트와 찾은 텍스트를 대체할 문자열을 정의하고 기본 Transform 스트림의 고급 설정을 위한 options 객체를 정의할 수 있습니다.

또한 나중에 `_transform()` 함수에서 내부에서 사용할 tail 변수를 초기화 합니다.
이제 새 클래스의 핵심인 `_transform()` 함수를 분석해 보겠습니다.
`_transform()` 함수는 Writable 스트림의 `_write()` 함수와 거의 동일한 특성을 갖지만, 리소스에 데이터를 쓰는 대신, Readable 스트림의 `_read()` 함수에서와 마찬가지로, `this.push()` 를 사용하여, 내부 읽기 버퍼로 데이터를 밀어 넣습니다.
이것이 Transform 스트림의 양쪽 측면이 어떻게 연결되는지를 보여줍니다.

`ReplaceStream` 의 `_transform()` 함수는 우리 알고리즘의 핵심을 구현하고 있습니다.
버퍼에서 문자열을 검색하고 바꾸는 것은 쉬운 작업입니다.

그러나 데이터가 스트리밍 될 대는 완전히 다른 이야기이며, 가능한 일치 항목이 여러 청크에 분산되어 있을 수도 잇습니다.
코드가 수행하는 절차의 설명은 다음과 같습니다.

1. 우리의 알고리즘은 searchStr을 구분자로 사용하여, 메모리에 있는 데이터(tail 데이터와 현재의 chunk)를 split 합니다.
2. 그런 다음 연산에 의해 생성된 배열의 마지막 항목을 가져와서 마지막 searchString.length-1 개의 문자들을 추출합니다.
	- 그 결과 `tail` 변수에 저장하고 다음 데이터 청크의 앞에 붙입니다.
3. 마지막으로 `split()`으로 인한 모든 조각들은 `replaceStr` 을 구분 기호로 사용하여, 함께 결합(`join`)되어 내부 버퍼로 푸시됩니다.

스트림이 종료 될 때, 내부 버퍼로 푸시되지 않은 tail 변수에 일부 콘텐츠(content) 가 잇을 수 있습니다.
이것이 바로 `_flush()` 함수의 용도입니다.
스트림이 종료되기 전에 호출되며, 여기에서 스트림을 완료하거나 스트림을 완전히 종료하기 전에 남은 데이터를 푸시할 수 잇는 마지막 기회가 존재합니다.

`_flush()` 함수는 콜백만을 받는데, 모든 작업이 완료되면 이를 호출하여, 스트림을 종료합니다.
이것으로 ReplaceStream 클래스를 완성했습니다.

스트림에 일부 데이터를 쓴 다음 변환된 결과를 읽는 스크립트를 만들어보겠습니다.

```js
import { ReaplaceStream } from './replace-stream.js'

const replaceStream = new ReplaceStream('World', 'NodeJs')
replaceStream.on('data', chunk=> console.log(chunk.toString()))

replaceStream.write('Hello W')
replaceStream.write('orld!')
replaceStream.end()
```

스트림의 처리를 조금 더 어렵게 만들기 위해 검색어(world)를 두 개의 다른 청크에 분산시킨 다음, flowing 모드를 사용하여 동일한 스트림에서 읽은 다음 변환된 각 청크를 기록합니다.
다음, flowing 모드를 사용하여 동일한 스트림에서 읽은 다음 변환된 각 청크를 기록합니다.

앞의 애플리케이션을 실행하며, 다음과 같은 출력이 생성됩니다.

:::note 앞의 출력은 `console.log()` 를 사용하여, 출력하기 때문에, 여러 줄로 나뉩니다.
- 이를 통해 일치하는 텍스트가 여러 데이터 청크에 걸쳐 잇는 경우에도 일치하는 문자열을 올바르게 대체할 수 있음을 보여줄 수 있습니다.

:::

### 단순화된 생성자(`Simplified construction`)
당연히 Transform 스트림도 단순화된 생성자를 지원합니다.
이 시점에서 우리는 이 API를 구현하는 방법에 익숙해져야 하므로 수고 스럽지만 앞선 예제를 이 접근 방식으로 다시 구현해보겠습니다.

```js
const searchStr = 'World'
const replaceStr = 'Node.js'

let tail = ''
const replaceStream = new Transform({
	defaultEncoding: 'utf8',
	transform(chunk, encoding, cb){
		const pieces = (tail+chunk).split(searchStr)
		const lastPiece = pieces[pieces.length - 1]
		const tailLen = searchStr.length - 1
		tail =  lastPiece.slice(-tailLen)
		pieces[pieces.length - 1] = lastPiece.slice(0, -tailLen)
		this.push(pices.join(replaceStr))
		cb()
	}
	
	flush(cb){
		this.push(tail)
		cb()
	}
})
```

예상대로, 단순화된 생성자는 새로운 Transform 객체를 직접 인스턴스화 하고, options 객체에 특정 변환 로직을 `transform()`과 `flush()` 함수에 담아 직접 전달함으로써, 동작합니다.
여기에서는 `transform()`과 `flush()` 함수에 `_` 이 붙지 않습니다.

### Transform 스트림을 사용한 데이터 필터링 및 집계
이전 섹션에서 언급햇듯이 Transform 스트림은 데이터 변환 파이프라인을 구현하기 위한 완벽한 블록입니다.
이전 섹션에서는 텍스트 스트림에서 단어를 대체할 수 있는 Transform 스트림의 예를 설명했습니다.
그러나 Transform 스트림을 사용하여 다른 유형의 데이터 변환을 구현할 수도 있습니다.
예를 들면, Transform 스트림을 이용해 데이터 필터링 및 데이터 집계를 구현하는 것은 매우 일반적입니다.

실례로, Frotune 500대 기업으로부터 이전 연도의 모든 매출을 포함하는 큰 파일을 분석하도록 요청 받았다고 가정해봅시다.
회사는 계산을 위해 csv 형식의 판매 보고서인 data.csv 를 사용하여, 이탈리어에서 이루어진 매출 총 수익을 계산하기를 원합니다.

단순화를 위해 CSV 파일에 저장된 판매 데이터에 품목 유형(type), 판매 국가 및 수익 이라는 라인당 세개의 필드가 포함되어 있다고 가정해보겠습니다.

이제 국가가 Italy 인 모든 레코드를 찾아야 하며 그 과정에서 일치하는 라인의 수익값을 단일 숫자로 합산해야 합니다.

스트리밍 방식으로 CSV파일을 처리하는데 유용한 [csvparse 모듈](nodejsdp.link/csv-parse) 를 사용할 것입니다.
데이터를 필터링 하고 집계하기 위해 사용자 지정 스트림을 이미 구현했다고 잠시 가정하면 이 작업에 대해 가능한 솔루션은 다음과 같습니다

```js
import { createReadStream } from 'fs'
import parse from 'csv-parse'
import { FilterByCountry } from './filter-by-country.js'
import { SumProfit } from './sum-profit.js'

const csvParser = parser({ columns: true })
createReadStream('data.csv')
	.pipe(csvParser)
	.pipe(new FileByCountry('Italy'))
	.pipe(new SumProfit())
	.pipe(process.stdout)
```

여기에서 제안하는 스트리밍 파이프라인은 5단계로 구성됩니다.
1. 소스 CSV 파일을 스트림으로 읽습니다.
2. `csv-parse` 라이브러리를 사용하여, 문서의 모든 줄을 CSV 레코드 구문 분석합니다.
	- 모든 라인에 대해 이 스트림은 속성 type, country 및 profit을 포함한 객체를 내보냅니다.
3. 국가별로 모든 레코드를 필터링하고, "Italy" 국가와 일치하는 레코드만 유지합니다.
	- 일치하지 않는 모든 레코드는 삭제됩니다.
	- 즉, 파이프라인의 다른 단계로 전달 되지 않습니다.
	- 이것은 우리가 구현해야 하는 커스텀 Transform 스트림 중 하나 입니다.
4. 모든 레코드에 대해 우리는 profit을 축척합니다.
	- 이 스트림은 결국 Italy 에서 판매된 제품의 총 수익값을 나타내는 단일 문자열을 방출(emit) 합니다.
	- 이 값은 원본 파일의 모든 데이터가 완전히 처리된 경우에만 스트림에서 내보냅니다.
	- 이것은 프로젝트를 완료하기 위해 구현해야 하는 두 번째 사용자 정의 Transform 스트림 입니다.
5. 마지막으로 이전 단계에서 내보낸 데이터가 표준 출력에 표시됩니다.

이제 FilterByCountry 스트림을 구현해보겠습니다.

```js
import { Transform } from 'stream'

export class FilterByCountry extends Transform{
	constructor (country, options = {}){
		options.objectMode = true
		super(options)
		this.country = country
	}

	_transform(record, enc, cb){
		if(record.country === this.country){
			this.push(record)
		}
		cb()
	}
}
```


`FilterByuCountry`는 사용자 지정 Transform 스트림이니다.
생성자가 country 라는 인수를 받아 필터링할 국가 이름을 지정할 수 있음을 알 수 있습니다.

생성자에서 스트림이 객체(CSV 파일의 레코드)를 처리하는데 사용된다는 것을 알고 있으므로, objectMode에서 실행되도록 설정합니다.

`_transform` 함수에서 현재 레코드의 국가가 생성 시 지정된 국가와 일치하는지 확인합니다.
일치하는 경우 `this.push()` 를 호출하여 파이프라인의 다음 단계로 레코드를 전달합니다.
레코드가 일치하는지 여부와 관계없이 `cb()` 를 호출하여 현재 레코드가 성공적으로 처리되었으며, 스트림이 다른 레코드를 수신할 준비가 되었음을 나타내야 합니다.

:::tip 패턴 : `Transform` 필터
- 조건부 방식으로 `this.push()` 를 호출하여 일부 데이터만 파이프라인의 다음 단계에 도달할 수 있도록 합니다.

:::

마지막으로, `SumProfit` 필터를 구현해보겠습니다.

```js
import { Transform } from 'stream'

export class SumProfit extends Transform{
	constructor (options = {}){
		options.objectMode = true
		super(options)
		this.total = 0
	}
	_transform(record, enc, cb){
		this.total += Number.parseFloat(record.profit)
		cb()
	}

	_flush(cb){
		this.push(this.total.toString())
		cb()
	}
}
```

이 스트림은 CSV 파일에서 레코드를 나타내는 객체를 수신하므로, `objectMode`에서 실행해야 합니다.
생성자에서 total 이라는 인스턴스 변수도 초기화 하여, 그 값을 0으로 설정합니다.

`_transform()` 함수에서 모든 레코드를 처리하고 현재 수익값을 사용하여 합계를 늘려갑니다.
이번에는 `this.push()` 를 호출하지 않는다는 점이 중요합니다.
이는 데이터가 스트림을 통해 흐르는 동안 값이 방출(`emit`)되지 않음을 의미합니다.

그러나 현재 레코드가 처리되었고, 스트림이 다른 레코드를 수신할 준비가 되었음을 나태내기 위해 여기서도 `cb()`를 호출해야합니다.

모든 데이터가 처리 되었을 때 최종 결과를 내보려면 `_flush()` 함수를 사용하여, 사용자 정의 `flush`의 동작을 정의해야합니다.
여기서 마지막으로 `this.push()`를 호출하여, 결과 합계값의 문자열을 내보냅니다.
`_flush()`는 스트림이 닫히기 전에 자동으로 호출됩니다.

:::note 패턴: 스트리밍 집계
- `_transform()`을 사용하여 데이터를 처리하고 부분 결과를 누적한 다음 `_flush()`함수에서만 `this.push()` 를 호출하여 모든 데이터가 처리 되엇을 때 결과를 내보냅니다.

:::

이것으로 예제가 완료되었습니다.
이제 코드 저장소에서 CSV 파일을 가져와 프로그램을 실행하여 이탈리어 총 수익이 얼마인지 확인 할 수있습니다.
Fortune 500대 기업의 이익을 계산한 결과이기 때문에, 분명 엄청나게 큰 금액일 것입니다.

### `PassThrough` 스트림
언급할 가치가 있는 다섯 번째 유형의 스트림은 PassThrough 입니다.
이 유형의 스트림은 변환을 적용하지 않고, 모든 데이터 청크를 출력하는 특수한 유형의 변환입니다.

PassThrough 는 아마도 가장 과소 평가된 유형의 스트림일 수 있지만, 실제로 많은 도구들 중에 매우 가치있는 도구가 될 수있는 몇가지 상황이 존재합니다.

예를 들어, PassThrough 스트림은 관찰이 가능하고 느린 파이프 연결과 지연 스트림을 구현하는데 유용할 수 있습니다.

### 관찰 가능성(Observability)
하나 이상의 스티림을 통해 흐르는 데이터의 양을 관찰하려면 데이터 이벤트 리스너를 PassThrough 인스턴스에 연결한 다음 스트림 파이프라인의 원하는 지정된 지점에서 이 인스턴스를 파이프라인으로 연결하여 수행할 수 잇습니다.
이 개념을 이해할 수 잇도록 간단한 예를 살펴보겠습니다.

```js
import { PassThrough } from 'stream'

let bytesWritten = 0
const monitor = new PassThrough()
monitor.on('data', (chunk)=>{
	bytesWritten += chunk.length
})
monitor.on('finish',()=>{
	console.log(`${bytesWritten} bytes written`)
})

monitor.write('Hello!')
monitor.end()
```

이 예에서는 새로운 PassThrough 인스턴스를 만들고, 데이터 이벤트를 사용하여, 스트림을 통해 흐르는 바이트 수를 계산합니다.
또한 finish 이벤트를 사용하여, 총 금액을 콘솔에 출력합니다.
마지막으로 `write()`와 `end()`를 사용하여, 일부 데이터를 스트림에 직접 씁니다.
이것은 단지 예시에 불괗바니다.
보다 현실적인 시나리오에서는 스트림 파이프라인의 지정된 지점에서 monitor 인스턴스 파이프로 연결합니다.
예를 들어, 이 장의 첫번째 파일 압축 예제에서 디스크에 기록하는 바이트 수를 모니터링 하려는 경우 다음과 같이 쉽게 작업을 수행할 수 있습니다.

```js
createReadStream(filename)
	.pipe(createGzip())
	.pipe(monitor)
	.pipe(createWriteStream(`${filename}.gz`))
```

이 접근 방식의 장점은 파이프라인의 다른 기존 스트림을 건드릴 필요가 없다는 것입니다.
따라서 파이프라인의 다른 부분을 관찰해야하는 경우(예를 들어, 압축되지 않은 데이터의 바이트 수를 알고 싶다고 상상해보십시오)
우리는 아주 적은 노력으로 monitor를 이동 시킬 수 있습니다.

:::note 대신 사용자 정의 `Transform` 스트림을 사용하여, monitor 스트림의 대체 버전을 구현할 수 있습니다.
- 이 경우 수신된 청크가 수정이나 지연 없이 푸시되는지 확인해야 합니다.
- 이는 PassThrough 스트림이 자동으로 수행하는 작업입니다.
- 두 접근 방식 모두 똑같이 유효하므로 더 자연스러운 접근 방식을 선택하세요

:::

### 느린 파이프 연결(Late piping)
어떤 상황에서는 스트림을 입력 매개 변수로 받아들이는 API를 사용해야 할 수도 있습니다.
스트림을 생성하고 사용하는 방법을 이미 알고 있기 때문에, 일반적으로 문제가 되지 않습니다.

그러나 주어진 API를 호출하고 나서야 스트림을 통해 읽거나 쓰려는 데이터를 사용할 수 있게 된다면, 조금 더 복잡해질 수 있습니다.

이 시나리오 를 보다 구체적인 용어로 살펴보도록 데이터 저장소 서비스에 파일을 업로드 하기 위해 다음 기능을 제공하는 API를 사용한다고 가정해보겟습니다.

```js
function upload(filename, contentStream){
	// ...
}
```

:::tip 이 함수는 AWS S3 또는 Azure Blob Storage 서비스와 같은 파일 스토리지 서비스의 SDK 에서 일반적으로 사용할 수 있는 기능을 효과적으로 단순화한 버전입니다.

:::

이제 파일 시스템에서 파일을 업로드하려면 간단히 할 수 있습니다.

```js
import { createReadStream } from 'fs'

upload('a-picture.jpg', createReadStream('/path/to/a-picture.jpg'))
```

그러나 업로드 전에 파일 스트림에 대한 처리를 수행하려고 합니다.
예를 들어, 데이터를 압축하거나 암호화 하고 싶다고 가정해봅시다.

또한 업로드 함수가 호출 된 후 이 변환을 비동기적으로 수행해야 하는 경우 어떻게 해야하나?

이러한 경우, 우리는 `upload()` 함수에 placeholder로서 passThrough 스트림을 전달 할 수 있습닏.
`upload()` 의 내부  구현은 즉시 데이터를 사용하려고 시도 하겠지만, 실제로 쓰기 전까지는 스트림에서 사용할 수 있는 데이터가 없습니다.

또한 스트림은 닫을 때까지 완료된 것으로 간주되지 않으므로, `upload()` 함수는 업로드를 시작하기 위해 데이터가 `PassThrough` 인스턴스를 통과할 때까지 기다려야 합니다.

이 접근 방식을 사용하여 파일 시스템에서 파일을 업로드 하고 프로틀리 압축(Brotli comporession) 을 사용하여 압축이 가능한 커맨드 라인 스크립트를 살펴보겠습니다.
서드파티 라이브러리에 있는 `upload()` 함수가 `upload.js` 라는 파일에서 제공된다고 가정합니다.

```js
import { createReadStream } from 'fs'
import { createBrotlicCompress } from 'zlib'
import { PassThrough } from 'stream'
import { basename } from 'path'
import { upload } from './upload.js'

const filepath = process.argv[2]
const filename = basename(filepath)
const contentStream = new PassThrough()

upload(`${filename}.br`, contentStream)
	.then((response)=>{
		console.log(`Server response: ${response.data}`)
	})
	.catch((err)=>{
		console.error(err)
		process.exit(1)
	})

createReadStream(filepath)
	.pipe(createBrotliCompress())
	.pipe(contentStream)
```

:::tip 이 책의 소스 저장소에는 로컬에서 실행할 수 있는 HTTP 서버에 파일을 업로드 할 수 있는 예제의 완전한 구현이 존재합니다.


:::

앞의 예제에서 무슨 일이 일어나고 있는지 검토 해보겠습니다.

1. 첫 번째 커맨드라인 인자에서 업로드 하려는 파일의 경로를 가져오고 `basename`을 사용하여 주어진 경로에서 파일 이름을 추정합니다.
2. `PassThrough` 인스턴스 콘텐츠 스트림을 위한 플레이스 홀더 용으로 생성합니다.
3. 이제 파일 이름(프로틀리 압축을 사용하고 있음을 나타내는 `.br` 확장자 추가)과 플레이스 홀더인 콘텐츠 스트림을 전달하여 업로드 기능을 호출합니다.
4. 마지막으로 파일 시스템 `Readable` 스트림, 프로틀리 압축 Transform 스트림 그리고 끝으로 콘텐츠 스트림을 목적지로 연결한 파이프 라인을 생성합니다.

이 코드가 실행되면 `upload()` 함수를 호출하자마자 업로드가 시작되지만, (원격 서버에 대한 연결 설정 가능), 데이터는 나주엥 파이프라인이 초기화 될 때만 흐르기 시작합니다.
처리가 완료 되면 파이프라인도 `contentStream` 을 닫고 모든 콘텐츠가 모두 사용되었음을 `upload()` 함수에 알립니다.

:::note 패턴
- 나중에 읽거나 쓸 데이터에 대한 플레이스 홀더를 제공해야 하는 경우 `PassThrough` 스트림을 사용합니다.

:::

이 패턴을 사용하여 `upload()` 함수의 인터페이스를 변환할 수 있습니다.
`Readable` 스트림을 입력으로 받아들이는 대신 `Writable` 스트림을 반환하도록 만들 수 있습니다.


그러면 반환된 Writable 스트림에 업로드할 데이터를 전달하도록 할 수 있습니다.
```js
function createUploadStream (filename){
	// ...
	// upload 데이터를 사용할 수 있도록 wirtable 스트림을 반환
}
```

이 함수를 구현해야 할 경우, 다음 예제 구현과 같이 PassThrough 인스턴스를 사용하여 매우 우아한 방식으로 이를 달성할 수 있습니다.

```js
function createUploadStream(filename){
	const connector = new PassThrough()
	upload(filename, connector)
	return connector
}
```

앞의 코드에서는 PassThrough 스트림을 커넥터로 사용하고 있습니다.
이 스트림은 라이브러리를 사용하는 소비자가 나중에 데이터를 쓸수 있게 되면 완벽한 추상화가 됩니다.

그러면 `createUploadStream()` 함수를 다음과 같이 사용할 수 있습니다.

```js
const upload = createUploadStream('a-file.txt')
upload.write('Hello World')
upload.end()
```

:::note 이 책의 소스 저장소에는 이 대체 패턴을 채택한 HTTP 업로드 예제 또한 존재합니다.

:::

## 지연 스트림
가끔식 동시에 다수의 스트림을 생성해야 하는 경우가 있습니다.
예를 들어 추가적인 처리를 위해 함수에 이 다수의 스트림을 전달해야 하는 경우 입니다.

일반적인 예는 `TAR` 밑 ZIP 과 같은 아카이브를 생성하기 위한 패키지인 [archiver](nodejsdp.link/archiver)를 사용하는 경우 입니다.
`archiver` 패키지를 사용하면 추가할 파일을 나타내는 일련의 스트림으로부터 아카이브를 만들 수 있습니다.
문제는 파일 시스템의 파일들로부터 많은 수의 스트림을 전달하려는 경우 `EMFILE` 이라는 너무 많은 파일 열기 오류가 발생할 가능성이 잇다는 것입니다.
이는 fs 모듈의 `createReadStream()` 과 같은 함수가 해당 스트림에서 읽기 시작하기 전에 새로운 스트림이 생성될 때마다 실제로 파일 디스크림터(`fd`)를 열기 때문입니다.

좀 더 일반적인 용어로 말하자면, 스트림 인스턴스를 만드는 것은, 실제로 그러한 스트림 사용을 시작하기 전에도 비용이 많이 드는 작업을 바로 초기화 합니다.
나중에 사용할 수 있도록 많은 수의 스트림 인스턴스를 만드는 경우에는 바람직하지 않을 수 있습니다.

이런 경우, 실제로 스트림에서 데이터를 소비해야할 때까지 비용이 많이 드는 초기화 지연을 시킬 수 있습니다.

[layzstream](nodejsdp.link/lazystream) 과 같은 라이브러리를 사용하여 이를 달성할 수 있습니다.
이 라이브러리를 사용하면, 실제 스트림 인스턴스에 대한 프록시를 효과적으로 생성하여, 실제로 프록시에서 데이터를 소비하기 시작할 때까지 프록시 된 인스턴스가 생성되지 않게 합니다.

다음 예제처럼 `lazystream` 을 사용하면, 특수 Unix 파일인 `/dev/urandom` 에 대한 지연 Readable 스트림을 생성할 수 있습니다.

```js
import lazystream from 'lazystream'
const lazyURandom = new lazystream.Readable(function(option)=>{
	return fs.createReadStream('/dev/urandom')
})
```

`new lazysteam.Readable()`에 매개 변수로 전달하는 함수는 사실상 필요할 대 Proxy 스트림을 생성하는 팩토리 함수 입니다.

이러한 이면에서 `lazystream` 은 `PassThrough` 스트림을 사용하여 구현됩니다.
`_read()` 함수가 처음 호출 될때만 팩토리 함수를 호출하여, 프록시된 인스턴스를 생성하고 생성된 스트림을 `PassThrough` 로 파이프 합니다.

다음 섹션에서는 `.pipe()` 함수에 대해 자세히 설명하고 처리 파이프라인을 형성하기 위해 다른 스트림을 연결하는 여러 방법을 살펴볼 것입니다.

### 파이프를 사용하여 스트림 연결하기
Unix 파이프 개념은 Douglas Mcllroy 가 발명햇습니다.
이를 통해 프로그램의 출력이 다음 프로그램의 입력으로 연결 될 수 있습니다.

다음 명령을 살펴 봅시다.

```shell
echo Hello World! | sed s/World/Node.js/g
```

앞의 명령에서 `echo`는 표준 출력에 Hello World! 를 쓴 다음, sed 명령의 표준 입력으로 리다이렉션 됩니다.
그런 다음 sed는 World를 NodeJS 로 바꾸고 결과를 표준 출력(여기서는 콘솔)으로 인쇄합ㄴ디ㅏ.

비슷한 방식으로 NodeJS 스트림은 다음의 인터페이스를 가진 Readable 스트림의 pipe() 함수를 사용하여 연결 할 수 있습니다.

```js
readable.pipe(writeable, [options])
```

매우 직관적으로 `pipe()` 함수는 Readable 스트림에서 방출된 데이터를 가져와서 제공된 Writable 스트림에 전달합니다.
또한 Readable 스트림이 종료 이벤트를 발생시키면 Writable 스트림은 자동으로 종료됩니다.
`({end:false} 옵션을 지정하지 않는 한)` `.pipe()` 함수는 첫 번째 인자에 전달된 `Writable` 스트림을 반환하는데, 이 스트림이 `Readable` 일 경우(Duplex 또는 Transform 스트림 처럼) 연결된 호출을 만들 수 있습니다.

두 개의 스트림을 함께 파이프로 연결하면 흡입(sunction) 이 생성되어 데이터가 자동으로 Writable 스트림으로 흐를 수 있으므로, `read()` 또는 `write()` 를 호출할 필요가 없으며, 가장 중요한 점은 배압(`backpressure`) 을 제어할 필요가 없단느 것입니다.

빠른 예제를 제공하기 위해 표준 입력에서 텍스트 스트림을 가져와 앞서 사용자 정의 `ReplaceStream`을 만들며 논의했던 치환 변환을 적용한 후 표준 출력으로 데이터를 내보내는 새로운 모듈을 만들어 보겠습니다.

```js
// replace.js
import { ReplaceStream } from './replace-stream.js'

process.stdin
	.pipe(new ReplaceStream(process.argv[2], process.argv[3]))
	.pipe(process.stdout)
```
``
앞의 애플리케이션은 표준 입력에서 가져온 데이터를 ReplaceStream 의 인스턴스로 파이프한 다음 다시 표준 출력으로 파이프 합니다.

이제 이 작은 애플리케이션을 사용해보기 위해 다음 예와 같이 Unix 파이프라인을 사용하여 일부 데이터를 표준 입력으로 리다이렉션을 할 수 있습니다.

```shell
echo Hello World! | node replace.js World Node.js
```

그러면 다음과 같은 출력이 생성됩니다.

```shell
Hello Node.js!
```

이 간단한 예제는 스트림(특히 텍스트 스트림)이 범용 인터페이스이며, 파이프가 이러한 모든 인터페이스를 거의 마술처럼 구성하고 상호 연결하는 방법임을 보여줍니다.

### 파이프 및 오류 처리
`pipe()`를 사용할 때 오류 이벤트는 파이프라인을 통해 자동으로 전파되지 않습니다.

예를 들어 다음코드를 살펴보십시오
```js
stream1
	.pipe(stream2)
	.on('error', ()=>{})
```

위의 파이프라인에서는 리스터를 연결한 스트림인 `stream2` 에서 발생하는 오류만 포착합니다.
즉, `stream1` 에서 생성된 오류를 포착하려면 다른 오류 리스터를 여기에 직접 연결해야 합니다.

그러면 예제가 다음과 같은 모양이 됩니다.

```js
stream1
	.on('error', ()=>{})
	.pipe('stream2')
	.on('error', ()=>{})
```
이것은 특히 많은 단계가 있는 파이프라인을 다룰 때 이상적이지 않아 보입니다.
설상가상으로 오류가 발생할 경우 실패한 스트림은 파이프라인에서 파이프가 해제됩니다.

실패한 스트림이 제대로 파괴되지 않아 리소스(예: 파일 설명자(`file descriptor`), 연결(connections) 등)가 적재된 채로 잇게 되어 메모리 누수가 생길 수 잇습니다.
이전 코드보다 장력한 (그러나 우아하지 않은) 구현은 다음과 같은 모양일 수 있습니다.

```js
function handleError(err){
	console.error(err)
	stream1.destory()
	stream2.destory()
}

stream1
	.on('error', handleError)
	.pipe(stream2)
	.on('error', handleError)
```

이 예제에서는 `stream1` 과 `stream2` 모두에 대한 error 이벤트 처리기를 등록했습니다.
에러가 발생하면 `handleError()` 함수가 호출되고 에러를 기록하고 파이프라인의 모든 스트림을 삭제할 수 있습니다.
이를 통해 할당된 모든 리소스가 제대로 해제되고 에러가 정상적으로 처리 되었는지 확인 할 수 있습니다.

### `pipeline()`을 사용한 개선된 오류 처리

파이프라인에서 수동으로 에러를 처리하는 것은 번거로운 뿐만 아니라 에러가 발생하기 쉽습니다.
가능하면 피해야 합니다.

다행히도 코어 스트림 패키지는 파이프라인 구축을 훨씬 더 안전하고 즐겁게 만들 수 있는 뛰어난 유틸리티 함수인 `pipeline()` 이라는 도우미 함수를 제공합니다.

간단히 말해서, 다음과 같이 `pipeline()` 함수를 사용할 수 있습니다.

```js
pipeline(stream1, stream2, stream3, ..., cb)
```

이 도우미 함수는 인자 목록에서 전달된 모든 스트림을 다음 스트림으로 파이프 합니다.
각 스트림에 대해서 적절한 에러를 등록하고 리스터를 닫습니다.

이렇게 하면 파이프라인이 성공적으로 완료되거나 오류로 인해 중단될 때 모든 스트림이 제대로 제거됩니다.
마지막 인자는 스트림이 완료될 때, 호출되는 옵션인 콜백입니다.
에러로 인해 종료되면 지정된 에러를 첫번째 인자로 사용하여 콜백이 호출됩니다.

이 도우미를 사용하여 테스트를 만들기 위해 파이프라인을 구현하는 간단한 커맨드라인 스크립트를 작성해보겠습니다.
- 표준 입력에서 `Gzip` 데이터 스트림을 읽습니다.
- 데이터 압축을 해제합니다.
- 모든 텍스트를 대문자로 만듭니다.
- 결과 데이터를 Gzip으로 압축합니다.
- 데이터를 표준출력으로 다시 내보냅니다.

이 모듈은 `uppercasify-gzipped.js` 라고 부르겠습니다.

```js
import { createGzip, createGunzip } from 'zlib'
import { Transform, pipelin } from 'stream'

const uppercastify = new Transform({
	transform(chunk, enc, cb){
		this.push(chunk.toString().toUpperCase())
		cb()
	}
})

pipeline(
	process.stdin,
	createGunzip(),
	uppercasify,
	createGzip(),
	process.stdout,
	(err)=>{
		if(err){
			console.error(err)
			process.exit(1)
		}
	}
)
```

이 예의 설명은 다음과 같습니다.
1. `zlib`및 `stream` 모듈에서 필요한 종속성을 가져옵니다.
2. 모든 청크를 대문자로 만드는 간단한 Transform 스트림을 만듭니다.
3. 모든 스트림 인스턴스를 순서대로 나열하는 파이프라인을 정의합니다.
4. 스트림 완료를 모니터링하기 위해 콜백을 추가합니다.
	- 에러가 발생하면 표준 오류 인터페이스에 에러를 표준 error 인터페이스에 에러를 출력하고 에러코드 1로 종료합니다.

파이프라인은 표준 입력의 데이터를 사용하고 표준 출력용 데이터를 생성하여 자동으로 시작됩니다.
다음 명령으로 스크립트를 테스트 할 수 있습니다.

```shell
echo 'Hello World!' | gzip | node uppercasify-gzipped.js | gunzip
```

그러면 다음과 같은 출력이 생성됩니다.

```shell
HELLO WORLD!
```

이전 명령 시퀀스에서 `gzip` 단계를 제거하려고 하면, 다음과 유사한 오류와 함께 스크립트 실행이 실패합니다.

이 오류는 데이터 압축 해제를 담당하는 `createGunzip()` 함수로 생성된 스트림에 의해 발생합니다.
실제로 데이터가 gzip으로 압축되어 있지 않으면, 압축 해제 알고리즘이 데이터를 처리할 수 없으며, 실패합니다.

이 경우 `pipeline()` 은 에러 발생의 정리 작업 후 파이프라인 안의 모든 스트림을 제거합니다.

:::tip `pipeline()` 함수는 내장 util 모듈의 `promisify()` 도우미를 사용하여 쉽게 프라미스화(promisify) 할 수 있습니다.

:::

이제 NodeJS 스트림에 대해 확실한 이해를 갖게 되었으므로, 제어 흐름 및 고급 파이핑 패턴과 같은 좀 더 관련 있는 스트림 패턴으로 이야기를 옮길 준비가 되었습니다.

# 스트림을 사용한 비동기 제어 흐름 패턴

지금까지 제시한 예제를 살펴보면 스트림이 I/O 를 처리할 뿐만 아니라, 모든 종류의 데이터를 처리하는데 사용할 수 있는 우아한 프로그래밍 패턴으로도 유용하게 할 수 있음을 분명히 알 수 있습니다.
그러나 장점은 단순한 이런 형식적인 면으로 끝나지 않습니다.

스트림을 활용하여, 비동기 제어 흐름(asynchronous control flow)을 흐름 제어(flow control)로 바꿀 수 있습니다.

## 순차적 실행

기본적으로 스트림은 데이터르 순서대로 처리합니다.
예를 들어 Transform 스트림의 `_transform()` 함수는 이전 호출이 `callback()`을 호출하여 완료될 때까지 다음 데이터 청크를 가지고 호출 되지 않습니다.
이 것은 스트림의 중요한 속성으로 각 청크를 올바룬 순서로 처리하는데 중요하지만, 스트림을 전통적인 제어 흐름 패턴에 대한 우아한 대안으로 전환하는데 이용할 수 있습니다.

약간의 코드를 보여주는 것이 많은 설명을 부가하는 것보다 낫기 때문에, 스트림을 사용하여 비동기 작업을 순서대로 실행하는 방법을 보여주는 예제를 살펴보겠습니다.
입력으로 받은 일련의 파일들을 연결하는 함수를 만들어 제공되는 순서를 준수해봅시다.
`concat-files.js` 라는 새로운 모듈을 만들고 다음과 같이 내용을 작성해보겠습니다.

```js
import { createWriteStream, createReadStream } from 'fs'
import { Readable, Transform } from 'stream'

export function concatFiles (dest, files){
	return new Promise((resolve, reject)=>{
		.pipe(new Transform({
			objectMode: true,
			transform(filename, enc, done){
				const src = createReadStream(filename)
				src.pipe(destStream, {end :false})
				src.on('error', done)
				src.on('end', done)
			}
		}))
		.on('error', reject)
		.on('finish', ()=>{
			destStream.end()
			resolve()
		})
	})
}
```

앞의 함수는 파일 배열의 스트림으로 변환하여 순차 반복을 구현합니다.
알고리즘은 다음과 같이 설명할 수 있습니다.

1. 먼저 `Readable.from()` 을 사용하여 파일 배열에서 Readable 스트림을 만듭니다.
	- 이 스트림은 객체 모드(object mode - Readable.form() 에서 생성된 스트림의 기본 설정) 에서 작동하며, 파일 이름을 내보냅니다.
	- 모든 청크 파일 경로를 나타내는 문자열 입니다.
	- 청크의 순서는 파일 배열의 순서를 따릅니다.
2. 다음으로, 순서대로 각 파일을 처리할 사용자 정의 Transform 스트림을 만듭니다.
	- 문자열을 수신하고 있으므로, `objectMode` 옵션을 `true` 로 설정합니다.
	- `Transform` 로직에서 각 파일에 대해 `Readable` 스트림을 만들어 파일 내용을 읽고 `destStream`(대상 파일에 대한 Writable 스트림)으로 파이프 합니다.
	- `pipe()` 옵션에 `{ end: false }`를 지정하여 소스 파일 읽기가 완료된 후에도 destStream 을 닫지 않도록 합니다.
3. 소스 파일의 모든 내용이 destStream으로 파이프 되면, done 함수를 호출하여 현재 처리의 완료를 알리며, 이는 파일의 처리를 시작시키는데 필요합니다.
4. 모든 파일이 처리되면 종료 이벤트가 발생합니다.
	- 마지막으로 `destStream`을 종료하고 `concatFiles()` 의 `cb()` 함수를 호출하여 전체 작업이 완료되었음을 알립니다.

이제 방금 만든 작은 모듈을 사용해 볼 수 있습니다.
`concat.js` 라는 새로운 파일에서 이 작업을 수행합니다.

```js
import { concatFiles } from './concat-files.js'

async function main(){
	try{
		await concatFiles(process.argv[2], process.argv.slice(3))
	}catch(err){
		console.error(err)
		process.exit(1)
	}

	console.log('All files concatenated succesfully')
}

main()
```

이제 첫 번째 커맨드라인 인자로 대상 파일을 전달한 다음 연결할 파일 목록을 전달하여 앞의 프로그램을 실행할 수 있습니다.

```shell
node concat.js all-together.txt file1.txt file2.txt
```

그러면 file1.txt와 file2.txt의 내용이 순서대로 포함된 `all-together.txt`라는 새로운 파일이 생성됩니다.

`concatFiles()` 함수에서 오직 스트림만 사용하여 비동기 순차 반복을 얻을 수 있었습니다.

:::note 패턴
- 일련의 비동기 작업을 순서대로 쉽게 반복하려면 스트림 또는 스트림 조합을 사용합니다.

:::

## 순서가 없는 병렬 실행
방금 스트림이 각 데이터 청크를 순서대로 처리하는 것을 보았지만, 때로는 NodeJS 의 동시성을 최대한 활용하지 못하기 때문에 병목 현상을 발생할 수 있습니다.

모든 데이터 청크에 대해 느린 비동기 작업을 실행해야 하는 경우, 실행을 병렬화하여, 전체 프로세스 속도를 높이는 것이 유리할 수 있습니다.

물론 이 패턴은 각 데이터 청크 사이에 관계가 없는 경우에만 적용할 수 있습니다.
이는 객체 스트림에서는 자주 발생하지만 바이너리 스트림에서는 거의 발생하지 않습니다.

:::warning 
- 데이터가 처리되는 순서가 중요한 경우 정렬되지 않는 병렬 스트림을 사용할 수 없습니다.

:::

`Transform` 스트림의 실행을 병렬화하기 위해 4장에서 배운것과 동일한 패턴을 적용할 수 있지만, 몇몇 수정을 통해 스트림과 함께 동작하도록 할 수 있습니다.

### 순서가 없는 병렬 스트림의 구현

예를 들어, 순서가 없는 병렬 스트림을 구현하는 방법을 바로 보여드리겠습니다.

`parallel-stream.js` 라는 모듈을 만들고, 주어진 변환 함수를 병렬로 실행하는 일반적인 Transform 스트림을 정의해보겠습니다.

```js
import { Transform } from 'stream'

export class ParallelStream extends Transform{
	constructor(userTransform, opts){
		super({ objectMode: true, ...opts})
		this.userTransform = userTransform
		this.running = 0
		this.terminatedCd = null
	}

	_transform(chunk, enc, done){
		this.running++
		this.userTransform(
			chunk,
			enc,
			this.push.bind(this)
			this._onComplete.bind(this)
		)
		done()
	}

	_flush(done){
		if(this.running>0){
			this.terminatedCb = done
		}else{
			done()
		}
	}

	_onComplete(err){
		this.running--
		if(err){
			return this.email('error', err)
		}
		if(this.running === 0){
			this.terminateCb && this.terminateCb()
		}
	}
}
```

이 새로운 클래스를 단계별로 분석해보겠습니다.

1. 보시다싶이 생성자는 `userTransform()` 함수를 받아 인스턴스 변수로 저장합니다.
	- 부모 생성자를 호출하고, 편의를 위해 기본적으로 객체모드를 활성화 합니다.
2. 다음으로 `_transform()` 함수의 차례입니다.
	- 이 함수에서는 `userTransform()` 함수를 실행한 다음, 실행 중인 작업의 수를 늘립니다.
	- 마지막으로 `done()` 을 호출하여 현재 변환 단계가 완료 되었음을 `Transform` 스트림에 알립니다.
	- 다른 항목을 병렬로 처리하는 트릭이 바로 이것입니다.
	- `done()` 을 호출하기 전에 `userTransform()` 함수가 완료되기를 기다리지 않습니다.
	- 대신에 바로 다음을 수행합니다.
	- 반면에 `this._onComplete()` 함수인 `userTransform()`에 대한 특수한 콜백이 전달됩니다.
	- 이를 통해 `userTransform()` 실행이 완료될 때 알림을 받을 수 있습니다.
3. `_flush()` 함수는 스트림이 종료되기 직전에 호출되므로 아직 작업이 실행중인 경우 `done()` 콜백을 특시 호출하지 않음으로써, 종료 이벤트를 보류 시킬 수 있습니다.
	- 대신 `this.terminatedCb` 변수에 이를 할당합니다.
4. 스트림이 제대로 종료되는 방법을 이해하려면, `_onComplete()` 함수를 살펴봐야합니다.
	- 이 마지막 함수는 비동기 작업이 완료 될 때 마다 호출 됩니다.
	- 실행중인 작업이 더 있는지 확인하고, 작업이 없으면 `this.terminatedCb()` 함수를 호출하여 스트림을 종료하고, `_flush()` 함수에서 보류된 종료 이벤트를 해제시킵니다.

방금 만든 `ParallelStream` 클래스를 사용하면 작업을 병렬로 실행하는 `Transform` 스트림을 쉽게 만들 수 잇지만, 주의해야할 점이 있습니다.
항목들이 수신될 때의 순서가 보존되지 않습니다.
실제로 비동기 작업은 시작 시점에 관계없이 언제든지 데이터를 완료하고 푸시 할 수 잇습니다.

우리는 이 특성이 일반적으로 데이터 순서가 중요한 바이너리 스트림에서는 잘 작동하지 않으리라는 것을 바로 이해할 수 있지만, 일부 유형의 객체 스트림에서는 확실히 유용할 수 있습니다.

### URL 상태 모니터링 애플리케이션 구현
이제 ParallelStream 을 구체적인 예제에 적용 해보겠습니다.
많은 URL 목록의 상태를 모니터링 하는 간단하나 서비스를 빌드한다고 가정해보겠습니다.
이러한 모든 URL 이 단일 파일에 포함되어 있고, 줄 바꿈으로 구분되어 잇다고 가정해 봅시다.

특히 ParallelStream 클래스를 사용하여 URL 검사를 병렬화하는 경우, 스트림은 이 문제에 대한 매우 효율적이고 우아한 솔루션을 제공할 수 있다.

`check-url.js` 라는 새로운 모듈에서 이 간단한 애플리케이션을 바로 만들어 보겠습니다.

```js
import { pipeline } from 'stream'
import { createReadStream, createWriteStream } from 'fs'
import split from 'split'
import superagent from 'superagent'
import { ParallelStream } from './parallel-stream.js'

pipeline(
	createReadStream(process.argv[2]),
	split(),
	new ParallelStream(
		async (url, enc, push, done)=>{
			if(!url){
				return done()
			}
			try{
				await superagent.head(url, { timeout: 5*1000 })
				push(`${url} is up\n`)
			}catch(err){
				push(`${url} is down\n`)
			}
			done()
		}
	),
	createWriteStream('result.txt'),
	(err)=>{
		if(err){
			console.error(err)
			process.exit(1)
		}
		console.log('All urls have been checked')
	}
)
```

보시다 싶이 스트림을 사용하면 코드가 매우 우아하고 간단 해보입니다.
모든 것이 단일 스트리밍 파이프 라인에 포함되어 있습니다.
1. 먼저 입력으로 제공된 파일에서 Readable 스트림을 만듭니다.
2. 입력 파일의 내용을 `split(nodejsdp.link/split)`을 통해 파이프 합니다.
	- 이는 각 라인이 서로 다른 청크로 방출되도록 하는 Transform 스트림 입닏.
3. 그런 다음, ParallelStream 을 사용하여 URL 을 확인할 시간 입닏.
	- head 요청을 보내고 응답을 기다리면 됩니다.
	- 작업이 완료되면 결과를 푸시 합니다.
4. 마지막으로 모든 결과가 reault.txt 파일로 파이프 됩니다.

이제 다음과 같은 명령으로 `check-url.js` 모듈을 실행해 봅시다.
```shell
node check-urls.js urls.txt
```

여기에서 `urls.txt` 파일에는 URL 목록(한 줄에 하나씩)이 포함되어 있습니다.
예를 들면 다음과 같은 식 입니다.

```shell
https://mario.fyi
https://loige.co
https://thiswillbedownforsure.com
```

결과가 기록되는 순서가 입력 파일에 URL이 지정된 순서와 다를 가능성이 높습니다.
이 결과는 우리 스트림이 작업을 병렬로 실행하고 스트림의 다양한 데이터 청크 사이에 어떤 순서도 적용되지 않는 다는 분명한 증거입니다.

:::note 호기심 차원에서 `ParallelStream` 을 일반 `Transform` 스트림으로 대체하고 둘의 동작과 성능을 비교해 볼 수 있을 것입니다.
- Transform 을 직업 사용하면 각 URL 이 순서대로 확인되기 때문에, 속도가 훨씬 느리지만, 입력 순서가 `result.txt` 파일의 결과의 순서에서 유지됩니다.

:::

다음 섹션에서는 주어진 시간에 실행되는 병렬 작업의 수를 제한하기 위해 이 패턴을 확장하는 방법을 알아보겠습니다.

### 순서가 없는 제한된 병렬 실행
수천 또는 수 백만 개의 URL이 포함된 파일에 대해 `check-urls.js` 애플리케이션을 실행하려고 하면 분명히 문제가 발생할 것입니다.
우리의 애플리케이션은 한번에 제어되지 않는 수의 연결을 생성하여 상당한 양의 데이터를 병렬로 전송하고 잠재적으로 애플리케이션의 안정성과 전체 시스템의 가용성을 손상 시킬 수 있을 것입니다.
이미 알고 있듯이 부하 및 리소스 사용량을 제어하는 솔루션은 병렬 작업의 동시성을 제한하는 것입니다.

이전 섹션에서 만든  `parallel-stream.js` 를 수정한 `limited-parallel-stream.js` 모듈을 생성하여 이것이 스트림에서 어떻게 동작하는지 살펴보겠습니다.

생성자에서 시작하여 어떤 형태인지 살펴보도록 하겠습니다.

```js
export class LimitedParallelStream extends Transform{
	constructor(concurrency, userTransform, opts){
		super({...opts, objectMode: true})
		this.concurrency = concurrency
		this.userTransform = userTransform
		this.running = 0
		this.contiuneCb = null
		this.terminateCb = null
	}
}
// ...
```

입력으로 받을 동시성의 제한(concurrency)이 필요합니다.
이번에는 보류 중인 `_transform` 함수에 대한 콜백(continueCb) `_flush` 함수의 콜백(terminateCb), 이렇게 두 개의 콜백을 저장합니다.

다음은 `_transform()` 함수 입니다.

```js
_transform(chunk, enc, done){
	this.running++
	this.userTransform(
		chunk,
		enc,
		this.push.bind(this),
		this._onComplete.bind(this)
	)
	if(this.running < this.cuncurrency){
		done()
	}else{
		this.cuntinueCb = done
	}
}
```
   
이번에는 `_transform()` 함수에서 `done()` 을 호출하고 다음 항목의 처리 트리거 하기 전에 사용 가능한 실행  슬롯이 있는지 확인해야 합니다.
동시에 실행되는 최대 스트림의 수에 이미 도달한 경우에는 작업이 완료되는 즉시, 호출 될 수 있도록 `continueCb` 변수에 `done()` 콜백을 저장하면 됩니다.

`_flush()` 함수는 `ParallelStream` 클래스와 동일하게 유지되므로, `_onComplete()` 함수 구현으로 직접 이동해 보겠습니다.

```js
_onComplete(err){
	this.running--
	if(err){
		return this.emit('error', err)
	}
	const tmpCb = this.continueCb
	this.continueCb = null
	tmpCb && tmpCb()
	if(this.running === 0){
		this.terminateCb && this.terminateCb()
	}
}
```

작업이 완료될 때마다, 저장된 `continueCb()` 를 호출하여 스트림 차단을 해제하고 다음 항목에 대한 처리를 트리거 합니다.
이것이 LimitedParallelStream 클래스에 대한 것입니다.
이제 ParallelStream 대신 `check-urls.js` 모듈에서 이를 사용하여, 설정한 값으로 작업의 동시성을 제한할 수 있습니다.

## 순서가 있는 병렬 실행
이전에 생성한 병렬 스트림은 내보낸 데이터의 순서를 지키지 않는데, 이것이 허용되지 않는 상황이 있습니다.
때때로 각 청크를 수신한 순서대로 내보내야 합니다.
그러나 희망이 없는 것은 아닙니다.

변환 기능을 병렬로 실행할 수도 있습니다.
우리가 해야 할 일은 각 작업에서 내보내는 데이터를 정렬하여, 데이터가 수신된 것과 동일한 순서를 따르게 하는 것입니다.

이 기술은 실행 중인 각 작업에서 청크를 내보내는 동안 버퍼를 사용하여, 청크를 재정렬합니다.
간결함을  위해 이러한 스크림의 구현은 제공되지 않을 것입니다.

이 책의 범위에 비하여 매우 장황하기 때문입니다.
대신 우리가 할 일은 이 특정 목적을 위해 만들어진 사용 가능한 패키지 중 하나를 `npm` 에서 골라 사용하는 것입니다.
우리는 [parallel-transform](nodejsdp.link/parallel-tranform) 을 사용할 것입니다.

기존 check-urls 모듈을 수정하여 순서가 잇는 병렬 실행의 동작을 빠르게 확인 할 수 있습니다.
검사를 병렬로 실행하면서, 입력 파일의 URL 과 동일한 순서로 결과를 작성하기 원한다고 가정하겠습니다.

`parallel-transform`을 사용하여 이를 수행하겠습니다.

```js
//...
import parallelTransform from 'parallel-transform'

pipeline(
	createReadStream(process.argv[2])
	split(),
	parallelTransform(4, async function(url, done){
		if(!url){
			return done()
		}
		console.log(url)
		try{
			await request.head(url, { timeout: 5 *1000 })
			this.push(`${url} is up\n`)
		}catch(err){
			this.push(`${url} is down\n`)
		}
		done()
	}),
	createWriteStream('results.txt'),
	(err)=>{
		if(err){
			console.error(err)
			process.exit(1)
		}
		console.log('All urls have been checked')
	}
)
```

이 예제에서 `parallelTransform()` 은 최대 4개의 병렬 실행으로 변환 로직을 실행하는 `Transfrom` 스트림을 객체 모드로 생성합니다.
이 새로운 버전의 `check-urls.js` 를 실행해 결과를 볼 수 있습니다.

txt 파일은 입력 파일에 나타내는 URL 과 동일한 순서로 결과를 나열합니다.
출력의 순서가 입력과 동일하더라도 비동기 작업은 여전히 병렬로 실행되며, 임의의 순서로 완료 될 수 잇다는 점을 인지하는 것이 중요합니다.

:::tip 정렬된 병렬 실행 패턴을 사용하는 경우
- 느린 항목이 파이프라인을 차단하거나 메모리를 무한정으로 증가 시킬 수 있다는 것을 알아야 합니다.
- 실제로 완료하는데 매우 긴 시간이 필요한 작업이 존재한다면, 구현한 패턴에 따라 다른 일이 벌어집니다.
- 경우에 따라 결과마다 순서를 정해 저장하는 버퍼가 보류된 결과들을 저장하며 무한히 증가하거나 느린 작업이 완료될 때까지 모든 처리가 차단 될 수 있습니다.
- 우선적으로는 성능을 최적화 하는 전략을 사용해볼 수 잇고, 다음으로는 예측 가능한 메모리 사용량을 얻는 전략을 적용해볼 수 있습니다.
- `parallel-transform`의 내부 구현에서는 예측 가능한 메모리 사용률을 선택하고 지정된 최대 동시성 이상으로 증가하지 않는 내부 버퍼를 유지하도록 하는 것입니다.

:::

# 파이핑(Piping) 패턴
실제 배관과 마찬가지로 NodeJS 스트림은 서로 다른 패턴들을 조합하여 함께 파이프로 연결할 수도 있습니다.
실제로 두 개의 서로다른 스트림의 흐름을 하나로 병합하거나 한 스트림의 흐름을 둘 이상의 파이프로 분할하거나 조건에 따라 흐름을 리다리렉션 할 수 있을 것입니다.
이 섹션에서는 NodeJS 스트림에 적용할 수 있는 중요한 연결 패턴을 살펴볼 것입니다.


## 스트림 결합
이 장에서는 스트림이 코드를 모듈화하고 재사용할 수 있는 간단한 인프라를 제공한다는 사실을 강조했지만, 이 퍼즐에서 마지막 부분이 누락 되었습니다.
전체 파이프 라인을 모듈화하고 재사용 하려면 어떻게 해야 할까요
외부에서 하나처럼 보이도록 여러 스트림을 결합하려면 어떻게 해야할까요
아래는 이것을 의미하는 바를 보여줍니다.

![Pasted-image-20250414132722.png](/img/이미지 창고/Pasted-image-20250414132722.png)
위 그림으로 이것이 어떻게 작동하는지에 대한 아이디어를 얻었습니다.
- 결합된 스트림에 쓸 때는 실제 파이프라인의 첫번째 스트림에 씁니다.
- 결합된 스트림에서 읽을 때는 실제로 파이프라인의 마지막 스트림에서 읽습니다.

결합된 스트림은 일반적으로 Duplex 스트림입니다.
첫 번째 스트림을 Writable 쪽에 연결하고 마지막 스트림을 Readable 측에 연결하여 만들어집니다.

:::note `Writable` 스트림과 `Readable` 스트림 두 개로 `Duplex` 스트림을 생성하려면 [dupexer2](nodejsdp.link/duplexer2) 또는 [duplexify](nodejsdp.link/deplexify) 와 같은 npm 모듈을 사용할 수 있습니다.

:::

하지만 그것만으로는 충분하지 않습니다.
실제로 결합된 스트림의 또 다른 중요한 특징은 파이프라인 내부의 스트림에서 발생하는 모든 에러를 인지하고 전파한다는 것입니다.

이미 언급했듯이 모든 오류 이벤트는 `pipe()` 를 사용할 때 파이프라인을 따라 자동으로 전파되지 않으며, 각 스트림에 `error` 리스너를 명시적으로 연결해야합니다.

`pipeline()` 도우미 함수를 사용하여 에러 관리를 통해 `pipe()` 의 한계를 극복할 수 있다는 것을 알았습니다.
그러나 `pipe()` 와 `pipeline()` 도우미 함수의 문제는 두 함수가 마지막 스트림만 반환한다는 것입니다.
따라서 우리는 (첫번째) Writable 컴포넌트가 아닌(마지막) Readable 컴포넌트만 얻게 됩니다.

다음 코드에서 이를 매우 쉽게 확인 가능합니다.
```js
import { createReadStream, createWriteStream } from 'fs'
import { Transform, pipeline } from 'stream'
import { strict as assert } from 'assert'

const streamA  = createReadStream('package.json)
const streamB = new Transform({
	trasnform(chunk, enc, done){
		this.push(chunk.toString().toUpperCase())
		done()
	}
})

const streamC = createWriteStream('package-uppercase.json')
const pipelineReturn = pipeline(
	streamA,
	streamB,
	streamC,
	()=>{
		// 여기서 에러를 처리
})
assert.strictEqual(streamC, pipelineReturn) // 유효함

const pipeReturn = streamA.pipe(streamB).pipe(streamC)
assert.strictEqual(streamC, pipeReturn) // 유효함

```

앞의 코드에서 `pipe()` 또는 `pipeline()` 만으로는 결합된 스트림을 구성하는 것이 간단하지 않다는 것이 분명합니다.

요약하자면, 결합된 스트림에는 두 가지 중요한 이점이 존재합니다.
- 내부 파이프라인을 숨겨서 블랙박스로 재배포할 수 있습니다.
- 파이프라인의 각 스트림에 error 리스터를 연결할 필요가 없고, 결합된 스트림 자체에만 연결하기 때문에 오류 관리가 단순해집니다.

스트림을 결합하는 것은 매우 일반적ㅇ니 관행이므로, 특별한 이유가 아니면 [pumpifiy](nodejsdp.link/pumpify) 와 같은 기존의 라이브러를 사용할 수 있습니다.

이 라이브러리는 매우 간단한 인터페이스를 제공하빈다.
사실 결합된 스트림을 얻기 위해서 할 일은 `pumpify()` 를 호출하여 파이프라인에서 원하는 모든 스트림을 전달하는 것입니다

이것은 콜백이 없다는 점을 제외한다면 `pipeline()`의 특징과 매우 흡사합니다ㅣ.

```js
const combinedStream = pumpify(streamA, streamB, streamC)
```

이와 같은 작업을 수행하면 `pumpify` 는 스트림에서 파이프라인을 생성하고, 파이프라인의 복잡성을 추상화하는 새로운 결합 스트림을 반환하며, 이전에 언급했던 장점들을 제공합니다.



### 결합된 스트림의 구현
스트림 결합의 간단한 예를 설명하기 위해 다음 두 가지 Transform 스트림을 살펴보겠습니다.
- 데이터를 압축하고 암호화
- 데이터의 암호를 복호화하고 압축 해제

`Pumpify` 와 같은 라이브러리를 사용하면, 기본 라이브러리에서 이미 사용할 수 있는 일부 스트림을 결합하여 이러한 스트림(`combined-stream.js`라는 파일)을 쉽게 만들 수 있다.

```js
import { createGzip, createGunZip } from 'zlib'
import { createCipheriv, createDecipheriv, scyptSync } from 'crypto'
import pumpify from 'pumpify'

function createKey (password){
	return scryptSync(password, 'salt', 24)
}

export function createCompressAndEncrypt(password, iv){
	const key = createKey(password)
	const combinedStream = pumpify( createGzip(), createCipheriv('aes192', key, iv))
	combinedStream.iv = iv

	return combinedStream
}

export function createDecryptAndDecompress (password, iv){
	const key = createKey(password)
	return pumpify(
		createDeciperiv('aes192', key, iv),
		createGunzip()
	)
}
```

이제 결합된 스트림을 마치 블랙박스처럼 사용하여 파일을 압축하고 암호화한 후 보관하는 작은 애플리케이션을 만들 수 있습니다.

`archive.js` 라는 새로운 모듈에서 이를 수행해보겠습니다.


```js
import { createReadStream, createWriteStream } from 'fs'
import { pipeline } from 'stream'
import { randomBytes } from 'crypto'
import { createCompressAndEncrypt } from './combined-streams.js'

const [,, password, source ] = process.argv
const iv = randomBytes(16)
const destination  = `${source}.gz.enc`

pipeline(
	createReadStream(source),
	createCompressAndEncrypt(password, iv),

	createWriteStream(destination),
	(err)=>{
		if(err){
			console.error(err)
			process.exit(1)
		}
		console.log(`${destination} created with iv: ${iv.toString('hex')}`)
	}
)
```

Archive 파일 내에 얼마나 많은 단계가 있는지 걱정할 필요가 없습니다.
실제로, 현재 파이프 라인 내에서 단일 스트림으로 처리합니다ㅣ.

이렇게 하면, 결합된 스트림을 다른 컨텍스트에서 쉽게 재사용 할 수 있습니다.

이제 archive 모듈을 실행하기 위해서 커맨드라인 인자에 암호화할 파일을 지정하기만 하면 됩니다.

```shell
node archive.js [mypassword] /path/to/a/file.txt
```

이 명령은 `/path/to/a/file.txt.gz.enc` 라는 파일을 생성하고 초기화 벡터를 콘솔에 출력합니다.
이제 연습으로 `createDecryptAndDecompress()` 함수를 사용하여 암호, 초기화 벡터 및 아카이브된 파일을 가져와서 이를 푸는 유사한 스크립트를 생성할 수 있습니다.

:::note 실제 애플리케이션에서는 일반적으로 초기화 벡터를 사용자가 직접 전달하도록 요구하는 것보다 암호화된 데이터의 일부로 포함 시키는 것이 좋습니다.
- 이를 구현하는 한 가지 방법은 초기화 벡터를 나타내기 위해 아카이브 스트림에서 처음 16바이트를 방출하는 것입니다.
- 아카이브를 푸는 유틸리티는 스트리밍 방식으로 데이터 처리를 시작하기 전에 처음 16바이트를 사용하도록 적절히 수정해야 합니다.
- 이 접근 방식은 이 예제의 범위를 벗어난 몇 가지 추가적인 복잡성을 더하므로, 예제에서는 간단한 해결책을 선택했습니다.
- 스트림에 익숙해준 후, 사용자가 초기화 벡터를 전달할 필요가 없는 솔루션을 연습삼아 구현해보는 것도 좋습니다.

:::

이 예제를 통해 스트림을 결합하는 것이 얼만큼 중요한지 명확하게 볼 수 있었습니다.
한 편으로는 재사용 가능한 스트림 구성을 만들 수 있고, 다른 편으로는 파이프라인의 오류를 단순화 할 수 있습니다.

## 스트림 분기
단일 Readable 스트림을 여러 Writable 스트림으로 파이핑하여 스트림 분기를 수행할 수 있습니다
이것은 동일한 데이터를 다른 목적지(예를 들어, 두 개의 다른 소켓 또는 두 개의 다른 파일)로 보내려고 할 때 유용합니다.
동일한 데이터에 대해 다른 변환을 수행하거나 데이터를 분할하는 경우에도 사용할 수 있습니다.
Unix 명령 [tee](nodejsdp.link/tee) 에 익숙한 분들께는 이 것을 NodeJS 스트림에 적용한 것이나 다름 없습니다.

![Pasted-image-20250414143033.png](/img/이미지 창고/Pasted-image-20250414143033.png)

### 다중 체크썸 생성기 구현
주어진 파일의 `sha1` 및 `md5` 해시를 모두 출력하는 작은 도구를 만들어 봅시다.
이 새로운 모듈을 `generate-hashes.js` 라 부르겠습니다.

```js
import { createReadStream, createWriteStream } from 'fs'
import { createHash } from 'crypto'

const filename = process.argv[2]
const sha1Stream = createHash('sha1').setEncoding('hex')
const md5Stream = createHash('md5').setEncoding('hex')
const inputStream = createReadStream(filename)

inputStream
	.pipe(sha1Stream)
	.pipe(craeteWriteStream(`${filename}.sha1`))

inputStream
	.pipe(md5Stream)
	.pipe(createWriteStream(`${filename}.md5`))
```
`inputStream` 변수는 한쪽에서는 `sha1Stream`으로, 다른 한쪽에서는 `md5Stream` 으로 파이프 됩니다.
이면에서 발생하는 몇가지 주의할 사항이 있습니다.

- `pipe()` 호출할 때  `{ end: false }`를 옵션으로 지정하지 않는 한, `md5Stream`과 `sha1Stream`은 `inputStream`이 종료될 때 자동으로 종료됩니다.
- 분기된 스트림은 동일한 청크를 수신하므로, 데이터를 내보내는 모든 스트림에 영향을 미치게 됩니다.
	- 이때 데이터에 대한 부작용을 주의하여 데이터 처리 작업을 수행해야 합니다.
- 배압(Backpressure)은 바로 발생합니다.
	- `InputStream`에서 오는 흐름은 매우 빠르나, 분기된 스트림은 느립니다.
	- 다시 말해, 한 스트림이 오랜 시간동안 배압을 처리하기 위해 소스 스트림을 일시정지 시킨다면 다른 모든 대상 스트림들도 대기하게 됩니다.
	- 또한 하나의 대상을 무기한 차단하게 되면, 전체 파이프라인이 차단됩니다.
- 소스(비동기 파이핑)에서 데이터 사용을 시작한 후 추가적인 스트림으로 파이프하면, 새로운 스트림은 파이프된 이후의 새로운 데이터 청크만 수신하게 됩니다.

이 경우 `PassThrough` 인스턴스를 플레이스홀더(placeholder)로 사용하여 스트림 소비를 시작하는 순간부터 모든 데이터를 수집할 수 있습니다.

그런 다음 `PassThrough` 스트림은 데이터의 손실 위험 없이 나중에 언제든지 읽을 수 있습니다.
이 접근 방식은 이전에 논의한 바에 같이 배압이 발생하게 하고, 전체 파이프라인을 차단하게 할 수 있습니다.

## 스트림 병합
병합은 분기와 반대되는 작업이며, 아래와 같이 Readable 스트림 집합을 하나의 Writable 스트림으로 파이프하는 작업이 포함됩니다.
![Pasted-image-20250414144250.png](/img/이미지 창고/Pasted-image-20250414144250.png)
여러 스트림을 하나로 병합하는 것은 일반적으로 간단한 작업입니다
그러나 기본 옵션 `({ end: true })` 을 사용하는 파이핑은 소스 중 하나가 끝나자마자 목적지 스트림을 종료하므로, end 이벤트를 처리하는 방법에 주의를 기울여야 합니다.

활성화된 다른 소스가 이미 종료된 스트림에 계속 쓰는 경우 이로 인해 종종 에러가 발생할 수 있습니다.

이 문제에 대한 해결책은 여러 소스를 단일 대상을 파이핑 할 때 `{ end: false }` 옵션을 사용하고 모든 소스가 읽기를 완료한 경우에만 목적지에서 `end()`를 호출하도록 하는 것입니다.

### 텍스트 파일 병합
간단한 예제를 만들기 위해 출력할 파일의 경로와 임의의 수의 텍스트 파일을 가져온 다음, 모든 파일의 행들은 목적지 파일에 병합하는 작은 프로그램을 구현 해봅시다.

우리는 이 새로운 모듈을 `merge-lines.js` 라고 부를 것입니다.

몇가지 초기화 단계부터 시작하여, 그 내용을 정의해보겟습니다.

```js
import { createReadStream, createWriteStream } from 'fs'
import split from 'split'
const dest = process.argv[2]
const sources = process.argv.slice(3)
```

앞의 코드에서는 모든 종속성을 로드하고 목적지(dest) 파일과 모든 소스 파일(sources)의 이름을 포함하는 변수를 초기화 합니다.

다음으로 목적지 스트림을 만듭니다.

```js
const destStream = createWriteStream(dest)
```

이제 소스 스트림을 초기화 할 시간입니다.

```js
let endCount = 0
for(const source of sources){
	const sourceStream = createReadStream(source, { highWaterMark: 16})
	sourceStream.on('end', ()=>{
		if(++endCount === sources.length){
			destStream.end()
			console.log(`${dest} created`)
		}
	})
	sourceStream
		.pipe(split((line)=> line+'\n'))
		.pipe(destStream, { end: false })
}
```

앞의 코드에서는 모든 소스 파일에 대해 Readable 스트림을 만들었습니다.
그런 다음 각 소스 스트림에 대해 모든 파일을 완전히 읽은 경우에만 목적지 스트림을 종료하도록 end  리스너를 추가하였습니다.
마지막으로 모든 소스 스트림을 텍스트 라인에 대해 청크를 생성하도록 하는 Transform 스트림인 `split()` 으로 파이프하고 마지막으로 결과를 목적지 스트림으로 파이프 했습니다.
여기서 실제 병합이 발생합니다.

여러 소스 스트림을 단일 목적지로 파이프 합니다.
이제 다음 명령으로 이 코드를 실행할 수 있습니다.

```shell
node merge-lines.js <destination> <source1> <source2> <source3> ...
```

충분히 큰 파일로 이 코드를 실행하면, 목적지 파일에 모든 소스파일에서 임의로 혼합되니 라인들이 포함되어 있음을 볼 수 있습니다.(highWaterMark가 16바이트 보다 낮으면 이 특성이 더욱 명확해집니다.)

이러한 종류의 동작은 일부 유형의 객체 스트림과 줄 단위로 분할된 일부 텍스트 스트림에서 허용될 수 있지만, 대부분 이진 스트림을 처리할 때는 바람직하지 않는 경우가 많습니다.

순서대로 스트림을 병합할 수 있는 한가지 변형이 존재합니다.
소스 스트림을 차례로 소비하는 것으로 구성됩니다.

이전 항목이 끝나면 다음 항목이 청크를 방출하기 시작합니다.(모든 소스의 출력을 연결하는 것과 같습니다.)
항상 그렇듯이 npm 에서 이러한 상황을 처리하기 위한 패키지를 찾을 수 있습니다.
그 중 하나가 [multistream](https://npmjs.org/pacakge/multistream) 입니다.

## 멀티플렉싱 및 디멀티플렉싱

병합 스트림 패턴에는 여러 스트림을 결합하는 것이 아니라 공유 채널(shared channel)을 사용하여, 일련의 스트림들의 데이터를 전달하는 특별한 변형이 존재합니다.
이는 소스 스트림이 공유 채널 내에서 논리적으로 분리되어 있기 때문에 개념적으로 다른 작업입니다.
데이터가 공유 채널의 다른 끝에 도달하면 스트림을 다시 분할 할 수 있습니다.


![Pasted-image-20250414150030.png](/img/이미지 창고/Pasted-image-20250414150030.png)


단일 스트림을 통한 전송을 허용하기 위해 여러 스트림(이 경우 채널이라고 함)을 결합하는 작업을 멀티플렉싱이라고 하며, 반대의 작업, 즉 공유 스트림에서 수신된 데이터를 원래의 스트림으로 재구성하는 작업을 디멀티플렉싱이라고 합니다.

이러한 작업을 수행하는 장치를 각각 멀티플렉서(혹은 mux)그리고 디멀티플렉서(혹은 demux)라고 합니다.
이것은 전화, 라디오, TV는 물론 인터넷 자체와 같은 거의 모든 유형의 통신 매체의 기초 중 하나이기 때문에 일반적으로 컴퓨터 과학 및 통신 분야에서 널리 연구되는 분야입니다.

이 섹션에서 보여주고 싶은 것은 NodeJS 공유 스트림을 사용하여, 논리적으로 분리된 여러 스트림을 전송한 다음 공유 스트림의 다른 쪽 끝에서 다시 분리하는 방식입니다.

### 원격 로거(logger) 만들기
이야기를 계속하기 위해 예제를 사용하겠습니다.
우리는 자식 프로세스를 시작하고 표준 출력과 표준 에러를 원격 서버로 리다이렉션 하는 작은 프로그램을 만들 것입니다.
원격 서버는 두 스트림을 두 개의 개별 파일에 저장합니다.
따라서 이 경우 공유 매체는 TCP 연결이고 다중화할 두 채널은 자식 프로세스의 stdout 및 stderr 입니다.

IP, TCP 및 UDP 와 같은 프로토콜에서 사용하는 것과 같은 기술인 패킷스위칭(packet switching) 이라는 기술을 사용합니다.
패킷 스위칭에서는 데이털르 패킷으로 감싸서 멀티플렉싱, 라우팅, 흐름제어, 손상된 데이터 검사 등에 유용한 다양한 메타 정보를 설정할 수 있습니다.

예제에서 구현하는 프로토콜은 매우 간단합니다.
![Pasted-image-20250414151500.png](/img/이미지 창고/Pasted-image-20250414151500.png)
위 그림에서 볼 수 있듯이, 패킷에는 실제 데이터가 포함되어있지만, 헤더(채널ID+데이터 길이)도 포함되어 있어서 각 스트림을 구분하고 디멀티플렉서가 패킷을 올바른 채널로 라우팅 할 수 잇습니다.

### 클라이언트 측 - 멀티플렉싱
클라이언트 측에서부터 애플리케이션 만들기를 시작하겟습니다.
우리는 이 모듈을 `client.js` 라고 부를 것입니다.

이것은 자식 프로세스를 시작하고 스트림을 멀티플렉싱하는 애플리케이션의 일부를 나타냅니다.

먼저 모듈을 정의하는 것으로 시작하겠습니다.
```js
import { fork } from 'child_process'
import { connect } from 'net'
```

이제 소스의 목록을 멀티플렉싱하는 함수를 구현해보겠습니다.
```js
function multiplexChannels (sources, destination){
	let openChannels = sources.length
	for(let i = 0; i< sources.length; i++){
		sources[i]
			.on('readable', function(){
				let chunk
				while((chunk = this.read()) !== null){
					const outBuff = Buffer.alloc(1+4++chunk.length)
					outBuff.writeUInt8(i,0)
					outBuff.writeUInt32BE(chunk.length,1)
					chunk.copy(outBuff, 5)
					console.log(`Sending packet to channel: ${i}`)
					destination.write(outBuff)
				}
			})
			.on('end', ()=>{
				if(--openChannels === 0){
					destination.end()
				}
			})
	}
}
```

`multiplexChannels()` 함수는 다중화(멀티플렉싱)할 소스 스트림과 목적지 채널을 입력으로 받은 후 다음 단계를 수행합니다.

1. 각 소스 스트림에 대해 readable 이벤트에 댛나 리스너를 등록합니다.
	- 여기서 `non-flowing` 모드를 사용하여 스트림에서 데이터를 읽습니다.
2. 청크를 읽으면 채널 ID로 `1byte(UInt8)`, 패킷의 크기로 4bytes(UInt32BE), 포함한 패킷으로 묶습니다.
3. 패킷이 준비되면 목적지 스트림에 기록합니다.
4. 마지막으로 모든 소스 스트림이 종료되었을 때 목적지 스트림을 종료할 수 있도록 end 이벤트에 대한 리스터를 등록합니다.

:::tip 우리의 프로토콜은 채널을 식별한느데 1byte 만 있기 대문에, 최대 256개의 서로 다른 소스 스트림을 다중화 할 수 있습니다.

:::

이제 클라이언트의 마지막 부분이 매우 간단해졌습니다.

```js
const socket = connect(3000,()=>{
	const child = fork(
		process.argv[2],
		process.argv.slice(3),
		{ silent: true }
	)
	multiplexChannels([child.stdout, child.stderr], socket)
})
```

이 마지막 코드에서 다음 작업을 수행합니다.
1. 주소 `localhost:3000` 에 대한 TCP 클라이언트 연결을 엽니다.
2. 첫 번째 커맨드라인 인자를 경로로 하여 자식 프로세스를 시작하고, 나머지 `process.argv` 배열은 자식 프로세스에 대한 인자로 제공합니다.
	- 자식 프로세스가 부모의 `stdout` 및 `stderr` 를 상속하지 않도록 `{ silent: true }` 옵션을 지정합니다.
3. 마지막으로 자식 프로세스의 `stdout`과 `stderr`를 가져와서 multiplexChannels() 함수를 사용하여 소켓의 Writable 스트림으로 멀티플렉싱 합니다.


### 서버 측 - 디멀티플렉스

이제 애플리케이션(`server.js`) 의 서버 측을 만들 수 있습니다.
여기서는 원격 연결로부터 스트림을 디멀티플렉싱해서 두 개의 다른 파일로 파이프 합니다.

`demultiplexChannel()`이라는 함수를 만들어 보겠습니다.
```js
import { createWriteStream } from 'fs'
import { createServer } from 'net'

function demultiplexChannel(source, destinations){
	let currentChannel = null
	let currentLength = null

	source
		.on('readable', ()=>{
			let chunk
			if(currentChannel === null){
				chunk = source.read(1)
				currentChannel = chunk && chunk.readUInt8(0)
			}

			if(currentLength === null){
				chunk = source.read(4)
				currentLength = chunk && chunk.readUInt32BE(0)
				if(currentLength === null){
					return null
				}
			}

			chunk = source.read(currentLength)
			if(chunk === null) {
				return null
			}

			console.log(`Received packet from: ${currentChannel}`)
			destinations[currentChannel].write(chunk)
			currentChannel = null
			currentLength = null
		})
		.on('end',()=>{
			destinations.forEach(destination=> destination.end())
			console.log('Source channel closed')
		})
}
```

위 코드는 복잡해 보일 수 있지만, 그렇지 않습니다.
NodeJS Readable 스트림의 기능을 사용하면 다음과 같이 작은 프로토콜의 디멀티플렉싱을 쉽게 구현할 수 있습니다.

1. `non-flowing` 모드를 사용하여 스트림에서 읽기를 시작합니다
2. 먼저, 아직 채널 ID 를 읽지 않았다면 스트림에서 1byte를 읽어서 숫자로 변환합니다.
3. 다음 단계는 데이터의 길이를 읽는 것입니다.
	- 이를 위해 4bytes가 필요하므로 내부 버퍼에 아직 충분한 데이터가 들어와있지 않은 경우에도 처리가 가능합니다.
	- 즉, `this.read()` 호출이 null을 반환했는데, 아직 데이터 길이만큼 읽지 못한 경우 파싱을 중단하다음 읽기 가능한 이베트에서 재시도 할 수 있습니다.
4. 마지막으로 데이터 크기만큼 읽을 수 있을 경우, 내부 버퍼에서 가져올 데이터의 크기를 알고 있으므로 모두 읽습니다.
5. 모든 데이터를 읽으면 적절한 목적지 채널에 쓸 수 있으며 `currentChannel` 및 `currentLength` 변수를 재 설정했는지 확인합니다.(다음 패킷을 분석하는데 사용됨)
6. 마지막으로 소스 채널이 종료되면 목적지 채널을 종료해야 합니다.

이제 소스 스트림을 디멀티플렉스할 수 있으므로 새로운 함수를 작동시켜 보겠습니다.
```js
const server = createServer((socket)=>{
	const stdoutStream = createWriteStream('stdout.log')
	const stderrStream = createWriteStream('stderr.log')
	demultiplexChannel(socket, [stdoutStream, stderrStream])
})
server.listen(3000, ()=> console.log('Server started'))
```

앞의 코드에서 우리는 우선 포트 TCP 서버를 포트 3000에서 대기하도록 시작시킵니다.
그런 다음 수신되는 각 연결에 대해 두 개의 서로 다른 파일을 가리키는 Writable 스트림을 만듭니다.

하나는 표준 출력 용이고, 다른 하나는 표준 에러 용입니다.
이것이 우리의 목적지 채널입니다. 마지막으로 `demultiplexChannel()` 을 사용하여 소켓 스트림을 stdoutStream 및 stderrStream 으로 디멀티플렉싱합니다.

### Mux/demux 애플리케이션 실행
이제 새로운 mux/demux 애플리케이션을 사용해 볼 준비가 되었습니다.
먼저 샘플 출력을 생성할 작은 NodeJS 애플리케이션을 만들어 보겠습니다.

우리는 이 모듈을 `generate-data.js` 라고 부를 것입니다.
```js
console.log('out1')
console.log('out2')
console.error('err1')
console.log('out3')
console.error('err2')
```

좋습니다. 이제 원격 로깅 애플리케이션을 사용해볼 수 있습니다.
먼저 서버를 시작합니다.

```shell
node server.js

# 하위 프로세스로 시작할 파일을 전달하여 클라이언트 시작
node client.js generateData.js
```

클라이언트는 거의 바로 실행되지만, 프로세스가 끝나면 `generate-data.js` 애플리케이션의 표준 입력 및 표준 출력이 단일 TCP 연결을 통해 이동한 다음, 서버 두 개의 개별 파일로 디멀티플렉싱 되어야 합니다.

:::tip `child_process.fork()` 를 사용하고 있기 때문에, 우리의 클라이언튼느 다른 NodeJS 모듈만 실행할 수 있다는 점에 유의하세요

:::

### 멀티 플렉싱 및 디멀티플렉싱 객체 스트림

방금 보여준 예제는 바이너리/텍스트 스트림을 멀티플렉싱하고 디멀티플렉싱하는 방법을 보여주지만 동일한 규칙이 객체 스트림에 적용한다는 점을 언급해야 할 듯 합니다.
가장 큰 차이점은 객체를 사용할 때 원자 메시지(객체)를 사용하여 데이터 전송하는 방법이 이미 존재하기 때문에, 멀티플렉싱은 각 객체 channelID 속성을 설정하는 것으로 충분합니다ㅣ.

디멀티플렉싱은 단순히 channelId 속성을 읽고 각 객체를 올바른 목적지 스트림으로 라우팅합니다.

디멀티플렉싱에만 관련된 또 다른 패턴은 특정 조건에 따라 소스에서 수신된 데이털르 라우팅하는 것입니다.
![Pasted-image-20250414164622.png](/img/이미지 창고/Pasted-image-20250414164622.png)
위 그림의 시스템에서 사용된 디멀티 플렉서는 동물을 나타내는 객체스트림을 가져와서 파충류, 양서류, 포유류와 같은 동물의 종류에 따라 올바른 대상 스트림에 각각 배분합니다.

동일한 원칙을 스트림에 대해 if...else 문을 사용하여 구현할 수 있습니다.
참고로 [ternary-stream](nodejsdp.link/ternary-stream) 패키지를 고려해볼 수도 잇습니다.














































---

#NodeJS #ApplicationArcithecture 

---